<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>类别不平衡问题的方法汇总</title>
    <url>/class-imbalance/</url>
    <content><![CDATA[<h1 id="类别不平衡问题-class-imbalance-是什么"><a href="#类别不平衡问题-class-imbalance-是什么" class="headerlink" title="类别不平衡问题(class-imbalance)是什么"></a>类别不平衡问题(class-imbalance)是什么</h1><ul>
<li>指分类任务中不同类别的训练样例数目差别很大的情况</li>
<li>若不同类别的训练样例数目稍有差别，通常影响不大，但若差别很大，则会对学习过程造成困扰。例如有998个反例，但是正例只有2个，那么学习方法只需要返回一个永远将新样本预测为反例的学习器，就能达到99.8%的精度；然而这样的学习器往往没有价值，因为它不能预测出任何正例</li>
</ul>
<h1 id="上采样-过采样-Oversampling"><a href="#上采样-过采样-Oversampling" class="headerlink" title="上采样(过采样, Oversampling)"></a>上采样(过采样, Oversampling)</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><ul>
<li>对训练集中的少数类进行“过采样”，即增加一些少数类样本使得正、反例数目接近，然后再进行学习</li>
</ul>
<h2 id="Random-Oversampling-随机上采样"><a href="#Random-Oversampling-随机上采样" class="headerlink" title="Random Oversampling(随机上采样)"></a>Random Oversampling(随机上采样)</h2><ul>
<li>简单复制样本的策略来增加少数类样本，容易产生模型过拟合的问题</li>
</ul>
<h2 id="SMOTE"><a href="#SMOTE" class="headerlink" title="SMOTE"></a>SMOTE</h2><ul>
<li>即合成少数类过采样技术(Synthetic Minority Oversampling Technique)，是基于随机采样算法的一种改进，其基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中</li>
</ul>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ul>
<li>1.对于少数类中每一个样本$x_i$，以欧氏距离为标准计算它到少数类样本集$S_{min}$中所有样本的距离，得到其k近邻</li>
<li>2.根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本$x_i$，从其k近邻中随机选择若干个样本，假设选择的近邻为$\tilde{x}$。</li>
<li>3.对于每一个随机选出的近邻$\tilde{x}$，分别于原样本按照如下的公式构建新的样本</li>
<li>$$x_{n e w}=x+\operatorname{rand}(0,1) \times |\tilde{x}-x|$$</li>
<li><img src="http://anki190912.xuexihaike.com/20201019195529.png?imageView2/2/h/250"></li>
</ul>
<h3 id="SMOTE的问题"><a href="#SMOTE的问题" class="headerlink" title="SMOTE的问题"></a>SMOTE的问题</h3><ul>
<li>随机选取少数类样本用以合成新样本，而不考虑周边样本的情况<ul>
<li>1.如果选取的少数类样本周围都是少数类样本，则新合成的样本不会提供太多有用信息。就像SVM中远离margin的点对决策边界影响不大</li>
<li>2.如果选取的少数类样本周围都是多数类样本，这类的样本可能是噪声，则新合成的样本会与周围的多数类样本产生大部分重叠，导致分类困难</li>
</ul>
</li>
</ul>
<h2 id="Borderline-SMOTE"><a href="#Borderline-SMOTE" class="headerlink" title="Borderline-SMOTE"></a>Borderline-SMOTE</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ul>
<li>由于原始SMOTE算法的对所有少数类样本都是一视同仁的，我们希望新合成的少数类样本能处于两个类别的边界附近，因为在实际建模过程中那些处于边界位置的样本更容易被错分，因此利用边界位置的样本信息产生新样本可以给模型带来更大的体征，能提供足够的信息用以分类，即Borderline SMOTE算法做的事情</li>
</ul>
<h3 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h3><ul>
<li>这个算法会先将所有的少数类样本分成三类，如图</li>
<li><img src="http://anki190912.xuexihaike.com/20201019140912.png"></li>
<li>“noise”：所有的k近邻个样本都属于多数类，可认为是噪声不能生成合成样本</li>
<li>“danger”：超过一半的k近邻样本属于多数类</li>
<li>“safe”：超过一半的k近邻样本属于少数类</li>
<li>borderline smote算法只会从处于”danger”状态的样本中随机选择，然后用SMOTE算法产生新的样本。处于”danger”状态的样本代表靠近”边界”附近的少数类样本往往更容易被误分类。因而Border-line SMOTE只对那些靠近”边界”的少数类样本进行人工合成样本，而SMOTE则对所有少数类样本一视同仁</li>
</ul>
<h3 id="危险集的判断流程"><a href="#危险集的判断流程" class="headerlink" title="危险集的判断流程"></a>危险集的判断流程</h3><ul>
<li>1.对于每个$x_{i} \subset S_{\min }$确定一系列K近邻样本集，称该数据集为$S_{i-kNN}$，且$S_{i-kNN} \subset S$</li>
<li>2.对每个样本$x_i$，判断出最近邻样本集中属于多数类样本的个数，即$\left|S_{i-k N N} \cap S_{m a j}\right|$</li>
<li>3.选择满足不等式$\frac{k}{2} \leq\left|S_{i-k N N} \cap S_{m a j}\right| \leq k$，将其加入危险集DANGER</li>
</ul>
<h3 id="Borderline-SMOTE分类两种："><a href="#Borderline-SMOTE分类两种：" class="headerlink" title="Borderline SMOTE分类两种："></a>Borderline SMOTE分类两种：</h3><ul>
<li>Borderline-1 SMOTE：在合成样本时所选的近邻是一个少数类样本</li>
<li>Borderline-2 SMOTE：在合成样本时所选的近邻是任意一个样本</li>
</ul>
<h2 id="ADASYN-Adaptive-Synthetic-Sampling，自适应合成采用"><a href="#ADASYN-Adaptive-Synthetic-Sampling，自适应合成采用" class="headerlink" title="ADASYN(Adaptive Synthetic Sampling，自适应合成采用)"></a>ADASYN(Adaptive Synthetic Sampling，自适应合成采用)</h2><ul>
<li>根据数据分布情况为不同的少数类样本生成不同数量的新样本</li>
<li>首先根据最终的平衡程度设定总共需要生成的新少数类样本数量，然后为每个少数类样本x计算分布比例</li>
</ul>
<h1 id="下采样-降采样-UnserSampling"><a href="#下采样-降采样-UnserSampling" class="headerlink" title="下采样(降采样, UnserSampling)"></a>下采样(降采样, UnserSampling)</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><ul>
<li>对训练集中多数类样本进行“下采样”(undersampling)，即去除一些多数类中的样本使得正例、反例数目接近，然后再学习</li>
</ul>
<h2 id="Random-Undersampling-随机下采样-或-原型选择-Prototype-Selection"><a href="#Random-Undersampling-随机下采样-或-原型选择-Prototype-Selection" class="headerlink" title="Random Undersampling(随机下采样) 或 原型选择(Prototype Selection)"></a>Random Undersampling(随机下采样) 或 原型选择(Prototype Selection)</h2><h3 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h3><ul>
<li>从多数类$S_{maj}$中随机选择一些样本组成样本集E。然后将样本集E从$S_{maj}$中移除。新的数据集$S_{n e w-m a j}=S_{m a j}-E$</li>
<li>通过改变多数类样本比例以达到修改样本分布的目的，从而使样本分布较为均衡</li>
</ul>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>操作简单，只依赖于样本分布，不依赖任何距离信息，属于非启发式方法</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>由于采样的样本集合要少于原来的样本集合，因此会造成一些信息缺失，即将多数类样本删除有可能导致分类器丢失有关多数类的重要信息</li>
</ul>
<h2 id="Ensemble-Methods"><a href="#Ensemble-Methods" class="headerlink" title="Ensemble Methods"></a>Ensemble Methods</h2><h3 id="EasyEnsemble"><a href="#EasyEnsemble" class="headerlink" title="EasyEnsemble"></a>EasyEnsemble</h3><h4 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h4><ul>
<li>多次随机欠采样，尽可能全面地涵盖所有信息，特点是利用boosting减少偏差(Adaboost)、bagging减少方差(集成分类器)。实际应用的时候可尝试选用不同的分类器来提高分类的效果</li>
<li><img src="http://anki190912.xuexihaike.com/20201020145429.png?imageView2/2/h/350"></li>
</ul>
<h4 id="算法流程-2"><a href="#算法流程-2" class="headerlink" title="算法流程"></a>算法流程</h4><ul>
<li>1.把数据划分为两部分，分别是多数类样本$S_{maj}$和少数类样本$S_{min}$</li>
<li>2.从多数类$S_{maj}$中有放回的随机采样n次，每次选取与少数类数目相近的样本个数即$|S_{imaj}|=|S_{min}|$，可得到n个样本集合，记作$\{S_{1 m a j}, S_{2 m a j}, \ldots, S_{n m a j}\}$</li>
<li>3.将每一个多数类样本的子集$S_{imaj}$与少数类样本$S_{min}$合并后训练出Adaboost分类器$H_i$，阈值设置为$\theta_i$，可得到n个模型，即$H_{i}(x)=\operatorname{sgn}\left(\sum\limits_{j=1}^{s_{i}} \alpha_{i j} h_{i, j}(x)-\theta_{i}\right)$</li>
<li>4.将这些模型组合形成一个集成学习系统，最终的模型结果是这n个模型的投票值。此处采用加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率小的弱分类器的权值，使其在表决中起较小的作用，即最终分类器为$H(x)=\operatorname{sgn}\left(\sum\limits_{i=1}^{n} \sum\limits_{j=1}^{s_{i}} \alpha_{i j} h_{i j}(x)-\sum\limits_{i=1}^{n} \theta_{i}\right)$</li>
</ul>
<h3 id="BalanceCascade"><a href="#BalanceCascade" class="headerlink" title="BalanceCascade"></a>BalanceCascade</h3><h4 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h4><ul>
<li>该算法得到的是一个级联分类器，基于Adaboost，将若干个强分类器由简单到复杂排列，只有和少数类样本特征比较接近的才有可能输入到后面的分类器，比如边界点，因此能更充分地利用多数类样本的信息，一定程度上解决随机欠采样的信息丢失问题</li>
</ul>
<h4 id="算法流程-3"><a href="#算法流程-3" class="headerlink" title="算法流程"></a>算法流程</h4><ul>
<li>输入：一个包含少数类阳性样本P和多数类阴性样本集N的训练集D，定义T是从N中抽取的子集个数，$s_i$是训练Adaboost基分类器$H_i$时的循环次数</li>
<li>输出：一个组合分类器H(x)</li>
<li>1.$f=\sqrt[(\mathrm{T}-1)]{\left(\frac{|\mathrm{P}|}{|\mathrm{N}|}\right)}$，$f$是每一层级的分类器$H_i$该达到的假阳性率(False Positive Rate)，即把多数类样本误分为少数类的错误率</li>
<li>for i = 1 to T:<ul>
<li>从多数类N中随机抽取一个样本子集$N_i$，使得$|N_i| = |P|$</li>
<li>使用少数类样本集P和样本子集$N_i$训练一个Adaboost分类器$H_i$($H_i$由$s_i$个基分类器$h_{i,j}$及其权重$\alpha_{i,j}$构成，$\theta_i$是$H_i$的调节参数)</li>
<li>$\mathrm{H}_{\mathrm{i}}(x)=\operatorname{sgn}(\sum\limits_{j=1}^{s_{i}} \alpha_{i, j} h_{i, j}(x)-\theta_{i})$</li>
<li>调节阈值$\theta_i$令$H_i$的FP率为$f$</li>
<li>移除多数类样本集N中所有被$H_i$正确分类的样本</li>
</ul>
</li>
<li>输出一个集成分类器</li>
<li>$\mathrm{H}(\mathrm{x})=\operatorname{sgn}\left(\sum\limits_{i=1}^{T} \sum\limits_{j=1}^{s_{l}} \alpha_{i, j} h_{i, j}(x)-\sum\limits_{i=1}^{T} \theta_{i}\right)$</li>
</ul>
<h2 id="NearMiss"><a href="#NearMiss" class="headerlink" title="NearMiss"></a>NearMiss</h2><ul>
<li>本质上是一种原型选择(prototype selection)方法，即从多数类样本中选取最具代表性的样本用于训练，主要是为了缓解随机欠采样中的信息丢失问题。Nearmiss采用了3中不同的启发式规则来选择样本<ul>
<li>NearMiss-1：选择到最近的K个少数类样本平均距离最近的多数类样本，考虑的是与最近的k个少数类样本的平均距离，是局部的。该方法得到的多数类样本分布是“不均衡”的，它倾向于在比较集中的少数类附近找到更多的多数类样本，而在孤立的(离群的)少数类附近找到更少的多数类样本，原因是该方法考虑的局部性质和平均距离</li>
<li>NearMiss-2：选择到最远的K个少数类样本平均距离最近的多数类样本，考虑的是与最远的k个少数类样本的平均距离，是全局的。实验结果表明该方法的不均衡分类性能最优</li>
<li>NearMiss-3：对于每个少数类样本选择K个最近的多数类样本，目的是保证每个少数类样本都被多数类样本包围，该方法会使每一个少数类样本附近都有足够多的多数类样本，显然这会使得模型的精确度高、召回率低</li>
</ul>
</li>
</ul>
<h2 id="原型生成-Prototype-generation"><a href="#原型生成-Prototype-generation" class="headerlink" title="原型生成(Prototype generation)"></a>原型生成(Prototype generation)</h2><ul>
<li>给定数据集S，原型生成算法将生成一个子集S’，其中|S’|&lt;|S|，但是子集并非来自于原始数据，而是由原始数据集生成，方法是聚类成|S’|个类，然后取其中心点</li>
</ul>
<h2 id="Data-Cleaning-Techniques"><a href="#Data-Cleaning-Techniques" class="headerlink" title="Data Cleaning Techniques"></a>Data Cleaning Techniques</h2><h3 id="Tomek-Links"><a href="#Tomek-Links" class="headerlink" title="Tomek Links"></a>Tomek Links</h3><h4 id="定义-3"><a href="#定义-3" class="headerlink" title="定义"></a>定义</h4><ul>
<li>给定一个样本对$(x_i, x_j)$，其中$x_{i} \in S_{m a j}, x_{j} \in S_{\min }$，记$d(x_i, x_j)$是样本$x_i$和样本$x_j$之间的距离，如果不存在任何样本$x_k$，使得$d\left(x_{i}, x_{k}\right)&lt;d\left(x_{i}, x_{j}\right)$，那么样本对$(x_i, x_j)$即称为Tomek Links。即Tomek links为相反类最近邻样本之间的一对连接</li>
<li>不属于Tomek Links的情况有这个少数类样本最近的样本是同一类</li>
</ul>
<h4 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h4><ul>
<li>如果两个样本来自Tomek Links，那么他们中的一个样本要么是噪声，要么它们都在两类的边界上</li>
</ul>
<h4 id="图示"><a href="#图示" class="headerlink" title="图示"></a>图示</h4><ul>
<li><img src="http://anki190912.xuexihaike.com/20201021171029.png?imageView2/2/h/300"></li>
</ul>
<h4 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h4><ul>
<li>欠采样：将Tomek Links中属于是多数类的样本剔除</li>
<li>数据清洗：将Tomek Links中的两个样本都剔除</li>
</ul>
<h3 id="ENN-edited-nearest-neighborhood"><a href="#ENN-edited-nearest-neighborhood" class="headerlink" title="ENN(edited nearest neighborhood)"></a>ENN(edited nearest neighborhood)</h3><ul>
<li>这种方法应用knn来编辑(edit)数据集，对于每一个要进行下采样的样本，那些绝大多数近邻样本不属于该类的样本会被移除，而绝大多数的近邻样本属于同一类的样本会被保留</li>
</ul>
<h1 id="综合采样-Oversampling-Undersampling"><a href="#综合采样-Oversampling-Undersampling" class="headerlink" title="综合采样(Oversampling + Undersampling)"></a>综合采样(Oversampling + Undersampling)</h1><h2 id="定义-4"><a href="#定义-4" class="headerlink" title="定义"></a>定义</h2><ul>
<li>先过采样，然后再进行数据的清洗</li>
</ul>
<h2 id="SMOTE-Tomek-Links"><a href="#SMOTE-Tomek-Links" class="headerlink" title="SMOTE+Tomek Links"></a>SMOTE+Tomek Links</h2><h3 id="算法流程-4"><a href="#算法流程-4" class="headerlink" title="算法流程"></a>算法流程</h3><ul>
<li>1.利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T</li>
<li>2.剔除T中的Tomek Links对</li>
</ul>
<h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul>
<li>普通的SMOTE方法生成的少数类样本是通过线性插值得到的，在平衡类别分布的同时也扩张了少数类的样本空间，产生的问题是可能原本属于多数类样本的空间被少数类“入侵”，容易造成模型的过拟合</li>
<li>Tomek Links对寻找的是那种噪声点或者边界点，可以很好地解决“入侵”问题，如图红色加号为SMOTE产生的少数类样本，可以看到红色样本“入侵”到原本属于多数类样本的空间，这种噪声数据问题可以通过Tomek Links很好地解决</li>
<li><img src="http://anki190912.xuexihaike.com/20201021171629.png?imageView2/2/h/150"></li>
</ul>
<h2 id="SMOTE-ENN"><a href="#SMOTE-ENN" class="headerlink" title="SMOTE+ENN"></a>SMOTE+ENN</h2><ul>
<li>1.利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T</li>
<li>2.对T中的每一个样本使用KNN(一般k取3)方法预测，若预测结果与实际类别标签不符，则剔除该样本</li>
</ul>
<h1 id="其它方法"><a href="#其它方法" class="headerlink" title="其它方法"></a>其它方法</h1><h2 id="基于异常检测的方法"><a href="#基于异常检测的方法" class="headerlink" title="基于异常检测的方法"></a>基于异常检测的方法</h2><ul>
<li>把小类的样本作为异常点(outliers)，因此该问题便转化为异常点检测(anomaly detection)与变化趋势检测问题(change detection)</li>
</ul>
<h2 id="分治ensemble"><a href="#分治ensemble" class="headerlink" title="分治ensemble"></a>分治ensemble</h2><ul>
<li>将大类中样本聚类到L个聚类中，然后训练L个分类器</li>
<li>每个分类器使用大类中的一个簇与所有的小类样本进行训练得到</li>
<li>最后对这L个分类器采取少数服从多数的方式对未知类别数据进行分类，如果是连续值，采用平均值</li>
</ul>
<h2 id="分层级ensemble"><a href="#分层级ensemble" class="headerlink" title="分层级ensemble"></a>分层级ensemble</h2><ul>
<li>使用原始数据集训练第一个学习器L1</li>
<li>将L1错分的数据集作为新的数据集训练L2</li>
<li>将L1和L2分类结果不一致的数据作为数据集训练L3</li>
<li>最后测试集上将三个分类器的结果汇总(结合这三个分类器，采用投票的方式来决定分类结果，因此只有当L2与L3都分类为false时，最终结果才为false，否则为true)</li>
</ul>
<h2 id="对小类错分进行加权惩罚"><a href="#对小类错分进行加权惩罚" class="headerlink" title="对小类错分进行加权惩罚"></a>对小类错分进行加权惩罚</h2><ul>
<li>对分类器的小类样本数据增加权值，降低大类样本的权重，从而使得分类器将重点集中在小类样本身上</li>
<li>一个具体做法是，在训练分类器时，若分类器将小类样本分错时，额外增加分类器一个小类样本分错代价，这个额外的代价可以使得分类器更加“关心”小类样本。如penalized-SVM和penalized-LDA算法</li>
<li>对小样本进行过采样(例如含L倍重复数据)，其实在计算小样本错分cost functions时会累加L倍的惩罚分数</li>
</ul>
<h2 id="尝试其它评价指标"><a href="#尝试其它评价指标" class="headerlink" title="尝试其它评价指标"></a>尝试其它评价指标</h2><ul>
<li>准确度这个评价指标在类别不均衡的分类任务中不够好，甚至会造成误导。可考虑更有说服力的评价指标。如混淆矩阵、精确度、召回率、F1得分，其中可关注Kappa和ROC曲线</li>
</ul>
<h2 id="尝试不同的分类算法"><a href="#尝试不同的分类算法" class="headerlink" title="尝试不同的分类算法"></a>尝试不同的分类算法</h2><ul>
<li>决策树在类别不均衡数据上表现不错。它使用基于类变量的划分规则去创建分类树，因此可以强制地将不同类别的样本分开</li>
<li>Lightgbm中有两个参数处理类别不平衡，分别是is_unbalance和scale_pos_weight</li>
<li>xgboost有一个参数类别不平衡，即scale_pos_weight</li>
</ul>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul>
<li><a href="https://www.jiqizhixin.com/articles/021704">机器学习中如何处理不平衡数据？</a></li>
<li><a href="https://blog.csdn.net/jiede1/article/details/70215477">SMOTE算法(人工合成数据)</a></li>
<li><a href="https://www.jianshu.com/p/13fc0f7f5565">SMOTE算法</a></li>
<li><a href="https://blog.csdn.net/anshuai_aw1/article/details/89177406">分类问题中类别不平衡问题的有效解决方法</a></li>
<li><a href="http://freewill.top/2017/04/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8817%EF%BC%89%EF%BC%9A%E9%9D%9E%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/">机器学习算法系列（17）：非平衡数据处理</a></li>
<li><a href="https://blog.csdn.net/weixin_44871660/article/details/90600522">样本不平衡处理</a></li>
<li><a href="https://imbalanced-learn.readthedocs.io/en/stable/index.html">imbalanced官网</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36093594">非平衡分类问题 | BalanceCascade方法及其Python实现</a></li>
<li><a href="https://blog.csdn.net/songhk0209/article/details/71484469">解决样本不平衡问题的奇技淫巧 汇总</a></li>
<li><a href="https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/">8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
        <tag>数据预处理</tag>
      </tags>
  </entry>
  <entry>
    <title>特征选择方法汇总</title>
    <url>/feature-selection/</url>
    <content><![CDATA[<h1 id="什么是特征选择"><a href="#什么是特征选择" class="headerlink" title="什么是特征选择"></a>什么是特征选择</h1><ul>
<li>对一个学习任务来说，给定属性集，有些属性很有用，另一些则可能没什么用。这里的属性即称为“特征”(feature)。对当前学习任务有用的属性称为“相关特征”(relevant feature)、没什么用的属性称为“无关特征”(irrelevant feature)。从给定的特征集合中选择出相关特征子集的过程，即“特征选择”(feature selection)</li>
</ul>
<h1 id="特征选择的目的"><a href="#特征选择的目的" class="headerlink" title="特征选择的目的"></a>特征选择的目的</h1><ul>
<li><p>1.简化模型，使模型更易于理解：去除不相关的特征会降低学习任务的难度。并且可解释性能对模型效果的稳定性有更多的把握</p>
</li>
<li><p>2.改善性能：节省存储和计算开销</p>
</li>
<li><p>3.改善通用性、降低过拟合风险：减轻维数灾难，特征的增多会大大增加模型的搜索空间，大多数模型所需要的训练样本随着特征数量的增加而显著增加。特征的增加虽然能更好地拟合训练数据，但也可能增加方差</p>
</li>
</ul>
<h1 id="使用特征选择的前提"><a href="#使用特征选择的前提" class="headerlink" title="使用特征选择的前提"></a>使用特征选择的前提</h1><ul>
<li><p>1.训练数据包含许多冗余或无用的特征，移除这些特征并不会导致丢失信息。其中冗余是指一个本身很有用的特征与另外一个有用的特征强相关，或它包含的信息能从其它特征推演出来</p>
</li>
<li><p>2.特征很多但样本相对较少</p>
</li>
</ul>
<h1 id="特征选择的4个步骤"><a href="#特征选择的4个步骤" class="headerlink" title="特征选择的4个步骤"></a>特征选择的4个步骤</h1><ul>
<li><p>1.产生过程：产生特征或特征子集候选集合</p>
</li>
<li><p>2.评价函数：衡量特征或特征子集的重要性或者好坏程度，即量化特征变量和目标变量之间的联系以及特征之间的相互联系。为了避免过拟合，可用交叉验证的方式来评估特征的好坏</p>
</li>
<li><p>3.停止准则：为了减少计算复杂度，需设定一个阈值，当评价函数值达到阈值后搜索停止</p>
</li>
<li><p>4.验证过程：在验证数据集上验证选出来的特征子集的有效性</p>
</li>
</ul>
<h1 id="特征选择的三个方法"><a href="#特征选择的三个方法" class="headerlink" title="特征选择的三个方法"></a>特征选择的三个方法</h1><ul>
<li><p>Filter(过滤法)</p>
</li>
<li><p>Wrapper(包装法)</p>
</li>
<li><p>Embedded(嵌入法)</p>
</li>
</ul>
<h1 id="Filter-过滤法"><a href="#Filter-过滤法" class="headerlink" title="Filter(过滤法)"></a>Filter(过滤法)</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><ul>
<li>按照发散性或相关性对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选，分为单变量过滤方法和多变量过滤方法</li>
</ul>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><ul>
<li><p>单变量过滤方法：不需要考虑特征之间的相互关系，按照特征变量和目标变量之间的相关性或互信息对特征进行排序，过滤掉最不相关的特征变量。优点是计算效率高、不易过拟合</p>
</li>
<li><p>多变量过滤方法：考虑特征之间的相互关系，常用方法有基于相关性和一致性的特征选择</p>
</li>
</ul>
<h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul>
<li>不依赖于任何机器学习方法，且不需要交叉验证，计算效率比较高</li>
</ul>
<h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ul>
<li>没有考虑机器学习算法的特点</li>
</ul>
<h2 id="图示"><a href="#图示" class="headerlink" title="图示"></a>图示</h2><ul>
<li><img src="http://anki190912.xuexihaike.com/20201014172426.png?imageView2/2/h/80"></li>
</ul>
<h2 id="常用的过滤方法"><a href="#常用的过滤方法" class="headerlink" title="常用的过滤方法"></a>常用的过滤方法</h2><h3 id="覆盖率"><a href="#覆盖率" class="headerlink" title="覆盖率"></a>覆盖率</h3><ul>
<li>即特征在训练集中出现的比例。若覆盖率很小，如有10000个样本，但某个特征只出现了5次，则次覆盖率对模型的预测作用不大，可删除</li>
</ul>
<h3 id="方差选择法"><a href="#方差选择法" class="headerlink" title="方差选择法"></a>方差选择法</h3><ul>
<li>先计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="comment"># 方差选择法，返回值为特征选择后的数据</span></span><br><span class="line"><span class="comment"># 参数threshold为方差的阈值</span></span><br><span class="line">VarianceThreshold(threshold=<span class="number">3</span>).fit_transform(iris.data)</span><br></pre></td></tr></table></figure>


<h3 id="Pearson-皮尔森-相关系数"><a href="#Pearson-皮尔森-相关系数" class="headerlink" title="Pearson(皮尔森)相关系数"></a>Pearson(皮尔森)相关系数</h3><ul>
<li>用于度量两个变量X和Y之间的线性相关性，结果的取值区间为[-1, 1]， -1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关性</li>
<li>计算方法为两个变量之间的协方差和标准差的商</li>
<li>$$\rho_{X, Y}=\frac{\operatorname{cov}(X, Y)}{\sigma_{X} \sigma_{Y}}=\frac{E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]}{\sigma_{X} \sigma_{Y}}$$</li>
<li>样本上的相关系数为</li>
<li>$$r=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}}$$</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="comment"># 选择K个最好的特征，返回选择特征后的数据</span></span><br><span class="line"><span class="comment"># 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，</span></span><br><span class="line"><span class="comment"># 输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。</span></span><br><span class="line"><span class="comment"># 在此为计算相关系数</span></span><br><span class="line"><span class="comment"># 其中参数k为选择的特征个数</span></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(map(<span class="keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, </span><br><span class="line">            k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h3 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h3><ul>
<li>检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量</li>
<li>$$\chi^{2}=\sum \frac{(A-E)^{2}}{E}$$</li>
<li>这个统计量的含义即自变量对因变量的相关性</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="comment">#选择K个最好的特征，返回选择特征后的数据</span></span><br><span class="line">SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h3 id="互信息法-KL散度、相对熵-和最大信息系数-Mutual-information-and-maximal-information-coefficient-MIC"><a href="#互信息法-KL散度、相对熵-和最大信息系数-Mutual-information-and-maximal-information-coefficient-MIC" class="headerlink" title="互信息法(KL散度、相对熵)和最大信息系数 Mutual information and maximal information coefficient (MIC)"></a>互信息法(KL散度、相对熵)和最大信息系数 Mutual information and maximal information coefficient (MIC)</h3><ul>
<li>评价定性自变量对定性因变量的相关性，评价类别型变量对类别型变量的相关性，互信息越大表明两个变量相关性越高，互信息为0时，两个变量相互独立。互信息的计算公式为</li>
<li>$$I(X ; Y)=\sum\limits_{x \in X} \sum\limits_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}=D_{K L}(p(x, y) | p(x) p(y))$$</li>
<li>其中，p(x)和p(y)为X和Y的边际概率分布函数，p(x,y)为X和Y的联合概率分布函数。直观上，互信息度量两个随机变量之间共享的信息，也可表示为由于X的引入而使Y的不确定性减少的量，这时互信息与信息增益相同</li>
<li>皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些</li>
<li>互信息不能直接用于特征选择的两个原因<ul>
<li>1.不属于度量方式，不能归一化，在不同数据上的结果不能做比较</li>
<li>2.对于连续变量的计算不是很方便(X和Y都是集合，$x_i,y$都是离散值)，通常变量需要先离散化，而互信息的结果对离散化的方式很敏感</li>
</ul>
</li>
<li>为了处理定量数据，提出了最大信息系数法，它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0, 1]</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> minepy <span class="keyword">import</span> MINE</span><br><span class="line"><span class="comment">#由于MINE的设计不是函数式的，定义mic方法将其为函数式的</span></span><br><span class="line"><span class="comment">#返回一个二元组，二元组的第2项设置成固定的P值0.5</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mic</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    m = MINE()</span><br><span class="line">    m.compute_score(x, y)</span><br><span class="line">    <span class="keyword">return</span> (m.mic(), <span class="number">0.5</span>)</span><br><span class="line"><span class="comment">#选择K个最好的特征，返回特征选择后的数据</span></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(map(<span class="keyword">lambda</span> x:mic(x, Y), X.T)).T, </span><br><span class="line">            k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h3 id="Fisher得分"><a href="#Fisher得分" class="headerlink" title="Fisher得分"></a>Fisher得分</h3><ul>
<li>对于分类问题，好的特征应该是在同一个类别中的取值比较相似，而在不同类别之间的取值差异比较大。因此特征i的重要性可用Fiser得分$S_i$来表示</li>
<li>$$S_{i}=\frac{\sum_{j=1}^{K} n_{j}\left(\mu_{i j}-\mu_{i}\right)^{2}}{\sum_{j=1}^{K} n_{j} \rho_{i j}^{2}}$$</li>
<li>其中，$u_{ij}$和$\rho_{ij}$分别是特征i在类别j中均值和方差，$\mu_i$为特征i的均值，$n_j$为类别j中的样本数。Fisher得分越高，特征在不同类别之间的差异性越大、在同一类别中的差异性越小，则特征越重要</li>
</ul>
<h3 id="相关特征选择-Correlation-Feature-Selection-CFS"><a href="#相关特征选择-Correlation-Feature-Selection-CFS" class="headerlink" title="相关特征选择(Correlation Feature Selection, CFS)"></a>相关特征选择(Correlation Feature Selection, CFS)</h3><ul>
<li>该方法基于的假设是，好的特征集合包含跟目标变量非常相关的特征，但这些特征之间彼此不相关。对于包含k个特征的集合，CFS准则定义为</li>
<li>$$\mathrm{CFS}=\max \limits_{S_{k}}\left[\frac{r_{c f_{1}}+r_{c f_{2}}+\cdots+r_{c f_{k}}}{\sqrt{k+2\left(r_{f_{1} f_{2}}+\cdots+r_{f_{i} f_{j}}+\cdots+r_{f_{k} f_{1}}\right)}}\right]$$</li>
<li>其中，$r_{cf_i}$和$r_{f_i f_j}$是特征变量和目标变量之间，以及特征变量和特征变量之间的相关性，这里的相关性不一定是皮尔森相关系数或斯皮尔曼相关系数</li>
</ul>
<h3 id="最小冗余最大相关性-Minimum-Redundancy-Maximum-Relevance-mRMR"><a href="#最小冗余最大相关性-Minimum-Redundancy-Maximum-Relevance-mRMR" class="headerlink" title="最小冗余最大相关性(Minimum Redundancy Maximum Relevance, mRMR)"></a>最小冗余最大相关性(Minimum Redundancy Maximum Relevance, mRMR)</h3><ul>
<li>由于单变量过滤法只考虑了单特征变量和目标变量之间的相关性，因此选择的特征子集可能过于冗余。mRMR在进行特征时考虑到了特征之间的冗余性，具体做法是对跟已选择特征相关性较高的冗余特征进行惩罚</li>
<li>mRMR可以使用多种相关性的度量指标，如互信息、相关系数以及其它距离或相似度分数</li>
<li>以互信息为例，特征集合S和目标变量c之间的相关性可定义为，特征集合中所有单个特征变量$f_i$和目标变量c的互信息值$I(f_i;c)$的平均值：</li>
<li>$$D(S, c)=\frac{1}{|S|} \sum\limits_{f_{i} \in S} I\left(f_{i} ; c\right)$$</li>
<li>S中所有特征的冗余性为所有特征变量之间的互信息$I(f_i;f_i)$的平均值</li>
<li>$$R(S)=\frac{1}{|S|^{2}} \sum\limits_{f_{i}, f_{j} \in S} I\left(f_{i} ; f_{j}\right)$$</li>
<li>则mRMR准则为</li>
<li>$$\operatorname{mRMR}=\max \limits_{S}[D(S, c)-R(S)]$$</li>
<li>通过求解上述优化问题即可得到特征子集</li>
<li>在一些特定的情况下，mRMR算法可能对特征的重要性估计不足，它没有考虑到特征之间的组合可能与目标变量比较相关。如果单个特征的分类能力都比较弱，但进行组合后分类能力很强，这时mRMR方法效果一般比较差(如目标变量由特征变量之间进行XOR运算得到)</li>
<li>mRMR是一种典型的进行特征选择的增量贪心策略：某个特征一旦被选择了，在后续的步骤不会删除</li>
<li>mRMR可改写为全局的二次规划的优化问题(即特征集合为特征全集的情况)：</li>
<li>$$\mathrm{QPFS}=\min\limits_{x}\left[\alpha \boldsymbol{x}^{\mathrm{T}} \boldsymbol{H} \boldsymbol{x}-\boldsymbol{x}^{\mathrm{T}} \boldsymbol{F}\right] \mathrm{s.t.} \sum\limits_{i=1}^{n} x_{i}=1, x_{i} \geqslant 0$$</li>
<li>其中$\boldsymbol{F}$为特征变量和目标变量相关性向量，$\boldsymbol{H}$为度量特征变量之间的冗余性的矩阵。QPFS可通过二次规划求解，QPFS偏向于选择熵比较小的特征，这是因为特征自身的冗余性$I(f_i;f_j)$</li>
<li>另一种全局的基于互信息的方法是基于条件相关性的</li>
<li>$$\mathrm{SPEC}_{\mathrm{CMI}}=\max\limits_{x}\left[\boldsymbol{x}^{\mathrm{T}} \boldsymbol{Q} \boldsymbol{x}\right] \text { s.t. }|x|=1, x_{i} \geqslant 0$$</li>
<li>其中，$Q_{i i}=I\left(f_{i} ; c\right), Q_{i j}=I\left(f_{i} ; c \mid f_{j}\right), i \neq j$。$\mathrm{SPEC}_{\mathrm{CMI}}$方法的优点是可以通过求解矩阵Q的主特征向量来求解，而且可以处理二阶的特征组合</li>
</ul>
<h1 id="Wrapper-包装法"><a href="#Wrapper-包装法" class="headerlink" title="Wrapper(包装法)"></a>Wrapper(包装法)</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><ul>
<li>使用机器学习算法评估特征子集的效果，可以检测两个或多个特征之间的交互关系，而且选择的特征子集让模型的效果达到最优。</li>
<li>这是特征子集搜索和评估指标相结合的方法。前者提供候选的新特征子集，后者基于新特征子集训练一个模型，并用验证集进行评估，为每一组特征子集进行打分</li>
<li>最简单的方法是在每一个特征子集上训练并评估模型，从而找出最优的特征子集</li>
</ul>
<h2 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h2><ul>
<li>1.需要对每一组特征子集训练一个模型，计算量很大</li>
<li>2.样本不够充分的情况下容易过拟合</li>
<li>3.特征变量较多时计算复杂度太高</li>
</ul>
<h2 id="图示-1"><a href="#图示-1" class="headerlink" title="图示"></a>图示</h2><p><img src="http://anki190912.xuexihaike.com/20201014173616.png?imageView2/2/h/120"></p>
<h2 id="3种常用的特征子集搜索方法"><a href="#3种常用的特征子集搜索方法" class="headerlink" title="3种常用的特征子集搜索方法"></a>3种常用的特征子集搜索方法</h2><h3 id="1-完全搜索"><a href="#1-完全搜索" class="headerlink" title="1.完全搜索"></a>1.完全搜索</h3><ul>
<li>即穷举法，遍历所有可能的组合达到全局最优，时间复杂度$2^n$</li>
</ul>
<h3 id="2-启发式搜索"><a href="#2-启发式搜索" class="headerlink" title="2.启发式搜索"></a>2.启发式搜索</h3><ul>
<li>序列向前选择：特征子集从空集开始，每次只加入一个特征，时间复杂度为$O(n+(n-1)+(n-2)+\ldots+1)=O\left(n^{2}\right)$</li>
<li>序列向后选择：特征子集从全集开始，每次删除一个特征，时间复杂度为$O(n^{2})$</li>
</ul>
<h3 id="3-随机搜索"><a href="#3-随机搜索" class="headerlink" title="3.随机搜索"></a>3.随机搜索</h3><ul>
<li>执行序列向前或向后选择时，随机选择特征子集</li>
</ul>
<h3 id="4-递归特征消除法"><a href="#4-递归特征消除法" class="headerlink" title="4.递归特征消除法"></a>4.递归特征消除法</h3><ul>
<li>使用一个基模型进行多轮训练，每轮训练后通过学习器返回的coef_或者feature_importances_消除若干权重较低的特征，再基于新的特征集进行下一轮训练</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment">#递归特征消除法，返回特征选择后的数据</span></span><br><span class="line"><span class="comment">#参数estimator为基模型</span></span><br><span class="line"><span class="comment">#参数n_features_to_select为选择的特征个数</span></span><br><span class="line">RFE(estimator=LogisticRegression(), </span><br><span class="line">    n_features_to_select=<span class="number">2</span>).fit_transform(iris.data, </span><br><span class="line">                                          iris.target)</span><br></pre></td></tr></table></figure>


<h1 id="Embedded-嵌入法"><a href="#Embedded-嵌入法" class="headerlink" title="Embedded(嵌入法)"></a>Embedded(嵌入法)</h1><h2 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h2><ul>
<li>将特征选择嵌入到模型的构建过程中，具有包装法与机器学习算法相结合的优点，也具有过滤法计算效率高的优点</li>
</ul>
<h2 id="图示-2"><a href="#图示-2" class="headerlink" title="图示"></a>图示</h2><ul>
<li><img src="http://anki190912.xuexihaike.com/20201014183741.png?imageView2/2/h/120"></li>
</ul>
<h2 id="LASSO方法"><a href="#LASSO方法" class="headerlink" title="LASSO方法"></a>LASSO方法</h2><ul>
<li>使用LASSO(Least Absolute Shrinkage and Selection Operator)方法</li>
<li>$$\min\limits_{\beta \in \mathbb{R}^{p}}\{\frac{1}{N}|y-X \boldsymbol{\beta}|_{2}^{2}+\lambda|\boldsymbol{\beta}| _{1}\}$$</li>
<li>通过对回归系数添加$L_1$惩罚项来防止过拟合，可以让特定的回归系数变为0，从而可以选择一个不包含那些系数的更简单的模型</li>
<li>实际应用中，$\lambda$越大，回归系数越稀疏，$\lambda$一般采用交叉验证的方式来确定</li>
<li>线性回归、逻辑回归、FM/FFM以及神经网络都可以添加$L_1$惩罚项</li>
<li>即使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维</li>
<li>实际上，L1惩罚项降维的原理是，在多个对目标值具有同等相关性的特征中，只保留一个，所以没保留的特征并不代表不重要</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment">#带L1惩罚项的逻辑回归作为基模型的特征选择</span></span><br><span class="line">SelectFromModel(LogisticRegression(</span><br><span class="line">          penalty=<span class="string">&quot;l1&quot;</span>, C=<span class="number">0.1</span>)).fit_transform(</span><br><span class="line">               iris.data,iris.target)</span><br></pre></td></tr></table></figure>


<h2 id="基于树模型的特征选择方法"><a href="#基于树模型的特征选择方法" class="headerlink" title="基于树模型的特征选择方法"></a>基于树模型的特征选择方法</h2><ul>
<li>在决策树中，深度较浅的节点一般对应的特征分类能力更强(可以将更多的样本区分开)</li>
<li>对于基于决策树的算法，如随机森林，重要的特征更有可能出现在深度较浅的节点，而且出现的次数可能越多</li>
<li>即可基于树模型中特征出现次数等指标对特征进行重要性排序<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line">  <span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">  <span class="comment">#GBDT作为基模型的特征选择</span></span><br><span class="line">SelectFromModel(</span><br><span class="line">      GradientBoostingClassifier()).fit_transform(</span><br><span class="line">        iris.data,iris.target)</span><br></pre></td></tr></table></figure>


</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><p>美团机器学习实践2.2节</p>
</li>
<li><p>精通特征工程2.6节</p>
</li>
<li><p>特征工程入门与实践第5章</p>
</li>
<li><p>机器学习-周志华第11章</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/74198735">【机器学习】特征选择(Feature Selection)方法汇总</a></p>
</li>
<li><p><a href="https://www.zhihu.com/question/28641663">机器学习中，有哪些特征选择的工程方法？</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>第一篇博客：为什么我要建博客和写博客</title>
    <url>/first-blog/</url>
    <content><![CDATA[<h1 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h1><p>很久之前就想搭建自己的博客了，只是嘛，一直都没有时间（划掉，就是懒）。这一次之所以突然下定决心要搞一波，完全是因为我的同桌真的是个超级大神，北大本科，杜克大学博士，也有一个自己的博客和介绍页，看着她的介绍页真的是超级牛，超级强，而且不仅是学习强，还在自己感兴趣的各个领域钻研的很深。让我不禁连连感叹，大牛的人生真的是超级强，真可谓最强鸡血，对比下来仿佛我白活了这么多年，没有留下任何值得留存的记录。</p>
<p>所以，受到这个启发（主要是刺激），我也要开始搭建自己的博客和个人主页，但同时我觉得我最好同时发布和维护3个平台，一个是CSDN，一个是公众号，一个是这个博客。毕竟文章写好之后，多发布一下几乎不费时间。</p>
<h1 id="为什么要选择github和hexo"><a href="#为什么要选择github和hexo" class="headerlink" title="为什么要选择github和hexo"></a>为什么要选择github和hexo</h1><p>选择github主要是因为能借助git的版本管理，顺便可以在github的热力图刷的好看一点，还有托管在github免费哇，不需要再另外维护云主机，不想当运维。省一笔主机钱，我只需要出钱买个域名即可，万网这个.tech域名买了10年也才199块，就是整个博客搭建中的唯一花销了</p>
<p>选择hexo是因为，能支持markdown的书写，和我现有的工具套件能配套上，可以无缝迁移过来，hexo的生态和主题都相对完善。</p>
<h1 id="过程和踩坑"><a href="#过程和踩坑" class="headerlink" title="过程和踩坑"></a>过程和踩坑</h1><h2 id="申请域名"><a href="#申请域名" class="headerlink" title="申请域名"></a>申请域名</h2><p>直接上<a href="https://wanwang.aliyun.com/">万网</a>购买自己的域名，做完实名认证之后即可先放着，详细步骤具体参考<a href="https://zhuanlan.zhihu.com/p/103860494">知乎</a>。</p>
<h2 id="安装node和hexo，并部署到github"><a href="#安装node和hexo，并部署到github" class="headerlink" title="安装node和hexo，并部署到github"></a>安装node和hexo，并部署到github</h2><p>具体参考<a href="https://zhuanlan.zhihu.com/p/105715224">知乎</a>，我是安装在macOS上，不需要搞这里面复杂的各种环境变量。</p>
<p>踩坑：我原本以为是建完git仓库后，把仓库pull下来，在里面初始化hexo，但后面看了一下，是要在空文件夹操作，并且后续发布到github的文件是hexo进行编译后的文件。</p>
<h2 id="域名解析"><a href="#域名解析" class="headerlink" title="域名解析"></a>域名解析</h2><p>能够部署之后保证通过github.io能访问即可做域名解析，具体参考<a href="https://zhuanlan.zhihu.com/p/103813944">知乎</a></p>
<h2 id="挑选主题"><a href="#挑选主题" class="headerlink" title="挑选主题"></a>挑选主题</h2><p>原本我想直接在<a href="https://hexo.io/themes/">官方的主题链接</a>里挑一个比较合适的，给自己定了几个标准：</p>
<ol>
<li>整体必须是简洁的，那种大量有图片装饰的，背景花哨的不考虑（原因是，挑图片暴露自己的垃圾审美，还要给每个博客挑配图太费心力了）</li>
<li>必须能支持公式、代码块高亮等的解析</li>
<li>偏好整体布局要简洁，偏好侧边栏在右边，并且偏好文章要有侧边栏</li>
<li>主题必须有开发者长期维护和更新</li>
<li>能有评论系统</li>
</ol>
<p>在上面看花了眼，都没有一个不合适的，看了大半天，猛然觉得自己挑选的思路不对，在最原始的未经过筛选的主题站里挑选，能不费劲吗？</p>
<p>转换思路，直接搜推荐的hexo主题，然后看到next主题是几乎完全符合我的要求的，然后发现next主题经历了好几个大版本的迭代，甚至github仓库都换了几次，直接上最新的8.0版本，拉下来</p>
<h2 id="next主题各种调整优化"><a href="#next主题各种调整优化" class="headerlink" title="next主题各种调整优化"></a>next主题各种调整优化</h2><p>next主题中可以进行自主化调整的地方还挺多的，而且8.0版本中，很多地方和以往版本中有不一样的调整方式，我尽量把我用到的写一下。所做的所有操作基本是改一下themes/next下的_config.yml，很少一部分是更改hexo下的_config.yml，偶尔会使用npm装个包</p>
<h3 id="设置首页信息"><a href="#设置首页信息" class="headerlink" title="设置首页信息"></a>设置首页信息</h3><figure class="highlight yaml"><figcaption><span>hexo/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">每天净瞎搞</span></span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">&#x27;关注：AI/CS/数学/自我提升等&#x27;</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&#x27;既然选择了远方，便只顾风雨兼程&#x27;</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">Shiqi</span> <span class="string">Lu</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">zh-CN</span></span><br><span class="line"><span class="attr">timezone:</span> <span class="string">&#x27;Asia/Shanghai&#x27;</span></span><br><span class="line"><span class="attr">url:</span> <span class="string">http://shiqi-lu.tech</span></span><br></pre></td></tr></table></figure>

<h3 id="风格选择"><a href="#风格选择" class="headerlink" title="风格选择"></a>风格选择</h3><p>我把四个风格都试了一遍，最后比较喜欢Gemini</p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Schemes</span></span><br><span class="line"><span class="comment"># scheme: Muse</span></span><br><span class="line"><span class="comment"># scheme: Mist</span></span><br><span class="line"><span class="comment"># scheme: Pisces</span></span><br><span class="line"><span class="attr">scheme:</span> <span class="string">Gemini</span></span><br></pre></td></tr></table></figure>

<h3 id="支持暗黑模式"><a href="#支持暗黑模式" class="headerlink" title="支持暗黑模式"></a>支持暗黑模式</h3><p>这可是个意外惊喜，还会根据系统的设置自动适配</p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Dark Mode</span></span><br><span class="line"><span class="attr">darkmode:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h3 id="设置建站时间"><a href="#设置建站时间" class="headerlink" title="设置建站时间"></a>设置建站时间</h3><figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">footer:</span></span><br><span class="line">  <span class="comment"># Specify the date when the site was setup. If not defined, current year will be used.</span></span><br><span class="line">  <span class="attr">since:</span> <span class="number">2020</span></span><br></pre></td></tr></table></figure>

<h3 id="设置网站脚注的信息（图标、备案等）"><a href="#设置网站脚注的信息（图标、备案等）" class="headerlink" title="设置网站脚注的信息（图标、备案等）"></a>设置网站脚注的信息（图标、备案等）</h3><figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">footer:</span></span><br><span class="line">  <span class="comment"># Icon between year and copyright info.</span></span><br><span class="line">  <span class="attr">icon:</span></span><br><span class="line">    <span class="comment"># Icon name in Font Awesome. See: https://fontawesome.com/icons</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">fa</span> <span class="string">fa-heart</span></span><br><span class="line">    <span class="comment"># If you want to animate the icon, set it to true.</span></span><br><span class="line">    <span class="attr">animated:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># Change the color of icon, using Hex Code.</span></span><br><span class="line">    <span class="attr">color:</span> <span class="string">&quot;#808080&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="网站图标"><a href="#网站图标" class="headerlink" title="网站图标"></a>网站图标</h3><p>先到网上找适合的图标，然后更新一下对应的文件，免费的图标素材网站：<a href="https://www.easyicon.net/1220579-maple_leaf_icon.html">Easyicon</a></p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">favicon:</span></span><br><span class="line">  <span class="attr">small:</span> <span class="string">/images/7-16.png</span></span><br><span class="line">  <span class="attr">medium:</span> <span class="string">/images/7-32.png</span></span><br><span class="line">  <span class="attr">apple_touch_icon:</span> <span class="string">/images/7-128.png</span></span><br><span class="line">  <span class="attr">safari_pinned_tab:</span> <span class="string">/images/7-128.png</span></span><br></pre></td></tr></table></figure>

<h3 id="标签页和分类页"><a href="#标签页和分类页" class="headerlink" title="标签页和分类页"></a>标签页和分类页</h3><p>参考<a href="https://theme-next.js.org/docs/theme-settings/custom-pages.html#Adding-%C2%ABTags%C2%BB-Page">next文档</a></p>
<h3 id="侧边栏"><a href="#侧边栏" class="headerlink" title="侧边栏"></a>侧边栏</h3><p>我喜欢放在右边，主要是因为视觉聚焦主要是在左边的</p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">sidebar:</span></span><br><span class="line">  <span class="comment"># Sidebar Position.</span></span><br><span class="line">  <span class="comment"># position: left</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">right</span></span><br></pre></td></tr></table></figure>

<h3 id="打开文章标题下方更新时间、阅读时长等信息"><a href="#打开文章标题下方更新时间、阅读时长等信息" class="headerlink" title="打开文章标题下方更新时间、阅读时长等信息"></a>打开文章标题下方更新时间、阅读时长等信息</h3><p>参考<a href="https://theme-next.js.org/docs/theme-settings/posts.html#Post-Wordcount">官方文档</a><br>先按照npm包：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ npm install hexo-word-counter</span><br><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><figcaption><span>hexo/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">symbols_count_time:</span></span><br><span class="line">  <span class="attr">symbols:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">total_symbols:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">total_time:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Post meta display settings</span></span><br><span class="line"><span class="attr">post_meta:</span></span><br><span class="line">  <span class="attr">item_text:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">created_at:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">updated_at:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">another_day:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">categories:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Post wordcount display settings</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/next-theme/hexo-word-counter</span></span><br><span class="line"><span class="attr">symbols_count_time:</span></span><br><span class="line">  <span class="attr">separated_meta:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">item_text_total:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h3 id="博客首页的摘要设置"><a href="#博客首页的摘要设置" class="headerlink" title="博客首页的摘要设置"></a>博客首页的摘要设置</h3><p>这个要配合文章中的description字段，或在文章中添加一行注释辅助，参考<a href="https://theme-next.js.org/docs/theme-settings/posts.html?highlight=more#Preamble-Text">官方文档</a></p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Automatically excerpt description in homepage as preamble text.</span></span><br><span class="line"><span class="attr">excerpt_description:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Read more button</span></span><br><span class="line"><span class="comment"># If true, the read more button will be displayed in excerpt section.</span></span><br><span class="line"><span class="attr">read_more_btn:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h3 id="置顶的百分比和顶部进度条"><a href="#置顶的百分比和顶部进度条" class="headerlink" title="置顶的百分比和顶部进度条"></a>置顶的百分比和顶部进度条</h3><p>默认给的颜色有点花哨，我改成了灰色</p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">back2top:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Back to top in sidebar.</span></span><br><span class="line">  <span class="attr">sidebar:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># Scroll percent label in b2t button.</span></span><br><span class="line">  <span class="attr">scrollpercent:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reading progress bar</span></span><br><span class="line"><span class="attr">reading_progress:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Available values: top | bottom</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">top</span></span><br><span class="line">  <span class="comment"># color: &quot;#37c6c0&quot;</span></span><br><span class="line">  <span class="attr">color:</span> <span class="string">&quot;#808080&quot;</span></span><br><span class="line">  <span class="attr">height:</span> <span class="string">3px</span></span><br></pre></td></tr></table></figure>

<h3 id="头像设置"><a href="#头像设置" class="headerlink" title="头像设置"></a>头像设置</h3><p>在url里放置本地图片或者图床链接</p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Sidebar Avatar</span></span><br><span class="line"><span class="attr">avatar:</span></span><br><span class="line">  <span class="comment"># Replace the default image and set the url here.</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">/images/7-128.png</span></span><br><span class="line">  <span class="comment"># If true, the avatar will be dispalyed in circle.</span></span><br><span class="line">  <span class="attr">rounded:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># If true, the avatar will be rotated with the cursor.</span></span><br><span class="line">  <span class="attr">rotated:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<h3 id="代码块高亮风格选择"><a href="#代码块高亮风格选择" class="headerlink" title="代码块高亮风格选择"></a>代码块高亮风格选择</h3><p>使用了hightlight.js的高亮样式</p>
<figure class="highlight yaml"><figcaption><span>hexo/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">highlight:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">line_number:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">auto_detect:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">tab_replace:</span> <span class="string">&#x27;    &#x27;</span></span><br><span class="line">  <span class="attr">wrap:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hljs:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">prismjs:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">preprocess:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">line_number:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">tab_replace:</span> <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">codeblock:</span></span><br><span class="line">  <span class="comment"># Code Highlight theme</span></span><br><span class="line">  <span class="comment"># All available themes: https://theme-next.js.org/highlight/</span></span><br><span class="line">  <span class="attr">theme:</span></span><br><span class="line">    <span class="attr">light:</span> <span class="string">default</span></span><br><span class="line">    <span class="attr">dark:</span> <span class="string">tomorrow-night-bright</span></span><br><span class="line">  <span class="attr">prism:</span></span><br><span class="line">    <span class="attr">light:</span> <span class="string">prism</span></span><br><span class="line">    <span class="attr">dark:</span> <span class="string">prism-dark</span></span><br><span class="line">  <span class="comment"># Add copy button on codeblock</span></span><br><span class="line">  <span class="attr">copy_button:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># Available values: default | flat | mac</span></span><br><span class="line">    <span class="attr">style:</span> <span class="string">flat</span></span><br></pre></td></tr></table></figure>

<h3 id="社交账号设置"><a href="#社交账号设置" class="headerlink" title="社交账号设置"></a>社交账号设置</h3><figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">social:</span></span><br><span class="line">  <span class="attr">GitHub:</span> <span class="string">https://github.com/shiqi-lu</span> <span class="string">||</span> <span class="string">fab</span> <span class="string">fa-github</span></span><br><span class="line">  <span class="attr">E-Mail:</span> <span class="string">mailto:traumlou@163.com</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-envelope</span></span><br><span class="line">  <span class="comment">#Weibo: https://weibo.com/yourname || fab fa-weibo</span></span><br><span class="line">  <span class="comment">#Google: https://plus.google.com/yourname || fab fa-google</span></span><br><span class="line">  <span class="comment">#Twitter: https://twitter.com/yourname || fab fa-twitter</span></span><br><span class="line">  <span class="comment">#FB Page: https://www.facebook.com/yourname || fab fa-facebook</span></span><br><span class="line">  </span><br><span class="line"><span class="attr">social_icons:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">icons_only:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">transition:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h3 id="支持本地搜索"><a href="#支持本地搜索" class="headerlink" title="支持本地搜索"></a>支持本地搜索</h3><p>参考<a href="https://theme-next.js.org/docs/third-party-services/search-services.html?highlight=search#Local-Search">官方文档</a><br>先装包：<code>$ npm install hexo-generator-searchdb </code></p>
<figure class="highlight yaml"><figcaption><span>hexo/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Local Search</span></span><br><span class="line"><span class="attr">search:</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">search.xml</span></span><br><span class="line">  <span class="attr">field:</span> <span class="string">post</span></span><br><span class="line">  <span class="attr">content:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">format:</span> <span class="string">html</span></span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Local search</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/next-theme/hexo-generator-searchdb</span></span><br><span class="line"><span class="attr">local_search:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># If auto, trigger search by changing input.</span></span><br><span class="line">  <span class="comment"># If manual, trigger search by pressing enter key or search button.</span></span><br><span class="line">  <span class="attr">trigger:</span> <span class="string">auto</span></span><br><span class="line">  <span class="comment"># Show top n results per article, show all results by setting to -1</span></span><br><span class="line">  <span class="attr">top_n_per_article:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment"># Unescape html strings to the readable one.</span></span><br><span class="line">  <span class="attr">unescape:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># Preload the search data when the page loads.</span></span><br><span class="line">  <span class="attr">preload:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<h3 id="评论系统"><a href="#评论系统" class="headerlink" title="评论系统"></a>评论系统</h3><p>评论系统选择了<a href="https://valine.js.org/">valine</a>，请参考<a href="https://theme-next.js.org/docs/third-party-services/comments.html?highlight=comme#Valine-China">next文档</a>，其中头像需要注册一下Gravatar，参考<a href="https://valine.js.org/avatar.html">头像配置</a>，这里的邮箱提醒好像有问题，官方说明的方法不能用了。这个以后再说吧，我也不想有个评论就给我发邮件，要真有比较紧急的事情，直接发我邮箱吧</p>
<h3 id="文章置顶"><a href="#文章置顶" class="headerlink" title="文章置顶"></a>文章置顶</h3><p>注意这里8.0更新之后，就不是通过安装插件改源码的方式实现，直接在文章的front-matter里面添加一个字段：sticky就可以实现了，值越高排的越前，默认为0是按照时间顺序，参考<a href="https://theme-next.js.org/docs/advanced-settings/front-matter.html?highlight=stick">官方文档</a></p>
<h3 id="文章赞赏"><a href="#文章赞赏" class="headerlink" title="文章赞赏"></a>文章赞赏</h3><p>要先准备好微信，支付宝等的二维码，然后放在images下或放在图床中</p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Donate (Sponsor) settings</span></span><br><span class="line"><span class="comment"># Front-matter variable (unsupport animation).</span></span><br><span class="line"><span class="attr">reward_settings:</span></span><br><span class="line">  <span class="comment"># If true, a donate button will be displayed in every article by default.</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">animation:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">comment:</span> <span class="string">觉得文章写得不错就请博主喝杯奶茶吧(*￣∇￣*)</span></span><br><span class="line"></span><br><span class="line"><span class="attr">reward:</span></span><br><span class="line">  <span class="attr">wechatpay:</span> <span class="string">/images/wechatpay.png</span></span><br><span class="line">  <span class="attr">alipay:</span> <span class="string">/images/alipay.png</span></span><br><span class="line">  <span class="comment">#paypal: /images/paypal.png</span></span><br><span class="line">  <span class="comment">#bitcoin: /images/bitcoin.png</span></span><br></pre></td></tr></table></figure>

<h1 id="尚未完成部分"><a href="#尚未完成部分" class="headerlink" title="尚未完成部分"></a>尚未完成部分</h1><p>这部分以后看时间和心情做吧，每做一部分记录一部分吧</p>
<ul>
<li>SEO</li>
<li>个人简介</li>
<li>README</li>
<li>访问速度比较慢，考虑使用除github外的托管服务</li>
<li>考虑使用CI</li>
<li>考虑CDN加速</li>
<li>考虑把http转换成https</li>
<li>备案</li>
<li>图床替换成自己的域名</li>
<li>完善和链接一下领英</li>
<li>研究一下博客如何分享链接到微信</li>
</ul>
<h1 id="已知问题"><a href="#已知问题" class="headerlink" title="已知问题"></a>已知问题</h1><ul>
<li>在ipad上的safari显示的时候没有font awesome图标显示，文章内容侧边栏等显示不出来，但ipad的chrome没问题，iphone的safari也没问题，真是奇怪</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://kchen.cc/2016/11/12/hexo-instructions/">基于 Hexo 的全自动博客构建部署系统</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/102592286">从零开始搭建个人博客（超详细）</a></li>
<li><a href="https://hexo.io/zh-cn/docs/">Hexo官方文档</a></li>
<li><a href="https://github.com/next-theme/hexo-theme-next">Next8.0 Github</a></li>
<li><a href="https://theme-next.js.org/">Next8.0 文档</a></li>
</ul>
]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>感想</tag>
        <tag>自我提升</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习项目清单</title>
    <url>/ml-checklist/</url>
    <content><![CDATA[<h1 id="1-框出问题并看整体"><a href="#1-框出问题并看整体" class="headerlink" title="1.框出问题并看整体"></a>1.框出问题并看整体</h1><ul>
<li>1.用业务术语定义目标。</li>
<li>2.你的解决方案将如何使用?</li>
<li>3.当前有什么解决方案/解决方法（如果有）?</li>
<li>4.你应该如何阐述这个问题（有监督/无监督，在线/离线等）？</li>
<li>5.应该如何衡量性能?</li>
<li>6.性能指标是否符合业务目标?</li>
<li>7.达到业务目标所需的最低性能是多少？</li>
<li>8.有没有一些相似的向题？你可以重用经验或工具吗？</li>
<li>9.有没有相关有经验的人？</li>
<li>10.你会如何手动解决问题？</li>
<li>11.列出你（或其他人）到目前为止所做的假设。</li>
<li>12.如果可能，请验证假设。</li>
</ul>
<h1 id="2-获取数据"><a href="#2-获取数据" class="headerlink" title="2.获取数据"></a>2.获取数据</h1><ul>
<li>注意：尽可能地自动化，以便你可以轻松地获取新数据</li>
<li>1.列出所需的数据以及你需要多少数据。</li>
<li>2.查找并记录可从何处获取该数据。</li>
<li>3.检查将占用多少空间。</li>
<li>4.检查法律义务，并在必要时获得授权。</li>
<li>5.获取访问授权。</li>
<li>6.创建一个工作空间（具有足够的存储空间）。</li>
<li>7.获取数据。</li>
<li>8.将数据转换为可以轻松操作的格式（无须更改数据本身）。</li>
<li>9.确保敏感信息被删除或受保护（例如匿名）。</li>
<li>10.检查数据的大小和类型（时间序列、样本、地理等）。</li>
<li>11.抽样一个测试集，将其放在一边，再也不要看它（无数据监听！）。</li>
</ul>
<h1 id="3-研究数据"><a href="#3-研究数据" class="headerlink" title="3.研究数据"></a>3.研究数据</h1><ul>
<li>注意：请尝试从现场专家那里获取有关这些步骤的见解。</li>
<li>1.创建数据副本来进行研究（必要时将其采样到可以管理的大小）。</li>
<li>2.创建Jupyter notebook以记录你的数据研究。</li>
<li>3.研究每个属性及其特征：<ul>
<li>名称</li>
<li>类型（分类、整数/浮点型、有界/无界、文本、结构化等）</li>
<li>缺失值的百分比</li>
<li>噪声和噪声类型（随机、异常值、舍入误差等）</li>
<li>任务的实用性</li>
<li>分布类型（高斯分布、均匀分布、对数分布等）</li>
</ul>
</li>
<li>4.对于有监督学习任务，请确定目标属性。</li>
<li>5.可视化数据。</li>
<li>6.研究属性之间的相关性。</li>
<li>7.研究如何手动解决问题。</li>
<li>8.确定你可能希望使用的转变。</li>
<li>9.确定有用的额外数据。</li>
<li>10.记录所学的知识。</li>
</ul>
<h1 id="4-准备数据"><a href="#4-准备数据" class="headerlink" title="4.准备数据"></a>4.准备数据</h1><ul>
<li>注意：<ul>
<li>在数据副本上工作（保持原始数据集完整）。</li>
<li>为你应用的所有数据转换编写函数，原因有5个：<ul>
<li>下次获取新的数据集时，你可以轻松准备数据。</li>
<li>可以在未来的项目中应用这些转换。</li>
<li>清理并准备测试集。</li>
<li>解决方案上线后清理并准备新的数据实例。</li>
<li>使你可以轻松地将准备选择视为超参数。</li>
</ul>
</li>
</ul>
</li>
<li>1.数据清理：<ul>
<li>修复或删除异常值（可选）。</li>
<li>填写缺失值（例如，零、均值、中位数）或删除其行（或列）。</li>
</ul>
</li>
<li>2.特征选择（可选）：<ul>
<li>删除没有为任务提供有用信息的属性。</li>
</ul>
</li>
<li>3.特征工程（如果适用）：<ul>
<li>离散化连续特征。</li>
<li>分解特征（例如分类、日期/时间等）。</li>
<li>添加有希望的特征转换(如 Iog(x)、sqrt(x）、$x^2$等）</li>
<li>将特征聚合为有希望的新特征。</li>
</ul>
</li>
<li>4.特征缩放：<ul>
<li>标准化或归一化特征。</li>
</ul>
</li>
</ul>
<h1 id="5-列出有前途的模型"><a href="#5-列出有前途的模型" class="headerlink" title="5.列出有前途的模型"></a>5.列出有前途的模型</h1><ul>
<li>注意：<ul>
<li>如果数据量巨大，则可能需要采样为较小的训练集，以便可以在合理的时间内训练许多不同的模型(请注意，这会对诸如大型神经网络或随机森林之类的复杂模型造成不利影响)</li>
<li>尽可能自动化地执行这些步骤</li>
</ul>
</li>
<li>1.使用标准参数训练来自不同类别（例如线性、朴素贝叶斯、SVM，随机森林、神经网络等）的许多快速和粗糙的模型。</li>
<li>2.衡量并比较其性能。</li>
<li>对于每个模型，使用N折交叉验证，在N折上计算性能度量的均值和标准差。</li>
<li>3.分析每种算法的最重要的变量。</li>
<li>4.分析模型所犯错误的类型。<ul>
<li>人类将使用什么数据来避免这些错误?</li>
</ul>
</li>
<li>5.快速进行特征选择和特征工程。</li>
<li>6.在前面5个步骤中执行一两个以上的快速迭代。</li>
<li>7.筛选出前三到五个最有希望的模型，优先选择会产生不同类型错误的模型。</li>
</ul>
<h1 id="6-微调系统"><a href="#6-微调系统" class="headerlink" title="6.微调系统"></a>6.微调系统</h1><ul>
<li>注意：<ul>
<li>你将需要在此步骤中使用尽可能多的数据，尤其是在微调结束时。</li>
<li>与往常一样，尽可能做到自动化。</li>
</ul>
</li>
<li>1.使用交叉验证微调超参数：<ul>
<li>将你的数据转换选择视为超参数，尤其是当你对它们不确定时（例如，如果不确定是否用零或中位数替换缺失值，或者只是删除行）。</li>
<li>除非要研究的超参数值很少，否则应优先选择随机搜索而不是网格搜索。如果训练时间很长，你可能更喜欢贝叶斯优化方法（如Jasper Snoek等人所述使用高斯过程先验）。</li>
</ul>
</li>
<li>2.尝试使用集成方法。组合最好的模型通常会比单独运行有更好的性能。</li>
<li>3.一旦对最终模型有信心，就可以在测试集中测量其性能，以估计泛化误差。</li>
<li>注意：在测量了泛化误差之后，请不要对模型进行调整：否则你会开始过拟合测试集。</li>
</ul>
<h1 id="7-演示你的解决方案"><a href="#7-演示你的解决方案" class="headerlink" title="7.演示你的解决方案"></a>7.演示你的解决方案</h1><ul>
<li>1.记录你所做的事情。</li>
<li>2.创建一个不错的演示文稿。<ul>
<li>确保先突出大的蓝图。</li>
</ul>
</li>
<li>3.说明你的解决方案为何可以实现业务目标。</li>
<li>4.别忘了介绍你一路上注意到的有趣观点。<ul>
<li>描述什么有效，什么无效。</li>
<li>列出你的假设和系统的局限性。</li>
</ul>
</li>
<li>5.确保通过精美的可视化效果或易于记忆的陈述来传达你的主要发现（例如，“中等收入是房价的第一大预测指标”）。</li>
</ul>
<h1 id="8-启动！"><a href="#8-启动！" class="headerlink" title="8.启动！"></a>8.启动！</h1><ul>
<li>1.使你的解决方案准备投入生产环境（插入生产数据输入、编写单元测试等）。</li>
<li>2.编写监控代码，以定期检查系统的实时性能，并在系统故障时触发警报。<ul>
<li>当心缓慢的退化：随着数据的发展，模型往往会“腐烂”。</li>
<li>评估性能可能需要人工流水线（例如通过众包服务）。</li>
<li>监视你的输入的质量（例如，传感器出现故障，发送了随机值，或者另一个团队的输出变得过时)。这对于在线学习系统尤其重要。</li>
</ul>
</li>
<li>3.定期根据新数据重新训练模型（尽可能自动进行）。</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch学习笔记</title>
    <url>/pytorch-learning/</url>
    <content><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>这个目录是从深度之眼的pytorch课程中学习并整理的学习笔记</p>
<ul>
<li><a href="https://ai.deepshare.net/detail/p_5df0ad9a09d37_qYqVmt85/6">课程页面入口</a></li>
<li><a href="https://github.com/JansonYuan/Pytorch-Camp">课程代码github</a></li>
<li><a href="https://github.com/greebear/pytorch-learning">作业讲解代码</a></li>
<li>课程所有代码汇总中配套数据百度网盘地址：<a href="https://pan.baidu.com/s/1mA8wSCLnKphByzvHBzc9Pw">https://pan.baidu.com/s/1mA8wSCLnKphByzvHBzc9Pw</a><br>提取码：g5ym</li>
<li>课程所有课件汇总百度网盘地址：<a href="https://pan.baidu.com/s/1svt3lbDgNGixk5lKM1zfig">https://pan.baidu.com/s/1svt3lbDgNGixk5lKM1zfig</a><br>提取码：9j2f</li>
</ul>
<h1 id="目录笔记"><a href="#目录笔记" class="headerlink" title="目录笔记"></a>目录笔记</h1><p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week1.ipynb">Week1 Pytorch基础概念</a></p>
<ul>
<li>Pytorch简介及环境配置</li>
<li>Pytorch基础数据结构——张量</li>
<li>张量操作与线性回归</li>
<li>计算图与动态图机制</li>
<li>autograd与逻辑回归</li>
</ul>
<p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week2.ipynb">Week2 PyTorch数据处理</a></p>
<ul>
<li>数据读取机制DataLoader与Dataset</li>
<li>数据预处理transforms模块机制</li>
<li>二十二种transforms数据预处理方法</li>
<li>学会自定义transforms方法</li>
</ul>
<p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week3.ipynb">Week3 PyTorch模型搭建</a></p>
<ul>
<li>nn.Module与网络模型构建步骤</li>
<li>模型容器与AlexNet构建</li>
<li>网络层中的卷积层</li>
<li>网络层中的池化层、全连接层和激活函数层</li>
</ul>
<p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week4.ipynb">Week4 PyTorch损失优化</a></p>
<ul>
<li>权值初始化</li>
<li>损失函数（一）</li>
<li>Pytorch的14种损失函数</li>
<li>优化器optimizer的概念</li>
<li>torch.optim.SGD</li>
</ul>
<p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week5.ipynb">Week5 PyTorch训练过程</a></p>
<ul>
<li>学习率调整</li>
<li>TensorBoard简介与安装</li>
<li>TensorBoard使用（一）</li>
<li>TensorBoard使用（二）</li>
<li>hook函数与CAM</li>
</ul>
<p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week6.ipynb">Week6 PyTorch的正则化</a></p>
<ul>
<li>weight_decay</li>
<li>dropout</li>
<li>Batch Normalization</li>
<li>Layer Normalization、Instance</li>
<li>Normalization和Group Normalization</li>
</ul>
<p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week7.ipynb">Week7 PyTorch训练技巧</a></p>
<ul>
<li>模型保存与加载</li>
<li>Finetune</li>
<li>GPU的使用</li>
<li>Pytorch中常见报错</li>
</ul>
<p>Week8、9 PyTorch深度体验</p>
<ul>
<li>图像分类一瞥</li>
<li>图像分割一瞥</li>
<li>目标检测一瞥（上）</li>
<li>目标检测一瞥（下）</li>
<li>对抗生成网络一瞥</li>
<li>循环神经网络一瞥</li>
</ul>
]]></content>
      <categories>
        <category>pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch第一周学习笔记</title>
    <url>/pytorch-week1/</url>
    <content><![CDATA[<p>最原始编辑版在<a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week1.ipynb">Github链接</a></p>
<h1 id="1-PyTorch简介与安装"><a href="#1-PyTorch简介与安装" class="headerlink" title="1.PyTorch简介与安装"></a>1.PyTorch简介与安装</h1><p>Q:如何安装Pytorch?</p>
<ul>
<li>安装anaconda：<code>conda install pytorch torchvision</code></li>
<li>测试是否安装成功：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cuda.is_available()</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.__version__</span><br><span class="line"><span class="string">&#x27;1.3.1&#x27;</span></span><br></pre></td></tr></table></figure>


<h1 id="2-张量简介与创建"><a href="#2-张量简介与创建" class="headerlink" title="2.张量简介与创建"></a>2.张量简介与创建</h1><p>Q:张量是什么？</p>
<ul>
<li>一个多维数组，它是标量、向量、矩阵的高维拓展</li>
<li><img src="http://anki190912.xuexihaike.com/20200918142143.png?imageView2/2/h/150"></li>
</ul>
<p>Q:Pytorch中的Variable是什么？与Tensor的关系是什么？</p>
<ul>
<li>Variable是torch.autograd中的数据类型主要用于封装Tensor，进行自动求导</li>
<li>data:被包装的Tensor</li>
<li>grad:data的梯度</li>
<li>grad_fn:创建Tensor的Function，是自动求导的关键</li>
<li>requires_grad:指示是否需要梯度</li>
<li>is_leaf:指示是否是叶子结点（张量）</li>
<li><img src="http://anki190912.xuexihaike.com/20200918143346.png?imageView2/2/w/200"></li>
</ul>
<p>Q:Pytorch中的Tensor是什么？</p>
<ul>
<li>PyTorch 0.4.0开始，Variable并入Tensor</li>
<li>dtype: 张量的数据类型，如torch.FloatTensor, torch.cuda.FloatTensor</li>
<li>shape: 张量的形状，如（64，3， 224， 224）</li>
<li>device: 张量所在设备，GPU/CPU，是加速的关键</li>
<li><img src="http://anki190912.xuexihaike.com/20200918143722.png?imageView2/2/h/100"></li>
</ul>
<p>Q:Tensor的函数原型是怎样？</p>
<ul>
<li><code>torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)</code></li>
<li>功能：从data创建tensor</li>
<li>data: 数据，可以是list，numpy</li>
<li>dtype: 数据类型，默认与data一致</li>
<li>device: 所在设备，cuda/cpu</li>
<li>requires_grad: 是否需要梯度</li>
<li>pin_memory:是否存于锁页内存</li>
</ul>
<p>Q:通过torch.tensor创建Tensor的代码是什么？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">arr = np.ones((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">print(arr)</span><br><span class="line">print(<span class="string">&#x27;ndarray的数据类型:&#x27;</span>, arr.dtype)</span><br><span class="line"></span><br><span class="line">t = torch.tensor(arr)</span><br><span class="line">print(t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 放到gpu上</span></span><br><span class="line">t = torch.tensor(arr, device=<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>

<pre><code>[[1. 1. 1.]
 [1. 1. 1.]
 [1. 1. 1.]]
ndarray的数据类型: float64
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device=&#39;cuda:0&#39;, dtype=torch.float64)</code></pre>
<p>Q:如何通过torch.from_numpy创建张量？</p>
<ul>
<li>函数原型：<code>torch.from_numpy(ndarray)</code></li>
<li>功能：从numpy创建tensor</li>
<li>注意事项：从torch.from_numpy创建的tensor于原ndarray共享内存，当修改其中一个的数据，另外一个也将会被改动</li>
<li><img src="http://anki190912.xuexihaike.com/20200918151039.png?imageView2/2/h/150"></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arr = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">t = torch.from_numpy(arr)</span><br><span class="line">print(<span class="string">&quot;numpy array:&quot;</span>)</span><br><span class="line">print(arr)</span><br><span class="line">print(<span class="string">&quot;tensor:&quot;</span>)</span><br><span class="line">print(t)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;修改arr:&quot;</span>)</span><br><span class="line">arr[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">print(<span class="string">&quot;numpy array:&quot;</span>)</span><br><span class="line">print(arr)</span><br><span class="line">print(<span class="string">&quot;tensor:&quot;</span>)</span><br><span class="line">print(t)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;修改tensor:&quot;</span>)</span><br><span class="line">arr[<span class="number">1</span>, <span class="number">1</span>] = <span class="number">-10</span></span><br><span class="line">print(<span class="string">&quot;numpy array:&quot;</span>)</span><br><span class="line">print(arr)</span><br><span class="line">print(<span class="string">&quot;tensor:&quot;</span>)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>

<pre><code>numpy array:
[[1 2 3]
 [4 5 6]]
tensor:
tensor([[1, 2, 3],
        [4, 5, 6]])
修改arr:
numpy array:
[[0 2 3]
 [4 5 6]]
tensor:
tensor([[0, 2, 3],
        [4, 5, 6]])
修改tensor:
numpy array:
[[  0   2   3]
 [  4 -10   6]]
tensor:
tensor([[  0,   2,   3],
        [  4, -10,   6]])</code></pre>
<p>Q:如何通过torch.zeros或torch.ones创建张量？</p>
<ul>
<li>函数原型：<code>torch.zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li>函数原型：<code>torch.ones(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li>功能：依size创建全0张量和全1</li>
<li>size:张量的形状</li>
<li>out:输出的张量，貌似其原始类型必须为tensor，通过out得到的和返回值得到的是完全一样的，相当于赋值</li>
<li>layout:内存中布局形式，有strided,sparse_coo等</li>
<li>device:所在设备,gpu/cpu</li>
<li>requires_grad: 是否需要梯度</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out_t = torch.tensor([<span class="number">1</span>])</span><br><span class="line">t = torch.zeros((<span class="number">3</span>,<span class="number">3</span>), out=out_t)</span><br><span class="line">print(t)</span><br><span class="line">print(out_t)</span><br><span class="line">print(id(t), id(out_t), id(t) == id(out_t))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
140556294938560 140556294938560 True</code></pre>
<p>Q:如何通过torch.zeros_like或torch.ones_like创建张量？</p>
<ul>
<li>函数原型：<code>torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False)</code></li>
<li>函数原型：<code>torch.ones_like(input, dtype=None, layout=None, device=None, requires_grad=False)</code></li>
<li>功能：依input形状创建全0张量或全1，input是一个tensor类型</li>
<li>input:创建与input同形状的全0张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.empty(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">print(torch.zeros_like(t))</span><br><span class="line">print(torch.ones_like(t))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])</code></pre>
<p>Q:如何通过torch.full创建张量？</p>
<ul>
<li><code>torch.full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li><code>torch.full_like(input, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format)</code></li>
<li>功能：创建全等张量</li>
<li>size: 张量的形状，如（3,3）</li>
<li>fill_value: 张量的值</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.full((<span class="number">3</span>,<span class="number">3</span>), <span class="number">8</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[8., 8., 8.],
        [8., 8., 8.],
        [8., 8., 8.]])</code></pre>
<p>Q:如何通过torch.arange创建等差数列的1维张量？</p>
<ul>
<li>函数原型：<code>torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li>功能：创建等差为1的张量</li>
<li>注意事项：数值区间为[start, end)</li>
<li>start: 数列起始值</li>
<li>end: 数列“结束值”</li>
<li>step: 数列公差，默认为1</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.arange(<span class="number">2</span>,<span class="number">10</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([2, 4, 6, 8])</code></pre>
<p>Q:如何通过torch.linspace创建均分数列张量</p>
<ul>
<li>函数原型：<code>torch.linspace(start=0, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li>功能：创建均分的1维张量</li>
<li>注意事项：数值区间为[start, end]</li>
<li>start: 数列起始值</li>
<li>end: 数列结束值</li>
<li>steps: 数列长度</li>
<li>步长为：(end-start)/(steps-1)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.linspace(<span class="number">2</span>, <span class="number">10</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 2.0000,  3.3333,  4.6667,  6.0000,  7.3333,  8.6667, 10.0000])</code></pre>
<p>Q:如何通过torch.logspace创建对数均分的1维张量？</p>
<ul>
<li>函数原型：<code>torch.logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li>功能：创建对数均分的1维张量</li>
<li>注意事项：长度为steps，底为base</li>
<li>base: 对数函数的低，默认为10</li>
</ul>
<p>Q:如何通过torch.eye创建单位对角矩阵？</p>
<ul>
<li>函数原型：<code>torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li>功能：创建单位对角矩阵（2维张量）</li>
<li>注意事项：默认为方阵</li>
<li>n: 矩阵行数</li>
<li>m: 矩阵列数</li>
</ul>
<p>Q:如何通过torch.normal生成正态分布的张量？</p>
<ul>
<li>函数原型：<code>torch.normal(mean, std, *, generator=None, out=None)</code></li>
<li>功能：生成正态分布（高斯分布）</li>
<li>mean: 均值</li>
<li>std: 标准差</li>
<li>因mean和std可以分别为标量和张量，有4种不同的组合</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mean：张量 std: 张量</span></span><br><span class="line"><span class="comment"># 其中t[i]是从mean[i],std[i]的标准正态分布中采样得来</span></span><br><span class="line">mean = torch.arange(<span class="number">1</span>, <span class="number">5</span>, dtype=torch.float)</span><br><span class="line">std = torch.arange(<span class="number">1</span>, <span class="number">5</span>, dtype=torch.float)</span><br><span class="line">t = torch.normal(mean, std)</span><br><span class="line">print(<span class="string">&quot;mean:&#123;&#125;\nstd:&#123;&#125;&quot;</span>.format(mean, std))</span><br><span class="line">print(t)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean：标量 std: 标量，此时要指定size大小</span></span><br><span class="line">t_normal = torch.normal(<span class="number">0.</span>, <span class="number">1.</span>, size=(<span class="number">4</span>,))</span><br><span class="line">print(t_normal)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean：张量 std: 标量</span></span><br><span class="line">mean = torch.arange(<span class="number">1</span>, <span class="number">5</span>, dtype=torch.float)</span><br><span class="line">std = <span class="number">1</span></span><br><span class="line">t_normal = torch.normal(mean, std)</span><br><span class="line">print(<span class="string">&quot;mean:&#123;&#125;\nstd:&#123;&#125;&quot;</span>.format(mean, std))</span><br><span class="line">print(t_normal)</span><br></pre></td></tr></table></figure>

<pre><code>mean:tensor([1., 2., 3., 4.])
std:tensor([1., 2., 3., 4.])
tensor([ 0.6063,  2.9914,  4.0138, -0.5877])

tensor([ 1.1977, -0.1746,  1.5572, -1.1905])

mean:tensor([1., 2., 3., 4.])
std:1
tensor([0.7165, 1.5649, 3.2308, 3.2504])</code></pre>
<p>Q:如何创建标准正态分布的张量？</p>
<ul>
<li><code>torch.randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor</code></li>
<li><code>torch.randn_like(input, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor</code></li>
<li>size:张量的形状</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(torch.randn(<span class="number">4</span>))</span><br><span class="line">print(torch.randn(<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([-0.2387, -0.3638,  0.3597,  0.1225])
tensor([[ 0.4709,  0.8593, -0.5970],
        [-0.1133,  0.3273,  0.0106]])</code></pre>
<p>Q:如何生成均匀分布和整数均匀分布的张量？</p>
<ul>
<li>在[0,1)区间上，生成均匀分布</li>
<li><code>torch.rand(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor</code></li>
<li><code>torch.rand_like(input, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor</code></li>
<li>在[low, high)区间生成整数均匀分布</li>
<li><code>torch.randint(low=0, high, size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor</code></li>
<li><code>torch.randint_like(input, low=0, high, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor</code></li>
<li>其中size是张量形状</li>
</ul>
<p>Q:如何生成从0到n-1的随机排列？</p>
<ul>
<li><code>torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False) → LongTensor</code></li>
<li>n是张量的长度</li>
<li>经常用于生成乱序索引</li>
</ul>
<p>Q:如何生成一个伯努利分布的张量？</p>
<ul>
<li><code>torch.bernoulli(input, *, generator=None, out=None) → Tensor</code></li>
<li>以input为概率，生成伯努利分布（0-1分布，两点分布）</li>
</ul>
<h1 id="3-张量操作与线性回归"><a href="#3-张量操作与线性回归" class="headerlink" title="3.张量操作与线性回归"></a>3.张量操作与线性回归</h1><h2 id="张量的操作：拼接、切分、索引和变换"><a href="#张量的操作：拼接、切分、索引和变换" class="headerlink" title="张量的操作：拼接、切分、索引和变换"></a>张量的操作：拼接、切分、索引和变换</h2><p>Q:如何用torch.cat对张量进行拼接？</p>
<ul>
<li><code>torch.cat(tensors, dim=0, out=None) → Tensor</code></li>
<li>功能：将张量按维度dim进行拼接</li>
<li>tensors: 张量序列</li>
<li>dim：要拼接的维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">print(t)</span><br><span class="line">t1 = torch.cat([t,t], dim=<span class="number">0</span>)</span><br><span class="line">print(t1)</span><br><span class="line">print(<span class="string">&quot;shape:&quot;</span>, t1.shape)</span><br><span class="line">t2 = torch.cat([t,t], dim=<span class="number">1</span>)</span><br><span class="line">print(t2)</span><br><span class="line">print(<span class="string">&quot;shape:&quot;</span>, t2.shape)</span><br><span class="line"><span class="comment"># dim是指在哪个方向上进行叠加</span></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.6851,  0.0099, -1.4586],
        [ 0.2965,  0.4991, -0.4938]])
tensor([[-0.6851,  0.0099, -1.4586],
        [ 0.2965,  0.4991, -0.4938],
        [-0.6851,  0.0099, -1.4586],
        [ 0.2965,  0.4991, -0.4938]])
shape: torch.Size([4, 3])
tensor([[-0.6851,  0.0099, -1.4586, -0.6851,  0.0099, -1.4586],
        [ 0.2965,  0.4991, -0.4938,  0.2965,  0.4991, -0.4938]])
shape: torch.Size([2, 6])</code></pre>
<p>Q:如何用torch.stack对张量进行拼接？</p>
<ul>
<li><code>torch.stack(tensors, dim=0, out=None) → Tensor</code></li>
<li>功能：在新创建的维度dim上进行拼接</li>
<li>tensors:张量序列</li>
<li>dim：要拼接的维度</li>
<li>注意：cat不会扩张张量的维度，stack会扩张，相当于insert</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">print(t)</span><br><span class="line">t1 = torch.stack([t,t], dim=<span class="number">0</span>)</span><br><span class="line">print(t1)</span><br><span class="line">print(<span class="string">&quot;shape:&quot;</span>, t1.shape)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.6266,  0.8918,  0.6165],
        [ 1.1646, -1.8152, -0.7309]])
tensor([[[ 0.6266,  0.8918,  0.6165],
         [ 1.1646, -1.8152, -0.7309]],

        [[ 0.6266,  0.8918,  0.6165],
         [ 1.1646, -1.8152, -0.7309]]])
shape: torch.Size([2, 2, 3])</code></pre>
<p>Q:如何用torch.chunk对切分张量？</p>
<ul>
<li><code>torch.chunk(input, chunks, dim=0) → List of Tensors</code></li>
<li>功能：将张量按维度dim进行平均切分</li>
<li>返回值：张量列表</li>
<li>注意事项：若不能整除，最后一份张量小于其它张量</li>
<li>input：要切分的张量</li>
<li>chunks：要切分的份数</li>
<li>dim：要切分的维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones((<span class="number">2</span>,<span class="number">7</span>))</span><br><span class="line">print(a)</span><br><span class="line">list_of_tensors = torch.chunk(a, dim=<span class="number">1</span>, chunks=<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(list_of_tensors):</span><br><span class="line">    print(<span class="string">f&quot;第<span class="subst">&#123;i&#125;</span>个张量：&quot;</span>)</span><br><span class="line">    print(t)</span><br><span class="line"><span class="comment"># 切分后的长度的计算方式为：7/3向上取整为3</span></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.]])
第0个张量：
tensor([[1., 1., 1.],
        [1., 1., 1.]])
第1个张量：
tensor([[1., 1., 1.],
        [1., 1., 1.]])
第2个张量：
tensor([[1.],
        [1.]])</code></pre>
<p>Q:如何用torch.split对张量进行切分？</p>
<ul>
<li><code>torch.split(tensor, split_size_or_sections, dim=0)</code></li>
<li>功能：将张量按维度dim进行平均切分</li>
<li>返回值：张量列表</li>
<li>tensor：要切分的张量</li>
<li>split_size_or_sections：为int时，表示每一份的长度；为list时，按list元素切分，list元素和必须为该维度的长度</li>
<li>dim：要切分的维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones((<span class="number">2</span>,<span class="number">7</span>))</span><br><span class="line">print(a)</span><br><span class="line">list_of_tensors = torch.split(a, [<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(list_of_tensors):</span><br><span class="line">    print(<span class="string">f&quot;第<span class="subst">&#123;i&#125;</span>个张量：&quot;</span>)</span><br><span class="line">    print(t)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.]])
第0个张量：
tensor([[1., 1.],
        [1., 1.]])
第1个张量：
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.]])
第2个张量：
tensor([[1.],
        [1.]])</code></pre>
<p>Q:如何在dim维度上，按index索引数据？</p>
<ul>
<li><code>torch.index_select(input, dim, index, out=None) → Tensor</code></li>
<li>返回值：按index索引数据拼接的张量</li>
<li>input：要索引的张量</li>
<li>dim：要索引的维度</li>
<li>index：要索引数据的序号</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.randint(<span class="number">0</span>,<span class="number">9</span>, size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">idx = torch.tensor([<span class="number">0</span>,<span class="number">2</span>], dtype=torch.long)</span><br><span class="line">t_select = torch.index_select(t, dim=<span class="number">0</span>, index=idx)</span><br><span class="line">print(t)</span><br><span class="line">print(idx)</span><br><span class="line">print(t_select)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1, 2, 8],
        [2, 3, 0],
        [0, 2, 0]])
tensor([0, 2])
tensor([[1, 2, 8],
        [0, 2, 0]])</code></pre>
<p>Q:如何对张量按mask中的True进行索引？</p>
<ul>
<li><code>torch.masked_select(input, mask, out=None) → Tensor</code></li>
<li>返回值：一维张量</li>
<li>input：要索引的张量</li>
<li>mask：与input同形状的布尔类型张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.randint(<span class="number">0</span>, <span class="number">9</span>, size=(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">print(t)</span><br><span class="line">mask = t.le(<span class="number">5</span>) <span class="comment"># le是小于等于，还有lt,gt,ge</span></span><br><span class="line">print(mask)</span><br><span class="line">t_select = torch.masked_select(t, mask)</span><br><span class="line">print(t_select)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[2, 5, 0],
        [5, 8, 5],
        [2, 8, 5]])
tensor([[ True,  True,  True],
        [ True, False,  True],
        [ True, False,  True]])
tensor([2, 5, 0, 5, 5, 2, 5])</code></pre>
<p>Q:如何改变张量的形状？</p>
<ul>
<li><code>torch.reshape(input, shape) → Tensor</code></li>
<li>注意事项：当张量在内存中是连续时，新张量与input共享数据内存</li>
<li>input：要变换的张量</li>
<li>shape：新张量的形状，允许某个维度为-1，意味着这个维度根据其它的算出来的</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.randperm(<span class="number">8</span>)</span><br><span class="line">t_reshape = torch.reshape(t, (<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">print(t)</span><br><span class="line">print(t_reshape)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([2, 0, 7, 5, 6, 4, 3, 1])
tensor([[2, 0, 7, 5],
        [6, 4, 3, 1]])</code></pre>
<p>Q:如何交换张量的两个维度？</p>
<ul>
<li><code>torch.transpose(input, dim0, dim1) → Tensor</code></li>
<li>input：要交换的张量</li>
<li>dim0，dim1：要交换的维度</li>
<li>若为2维张量转置，即矩阵转置，可使用<code>torch.t(input) → Tensor</code>，等价于<code>torch.transpose(input, 0, 1)</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.rand((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">t_transpose = torch.transpose(t, dim0=<span class="number">1</span>,dim1=<span class="number">2</span>)</span><br><span class="line">print(t)</span><br><span class="line">print(t_transpose)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[0.8657, 0.5869, 0.1105, 0.4381],
         [0.7276, 0.6606, 0.3778, 0.3643],
         [0.6180, 0.6693, 0.9983, 0.4252]],

        [[0.3526, 0.6365, 0.6643, 0.5310],
         [0.4653, 0.5056, 0.1065, 0.7873],
         [0.6175, 0.6650, 0.1325, 0.5837]]])
tensor([[[0.8657, 0.7276, 0.6180],
         [0.5869, 0.6606, 0.6693],
         [0.1105, 0.3778, 0.9983],
         [0.4381, 0.3643, 0.4252]],

        [[0.3526, 0.4653, 0.6175],
         [0.6365, 0.5056, 0.6650],
         [0.6643, 0.1065, 0.1325],
         [0.5310, 0.7873, 0.5837]]])</code></pre>
<p>Q:如何压缩长度为1的维度（轴）？</p>
<ul>
<li><code>torch.squeeze(input, dim=None, out=None) → Tensor</code></li>
<li>dim: 若为None，移除所有长度为1的轴；若指定维度，当且仅当该轴长度为1时，可以被移除</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.rand((<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">t_sq = torch.squeeze(t)</span><br><span class="line">t_0 = torch.squeeze(t, dim=<span class="number">0</span>)</span><br><span class="line">t_1 = torch.squeeze(t, dim=<span class="number">1</span>)</span><br><span class="line">print(t.shape)</span><br><span class="line">print(t_sq.shape)</span><br><span class="line">print(t_0.shape)</span><br><span class="line">print(t_1.shape)</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([1, 2, 3, 1])
torch.Size([2, 3])
torch.Size([2, 3, 1])
torch.Size([1, 2, 3, 1])</code></pre>
<p>Q:如何根据dim扩展维度？</p>
<ul>
<li><code>torch.unsqueeze(input, dim) → Tensor</code></li>
<li>dim:扩展的维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">print(t)</span><br><span class="line">t1 = torch.unsqueeze(t, <span class="number">0</span>)</span><br><span class="line">print(t1)</span><br><span class="line">t2 = torch.unsqueeze(t, <span class="number">1</span>)</span><br><span class="line">print(t2)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([1, 2, 3, 4])
tensor([[1, 2, 3, 4]])
tensor([[1],
        [2],
        [3],
        [4]])</code></pre>
<h2 id="张量的数学运算"><a href="#张量的数学运算" class="headerlink" title="张量的数学运算"></a>张量的数学运算</h2><p>Q:有哪些常见的数学运算？</p>
<ul>
<li>一、加减乘除<ul>
<li>torch.add()</li>
<li>torch.addcdiv()</li>
<li>torch.addcmul()</li>
<li>torch.sub() </li>
<li>torch.div()</li>
<li>torch.mul()</li>
</ul>
</li>
<li>二、对数，指数，幂函数<ul>
<li>torch.log(input, out=None)</li>
<li>torch.log10(input, out=None)</li>
<li>torch.log2(input, out=None)</li>
<li>torch.exp(input, out=None)</li>
<li>torch.pow()</li>
</ul>
</li>
<li>三、三角函数<ul>
<li>torch.abs(input, out=None)</li>
<li>torch.acos(input, out=None)</li>
<li>torch.cosh(input, out=None)</li>
<li>torch.cos(input, out=None)</li>
<li>torch.asin(input, out=None)</li>
<li>torch.atan(input, out=None)</li>
<li>torch.atan2(input, other, out=None)</li>
</ul>
</li>
</ul>
<p>Q:如何逐元素计算input + alpha x other?</p>
<ul>
<li><code>torch.add(input, other, *, alpha=1, out=None)</code></li>
<li>input：第一个张量</li>
<li>alpha：乘项因子</li>
<li>other：第二个张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t_0 = torch.randn((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">t_1 = torch.ones_like(t_0)</span><br><span class="line">t_add = torch.add(t_0, t_1, alpha=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;t_0:\n&#123;&#125;\nt_1:\n&#123;&#125;\nt_add_10:\n&#123;&#125;&quot;</span>.format(t_0, t_1, t_add))</span><br></pre></td></tr></table></figure>

<pre><code>t_0:
tensor([[ 0.5570, -0.4743,  1.0113],
        [-1.2665,  0.1997, -0.6957],
        [-0.0714, -0.7002, -1.4687]])
t_1:
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
t_add_10:
tensor([[10.5570,  9.5257, 11.0113],
        [ 8.7335, 10.1997,  9.3043],
        [ 9.9286,  9.2998,  8.5313]])</code></pre>
<p>Q:如何计算$\text { out }<em>{i}=\text { input }</em>{i}+\text { value } \times \text { tensor } 1_{i} \times \text { tensor } 2_{i}$</p>
<ul>
<li><code>torch.addcmul(input, tensor1, tensor2, *, value=1, out=None) → Tensor</code></li>
</ul>
<p>Q:如何计算$\text { out }<em>{i}=\text { input }</em>{i}+\text { value } \times \frac{\text { tensor } 1}{\text { tensor } 2_{i}}$</p>
<ul>
<li><code>torch.addcdiv(input, tensor1, tensor2, *, value=1, out=None) → Tensor</code></li>
</ul>
<h2 id="线性回归的Pytorch实现"><a href="#线性回归的Pytorch实现" class="headerlink" title="线性回归的Pytorch实现"></a>线性回归的Pytorch实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.05</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练数据</span></span><br><span class="line">x = torch.rand(<span class="number">20</span>, <span class="number">1</span>) * <span class="number">10</span>  <span class="comment"># x data (tensor), shape=(20, 1)</span></span><br><span class="line">y = <span class="number">2</span>*x + (<span class="number">5</span> + torch.randn(<span class="number">20</span>, <span class="number">1</span>))  <span class="comment"># y data (tensor), shape=(20, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建线性回归参数的初始值</span></span><br><span class="line">w = torch.randn((<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros((<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播，计算y_pred=w * x+b</span></span><br><span class="line">    wx = torch.mul(w, x)</span><br><span class="line">    y_pred = torch.add(wx, b)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 MSE loss</span></span><br><span class="line">    loss = (<span class="number">0.5</span> * (y - y_pred) ** <span class="number">2</span>).mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    b.data.sub_(lr * b.grad)</span><br><span class="line">    w.data.sub_(lr * w.grad)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 清零张量的梯度</span></span><br><span class="line">    w.grad.zero_()</span><br><span class="line">    b.grad.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图</span></span><br><span class="line">    <span class="keyword">if</span> iteration % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        plt.plot(x.data.numpy(), y_pred.data.numpy(), <span class="string">&#x27;r-&#x27;</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">2</span>, <span class="number">20</span>, <span class="string">&#x27;Loss=%.4f&#x27;</span> % loss.data.numpy(), fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>:  <span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.xlim(<span class="number">1.5</span>, <span class="number">10</span>)</span><br><span class="line">        plt.ylim(<span class="number">8</span>, <span class="number">28</span>)</span><br><span class="line">        plt.title(<span class="string">f&quot;Iteration: <span class="subst">&#123;iteration&#125;</span>\nw: <span class="subst">&#123;w.data.numpy()&#125;</span> b: <span class="subst">&#123;b.data.numpy()&#125;</span>&quot;</span>)</span><br><span class="line">        plt.pause(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> loss.data.numpy() &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="http://anki190912.xuexihaike.com/20200920095346.png"></p>
<h1 id="4-计算图与动态图机制"><a href="#4-计算图与动态图机制" class="headerlink" title="4.计算图与动态图机制"></a>4.计算图与动态图机制</h1><p>Q:计算图是什么？</p>
<ul>
<li>用来描述运算的有向无环图</li>
<li>有两个主要元素：结点（Node）和边（Edge）</li>
<li>结点表示数据，如向量、矩阵、张量，边表示运算，如加减乘除卷积等</li>
</ul>
<p>Q:如何用计算图表示$y = (x+w)*(w+1)$?</p>
<ul>
<li>$a = x + w, b = w + 1, y = a * b$</li>
<li><img src="http://anki190912.xuexihaike.com/20200919144309.png?imageView2/2/h/200"></li>
</ul>
<p>Q:如何用计算图进行梯度求导，如$y = (x+w)*(w+1)$</p>
<ul>
<li>$a = x + w, b = w + 1, y = a * b$</li>
<li>$$\begin{aligned}<br>\frac{\partial \mathrm{y}}{\partial w} &amp;=\frac{\partial \mathrm{y}}{\partial a} \frac{\partial a}{\partial w}+\frac{\partial \mathrm{y}}{\partial b} \frac{\partial b}{\partial w} \\<br>&amp;=b * 1+\mathrm{a} * 1 \\<br>&amp;=\mathrm{b}+\mathrm{a} \\<br>&amp;=(\mathrm{w}+1)+(\mathrm{x}+\mathrm{w}) \\<br>&amp;=2 * \mathrm{w}+\mathrm{x}+1 \\<br>&amp;=2 * 1+2+1=5\end{aligned}$$</li>
<li><img src="http://anki190912.xuexihaike.com/20200919144541.png?imageView2/2/h/200"></li>
<li>y对w求导在计算图中其实就是找到y到w的所有路径上的导数，进行求和</li>
</ul>
<p>Q:叶子结点是什么？</p>
<ul>
<li><img src="http://anki190912.xuexihaike.com/20200919144541.png?imageView2/2/h/200"></li>
<li>用户创建的结点称为叶子结点，如X和W</li>
<li>torch.Tensor中有is_leaf指示张量是否为叶子结点</li>
<li>设置叶子结点主要是为了节省内存，因为非叶子结点的梯度在反向传播后会被释放掉</li>
<li>若需要保留非叶子结点的梯度，可使用retain_grad()方法</li>
</ul>
<p>Q:torch.Tensor中的grad_fn作用是什么？</p>
<ul>
<li>记录创建该张量时所用的方法（函数）</li>
<li><img src="http://anki190912.xuexihaike.com/20200919144541.png?imageView2/2/h/200"></li>
<li>y.grad_fn = &lt;MulBackward0&gt;</li>
<li>a.grad_fn = &lt;AddBackward0&gt;</li>
</ul>
<p>Q:$y = (x+w)*(w+1)$计算图的代码示例，求解y对w的梯度？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">w = torch.tensor([<span class="number">1.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">x = torch.tensor([<span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">a = torch.add(w, x)</span><br><span class="line"><span class="comment"># 若需要保留非叶子结点a的梯度，否则调用a.grad时为None</span></span><br><span class="line"><span class="comment"># a.retain_grad()</span></span><br><span class="line">b = torch.add(w, <span class="number">1</span>)</span><br><span class="line">y = torch.mul(a, b)</span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line">print(w.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看叶子结点</span></span><br><span class="line">print(<span class="string">&quot;\nis_leaf:\n&quot;</span>, w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看梯度</span></span><br><span class="line">print(<span class="string">&quot;\ngradient:\n&quot;</span>, w.grad, x.grad, a.grad, b.grad, y.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 grad_fn</span></span><br><span class="line">print(<span class="string">&quot;\ngrad_fn:\n&quot;</span>, w.grad_fn, x.grad_fn, a.grad_fn, b.grad_fn, y.grad_fn)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([5.])

is_leaf:
 True True False False False

gradient:
 tensor([5.]) tensor([2.]) None None None

grad_fn:
 None None &lt;AddBackward0 object at 0x7fd54938bb00&gt; &lt;AddBackward0 object at 0x7fd5285f4c50&gt; &lt;MulBackward0 object at 0x7fd5285f4be0&gt;</code></pre>
<h1 id="5-autograd与逻辑回归"><a href="#5-autograd与逻辑回归" class="headerlink" title="5.autograd与逻辑回归"></a>5.autograd与逻辑回归</h1><p>Q:torch.autograd.backward是什么？</p>
<ul>
<li>torch.autograd.backward(tensors: Union[torch.Tensor, Sequence[torch.Tensor]], grad_tensors: Union[torch.Tensor, Sequence[torch.Tensor], None] = None, retain_graph: Optional[bool] = None, create_graph: bool = False, grad_variables: Union[torch.Tensor, Sequence[torch.Tensor], None] = None) → None</li>
<li>功能：自动求取梯度</li>
<li>tensors：用于求导的张量，如loss</li>
<li>retain_graph：保存计算图，若不保存，则紧接着再调用一次backward()会报错</li>
<li>create_graph：创建导数计算图，用于高阶求导</li>
<li>grad_tensors：多梯度权重</li>
</ul>
<p>Q:torch.autograd.backward中的retain_graph的代码示例？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = torch.tensor([<span class="number">1.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">x = torch.tensor([<span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">a = torch.add(w, x)</span><br><span class="line">b = torch.add(w, <span class="number">1</span>)</span><br><span class="line">y = torch.mul(a, b)</span><br><span class="line"></span><br><span class="line">y.backward(retain_graph=<span class="literal">True</span></span><br><span class="line">          )</span><br><span class="line">print(w.grad)</span><br><span class="line">y.backward()</span><br><span class="line">print(w.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([5.])
tensor([10.])</code></pre>
<p>Q:torch.autograd.backward中的grad_tensors的代码示例？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = torch.tensor([<span class="number">1.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">x = torch.tensor([<span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">a = torch.add(w, x)     <span class="comment"># retain_grad()</span></span><br><span class="line">b = torch.add(w, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y0 = torch.mul(a, b)    <span class="comment"># y0 = (x+w) * (w+1)</span></span><br><span class="line">y1 = torch.add(a, b)    <span class="comment"># y1 = (x+w) + (w+1)    dy1/dw = 2</span></span><br><span class="line"></span><br><span class="line">loss = torch.cat([y0, y1], dim=<span class="number">0</span>)       <span class="comment"># [y0, y1]</span></span><br><span class="line">grad_tensors = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># gradient 传入 torch.autograd.backward()中的grad_tensors</span></span><br><span class="line">loss.backward(gradient=grad_tensors)</span><br><span class="line"><span class="comment"># 实际上相当于1*y0导数+2*y1导数</span></span><br><span class="line"></span><br><span class="line">print(w.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([9.])</code></pre>
<p>Q:torch.autograd.grad是什么？</p>
<ul>
<li>torch.autograd.grad(outputs: Union[torch.Tensor, Sequence[torch.Tensor]], inputs: Union[torch.Tensor, Sequence[torch.Tensor]], grad_outputs: Union[torch.Tensor, Sequence[torch.Tensor], None] = None, retain_graph: Optional[bool] = None, create_graph: bool = False, only_inputs: bool = True, allow_unused: bool = False) → Tuple[torch.Tensor, …]</li>
<li>功能：求取梯度</li>
<li>outputs：用于求导的张量，如loss</li>
<li>inputs：需要梯度的张量</li>
<li>create_graph：创建导数计算图，用于高阶求导</li>
<li>retain_graph：保存计算图</li>
<li>grad_outputs：多梯度权重</li>
</ul>
<p>Q:如何使用torch.autograd.grad对$y=x^2$进行一阶和二阶求导？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.pow(x, <span class="number">2</span>)  <span class="comment"># y = x**2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># grad_1 = dy/dx = 2x = 2 * 3 = 6</span></span><br><span class="line">grad_1 = torch.autograd.grad(y, x, create_graph=<span class="literal">True</span>)</span><br><span class="line">print(grad_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># grad_2 = d(dy/dx)/dx = d(2x)/dx = 2</span></span><br><span class="line"><span class="comment"># grad_1的返回值是元组，所以要取出第一个</span></span><br><span class="line">grad_2 = torch.autograd.grad(grad_1[<span class="number">0</span>], x)</span><br><span class="line">print(grad_2)</span><br></pre></td></tr></table></figure>

<pre><code>(tensor([6.], grad_fn=&lt;MulBackward0&gt;),)
(tensor([2.]),)</code></pre>
<p>Q:autograd的3点使用小贴士是什么？</p>
<ul>
<li>1.梯度不自动清零，每次传播时会一直叠加上去，所以使用梯度之后要手动进行清零，即w.grad.zero_()，其中下划线表示inplace（原地）操作</li>
<li>2.依赖于叶子结点的节点，requires_grad默认为True</li>
<li>3.叶子结点不可执行in-place</li>
</ul>
<p>Q:逻辑回归的pytorch代码实现是什么？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">torch.manual_seed(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">sample_nums = <span class="number">100</span></span><br><span class="line">mean_value = <span class="number">1.7</span></span><br><span class="line">bias = <span class="number">1</span></span><br><span class="line">n_data = torch.ones(sample_nums, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 类别0 数据 shape=(100, 2)</span></span><br><span class="line">x0 = torch.normal(mean_value * n_data, <span class="number">1</span>) + bias</span><br><span class="line"><span class="comment"># 类别0 标签 shape=(100)</span></span><br><span class="line">y0 = torch.zeros(sample_nums)</span><br><span class="line"><span class="comment"># 类别1 数据 shape=(100, 2)</span></span><br><span class="line">x1 = torch.normal(-mean_value * n_data, <span class="number">1</span>) + bias</span><br><span class="line"><span class="comment"># 类别1 标签 shape=(100)</span></span><br><span class="line">y1 = torch.ones(sample_nums)</span><br><span class="line">train_x = torch.cat((x0, x1), <span class="number">0</span>)</span><br><span class="line">train_y = torch.cat((y0, y1), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LR</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(LR, self).__init__()</span><br><span class="line">        self.features = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = self.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化逻辑回归模型</span></span><br><span class="line">lr_net = LR()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择损失函数，交叉熵损失</span></span><br><span class="line">loss_fn = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择优化器</span></span><br><span class="line">lr = <span class="number">0.01</span> <span class="comment"># 学习率</span></span><br><span class="line">optimizer = torch.optim.SGD(lr_net.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    y_pred = lr_net(train_x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算loss</span></span><br><span class="line">    loss = loss_fn(y_pred.squeeze(), train_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绘图</span></span><br><span class="line">    <span class="keyword">if</span> iteration % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 以0.5为阈值进行分类</span></span><br><span class="line">        mask = y_pred.ge(<span class="number">0.5</span>).float().squeeze()</span><br><span class="line">        <span class="comment"># 计算正确预测的样本个数</span></span><br><span class="line">        correct = (mask == train_y).sum()</span><br><span class="line">        <span class="comment"># 计算分类准确率</span></span><br><span class="line">        acc = correct.item() / train_y.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        plt.scatter(x0.data.numpy()[:, <span class="number">0</span>], x0.data.numpy()[:, <span class="number">1</span>], c=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;class 0&#x27;</span>)</span><br><span class="line">        plt.scatter(x1.data.numpy()[:, <span class="number">0</span>], x1.data.numpy()[:, <span class="number">1</span>], c=<span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;class 1&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        w0, w1 = lr_net.features.weight[<span class="number">0</span>]</span><br><span class="line">        w0, w1 = float(w0.item()), float(w1.item())</span><br><span class="line">        plot_b = float(lr_net.features.bias[<span class="number">0</span>].item())</span><br><span class="line">        plot_x = np.arange(<span class="number">-6</span>, <span class="number">6</span>, <span class="number">0.1</span>)</span><br><span class="line">        plot_y = (-w0 * plot_x - plot_b) / w1</span><br><span class="line">        </span><br><span class="line">        plt.xlim(<span class="number">-5</span>, <span class="number">7</span>)</span><br><span class="line">        plt.ylim(<span class="number">-7</span>, <span class="number">7</span>)</span><br><span class="line">        plt.plot(plot_x, plot_y)</span><br><span class="line">        </span><br><span class="line">        plt.text(<span class="number">-5</span>, <span class="number">5</span>, <span class="string">&#x27;Loss=%.4f&#x27;</span> % loss.data.numpy(), fontdict=&#123;<span class="string">&#x27;size&#x27;</span>:<span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>:<span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.title(<span class="string">&quot;Iteration: &#123;&#125;\nw0:&#123;:.2f&#125; w1:&#123;:.2f&#125; b: &#123;:.2f&#125; accuracy:&#123;:.2%&#125;&quot;</span>.format(iteration, w0, w1, plot_b, acc))</span><br><span class="line">        plt.legend()</span><br><span class="line"></span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> acc &gt; <span class="number">0.99</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p><img src="http://anki190912.xuexihaike.com/20200920100028.png"></p>
]]></content>
      <categories>
        <category>pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
</search>
