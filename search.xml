<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>类别不平衡问题的方法汇总</title>
    <url>/class-imbalance/</url>
    <content><![CDATA[<h1 id="类别不平衡问题-class-imbalance-是什么"><a href="#类别不平衡问题-class-imbalance-是什么" class="headerlink" title="类别不平衡问题(class-imbalance)是什么"></a>类别不平衡问题(class-imbalance)是什么</h1><ul>
<li>指分类任务中不同类别的训练样例数目差别很大的情况</li>
<li>若不同类别的训练样例数目稍有差别，通常影响不大，但若差别很大，则会对学习过程造成困扰。例如有998个反例，但是正例只有2个，那么学习方法只需要返回一个永远将新样本预测为反例的学习器，就能达到99.8%的精度；然而这样的学习器往往没有价值，因为它不能预测出任何正例</li>
</ul>
<h1 id="上采样-过采样-Oversampling"><a href="#上采样-过采样-Oversampling" class="headerlink" title="上采样(过采样, Oversampling)"></a>上采样(过采样, Oversampling)</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><ul>
<li>对训练集中的少数类进行“过采样”，即增加一些少数类样本使得正、反例数目接近，然后再进行学习</li>
</ul>
<h2 id="Random-Oversampling-随机上采样"><a href="#Random-Oversampling-随机上采样" class="headerlink" title="Random Oversampling(随机上采样)"></a>Random Oversampling(随机上采样)</h2><ul>
<li>简单复制样本的策略来增加少数类样本，容易产生模型过拟合的问题</li>
</ul>
<h2 id="SMOTE"><a href="#SMOTE" class="headerlink" title="SMOTE"></a>SMOTE</h2><ul>
<li>即合成少数类过采样技术(Synthetic Minority Oversampling Technique)，是基于随机采样算法的一种改进，其基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中</li>
</ul>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ul>
<li>1.对于少数类中每一个样本$x_i$，以欧氏距离为标准计算它到少数类样本集$S_{min}$中所有样本的距离，得到其k近邻</li>
<li>2.根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本$x_i$，从其k近邻中随机选择若干个样本，假设选择的近邻为$\tilde{x}$。</li>
<li>3.对于每一个随机选出的近邻$\tilde{x}$，分别于原样本按照如下的公式构建新的样本</li>
<li>$$x_{n e w}=x+\operatorname{rand}(0,1) \times |\tilde{x}-x|$$</li>
<li><img src="http://anki190912.xuexihaike.com/20201019195529.png?imageView2/2/h/250"></li>
</ul>
<h3 id="SMOTE的问题"><a href="#SMOTE的问题" class="headerlink" title="SMOTE的问题"></a>SMOTE的问题</h3><ul>
<li>随机选取少数类样本用以合成新样本，而不考虑周边样本的情况<ul>
<li>1.如果选取的少数类样本周围都是少数类样本，则新合成的样本不会提供太多有用信息。就像SVM中远离margin的点对决策边界影响不大</li>
<li>2.如果选取的少数类样本周围都是多数类样本，这类的样本可能是噪声，则新合成的样本会与周围的多数类样本产生大部分重叠，导致分类困难</li>
</ul>
</li>
</ul>
<h2 id="Borderline-SMOTE"><a href="#Borderline-SMOTE" class="headerlink" title="Borderline-SMOTE"></a>Borderline-SMOTE</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ul>
<li>由于原始SMOTE算法的对所有少数类样本都是一视同仁的，我们希望新合成的少数类样本能处于两个类别的边界附近，因为在实际建模过程中那些处于边界位置的样本更容易被错分，因此利用边界位置的样本信息产生新样本可以给模型带来更大的体征，能提供足够的信息用以分类，即Borderline SMOTE算法做的事情</li>
</ul>
<h3 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h3><ul>
<li>这个算法会先将所有的少数类样本分成三类，如图</li>
<li><img src="http://anki190912.xuexihaike.com/20201019140912.png"></li>
<li>“noise”：所有的k近邻个样本都属于多数类，可认为是噪声不能生成合成样本</li>
<li>“danger”：超过一半的k近邻样本属于多数类</li>
<li>“safe”：超过一半的k近邻样本属于少数类</li>
<li>borderline smote算法只会从处于”danger”状态的样本中随机选择，然后用SMOTE算法产生新的样本。处于”danger”状态的样本代表靠近”边界”附近的少数类样本往往更容易被误分类。因而Border-line SMOTE只对那些靠近”边界”的少数类样本进行人工合成样本，而SMOTE则对所有少数类样本一视同仁</li>
</ul>
<h3 id="危险集的判断流程"><a href="#危险集的判断流程" class="headerlink" title="危险集的判断流程"></a>危险集的判断流程</h3><ul>
<li>1.对于每个$x_{i} \subset S_{\min }$确定一系列K近邻样本集，称该数据集为$S_{i-kNN}$，且$S_{i-kNN} \subset S$</li>
<li>2.对每个样本$x_i$，判断出最近邻样本集中属于多数类样本的个数，即$\left|S_{i-k N N} \cap S_{m a j}\right|$</li>
<li>3.选择满足不等式$\frac{k}{2} \leq\left|S_{i-k N N} \cap S_{m a j}\right| \leq k$，将其加入危险集DANGER</li>
</ul>
<h3 id="Borderline-SMOTE分类两种："><a href="#Borderline-SMOTE分类两种：" class="headerlink" title="Borderline SMOTE分类两种："></a>Borderline SMOTE分类两种：</h3><ul>
<li>Borderline-1 SMOTE：在合成样本时所选的近邻是一个少数类样本</li>
<li>Borderline-2 SMOTE：在合成样本时所选的近邻是任意一个样本</li>
</ul>
<h2 id="ADASYN-Adaptive-Synthetic-Sampling，自适应合成采用"><a href="#ADASYN-Adaptive-Synthetic-Sampling，自适应合成采用" class="headerlink" title="ADASYN(Adaptive Synthetic Sampling，自适应合成采用)"></a>ADASYN(Adaptive Synthetic Sampling，自适应合成采用)</h2><ul>
<li>根据数据分布情况为不同的少数类样本生成不同数量的新样本</li>
<li>首先根据最终的平衡程度设定总共需要生成的新少数类样本数量，然后为每个少数类样本x计算分布比例</li>
</ul>
<h1 id="下采样-降采样-UnserSampling"><a href="#下采样-降采样-UnserSampling" class="headerlink" title="下采样(降采样, UnserSampling)"></a>下采样(降采样, UnserSampling)</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><ul>
<li>对训练集中多数类样本进行“下采样”(undersampling)，即去除一些多数类中的样本使得正例、反例数目接近，然后再学习</li>
</ul>
<h2 id="Random-Undersampling-随机下采样-或-原型选择-Prototype-Selection"><a href="#Random-Undersampling-随机下采样-或-原型选择-Prototype-Selection" class="headerlink" title="Random Undersampling(随机下采样) 或 原型选择(Prototype Selection)"></a>Random Undersampling(随机下采样) 或 原型选择(Prototype Selection)</h2><h3 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h3><ul>
<li>从多数类$S_{maj}$中随机选择一些样本组成样本集E。然后将样本集E从$S_{maj}$中移除。新的数据集$S_{n e w-m a j}=S_{m a j}-E$</li>
<li>通过改变多数类样本比例以达到修改样本分布的目的，从而使样本分布较为均衡</li>
</ul>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>操作简单，只依赖于样本分布，不依赖任何距离信息，属于非启发式方法</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>由于采样的样本集合要少于原来的样本集合，因此会造成一些信息缺失，即将多数类样本删除有可能导致分类器丢失有关多数类的重要信息</li>
</ul>
<h2 id="Ensemble-Methods"><a href="#Ensemble-Methods" class="headerlink" title="Ensemble Methods"></a>Ensemble Methods</h2><h3 id="EasyEnsemble"><a href="#EasyEnsemble" class="headerlink" title="EasyEnsemble"></a>EasyEnsemble</h3><h4 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h4><ul>
<li>多次随机欠采样，尽可能全面地涵盖所有信息，特点是利用boosting减少偏差(Adaboost)、bagging减少方差(集成分类器)。实际应用的时候可尝试选用不同的分类器来提高分类的效果</li>
<li><img src="http://anki190912.xuexihaike.com/20201020145429.png?imageView2/2/h/350"></li>
</ul>
<h4 id="算法流程-2"><a href="#算法流程-2" class="headerlink" title="算法流程"></a>算法流程</h4><ul>
<li>1.把数据划分为两部分，分别是多数类样本$S_{maj}$和少数类样本$S_{min}$</li>
<li>2.从多数类$S_{maj}$中有放回的随机采样n次，每次选取与少数类数目相近的样本个数即$|S_{imaj}|=|S_{min}|$，可得到n个样本集合，记作$\{S_{1 m a j}, S_{2 m a j}, \ldots, S_{n m a j}\}$</li>
<li>3.将每一个多数类样本的子集$S_{imaj}$与少数类样本$S_{min}$合并后训练出Adaboost分类器$H_i$，阈值设置为$\theta_i$，可得到n个模型，即$H_{i}(x)=\operatorname{sgn}\left(\sum\limits_{j=1}^{s_{i}} \alpha_{i j} h_{i, j}(x)-\theta_{i}\right)$</li>
<li>4.将这些模型组合形成一个集成学习系统，最终的模型结果是这n个模型的投票值。此处采用加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率小的弱分类器的权值，使其在表决中起较小的作用，即最终分类器为$H(x)=\operatorname{sgn}\left(\sum\limits_{i=1}^{n} \sum\limits_{j=1}^{s_{i}} \alpha_{i j} h_{i j}(x)-\sum\limits_{i=1}^{n} \theta_{i}\right)$</li>
</ul>
<h3 id="BalanceCascade"><a href="#BalanceCascade" class="headerlink" title="BalanceCascade"></a>BalanceCascade</h3><h4 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h4><ul>
<li>该算法得到的是一个级联分类器，基于Adaboost，将若干个强分类器由简单到复杂排列，只有和少数类样本特征比较接近的才有可能输入到后面的分类器，比如边界点，因此能更充分地利用多数类样本的信息，一定程度上解决随机欠采样的信息丢失问题</li>
</ul>
<h4 id="算法流程-3"><a href="#算法流程-3" class="headerlink" title="算法流程"></a>算法流程</h4><ul>
<li>输入：一个包含少数类阳性样本P和多数类阴性样本集N的训练集D，定义T是从N中抽取的子集个数，$s_i$是训练Adaboost基分类器$H_i$时的循环次数</li>
<li>输出：一个组合分类器H(x)</li>
<li>1.$f=\sqrt[(\mathrm{T}-1)]{\left(\frac{|\mathrm{P}|}{|\mathrm{N}|}\right)}$，$f$是每一层级的分类器$H_i$该达到的假阳性率(False Positive Rate)，即把多数类样本误分为少数类的错误率</li>
<li>for i = 1 to T:<ul>
<li>从多数类N中随机抽取一个样本子集$N_i$，使得$|N_i| = |P|$</li>
<li>使用少数类样本集P和样本子集$N_i$训练一个Adaboost分类器$H_i$($H_i$由$s_i$个基分类器$h_{i,j}$及其权重$\alpha_{i,j}$构成，$\theta_i$是$H_i$的调节参数)</li>
<li>$\mathrm{H}_{\mathrm{i}}(x)=\operatorname{sgn}(\sum\limits_{j=1}^{s_{i}} \alpha_{i, j} h_{i, j}(x)-\theta_{i})$</li>
<li>调节阈值$\theta_i$令$H_i$的FP率为$f$</li>
<li>移除多数类样本集N中所有被$H_i$正确分类的样本</li>
</ul>
</li>
<li>输出一个集成分类器</li>
<li>$\mathrm{H}(\mathrm{x})=\operatorname{sgn}\left(\sum\limits_{i=1}^{T} \sum\limits_{j=1}^{s_{l}} \alpha_{i, j} h_{i, j}(x)-\sum\limits_{i=1}^{T} \theta_{i}\right)$</li>
</ul>
<h2 id="NearMiss"><a href="#NearMiss" class="headerlink" title="NearMiss"></a>NearMiss</h2><ul>
<li>本质上是一种原型选择(prototype selection)方法，即从多数类样本中选取最具代表性的样本用于训练，主要是为了缓解随机欠采样中的信息丢失问题。Nearmiss采用了3中不同的启发式规则来选择样本<ul>
<li>NearMiss-1：选择到最近的K个少数类样本平均距离最近的多数类样本，考虑的是与最近的k个少数类样本的平均距离，是局部的。该方法得到的多数类样本分布是“不均衡”的，它倾向于在比较集中的少数类附近找到更多的多数类样本，而在孤立的(离群的)少数类附近找到更少的多数类样本，原因是该方法考虑的局部性质和平均距离</li>
<li>NearMiss-2：选择到最远的K个少数类样本平均距离最近的多数类样本，考虑的是与最远的k个少数类样本的平均距离，是全局的。实验结果表明该方法的不均衡分类性能最优</li>
<li>NearMiss-3：对于每个少数类样本选择K个最近的多数类样本，目的是保证每个少数类样本都被多数类样本包围，该方法会使每一个少数类样本附近都有足够多的多数类样本，显然这会使得模型的精确度高、召回率低</li>
</ul>
</li>
</ul>
<h2 id="原型生成-Prototype-generation"><a href="#原型生成-Prototype-generation" class="headerlink" title="原型生成(Prototype generation)"></a>原型生成(Prototype generation)</h2><ul>
<li>给定数据集S，原型生成算法将生成一个子集S’，其中|S’|&lt;|S|，但是子集并非来自于原始数据，而是由原始数据集生成，方法是聚类成|S’|个类，然后取其中心点</li>
</ul>
<h2 id="Data-Cleaning-Techniques"><a href="#Data-Cleaning-Techniques" class="headerlink" title="Data Cleaning Techniques"></a>Data Cleaning Techniques</h2><h3 id="Tomek-Links"><a href="#Tomek-Links" class="headerlink" title="Tomek Links"></a>Tomek Links</h3><h4 id="定义-3"><a href="#定义-3" class="headerlink" title="定义"></a>定义</h4><ul>
<li>给定一个样本对$(x_i, x_j)$，其中$x_{i} \in S_{m a j}, x_{j} \in S_{\min }$，记$d(x_i, x_j)$是样本$x_i$和样本$x_j$之间的距离，如果不存在任何样本$x_k$，使得$d\left(x_{i}, x_{k}\right)&lt;d\left(x_{i}, x_{j}\right)$，那么样本对$(x_i, x_j)$即称为Tomek Links。即Tomek links为相反类最近邻样本之间的一对连接</li>
<li>不属于Tomek Links的情况有这个少数类样本最近的样本是同一类</li>
</ul>
<h4 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h4><ul>
<li>如果两个样本来自Tomek Links，那么他们中的一个样本要么是噪声，要么它们都在两类的边界上</li>
</ul>
<h4 id="图示"><a href="#图示" class="headerlink" title="图示"></a>图示</h4><ul>
<li><img src="http://anki190912.xuexihaike.com/20201021171029.png?imageView2/2/h/300"></li>
</ul>
<h4 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h4><ul>
<li>欠采样：将Tomek Links中属于是多数类的样本剔除</li>
<li>数据清洗：将Tomek Links中的两个样本都剔除</li>
</ul>
<h3 id="ENN-edited-nearest-neighborhood"><a href="#ENN-edited-nearest-neighborhood" class="headerlink" title="ENN(edited nearest neighborhood)"></a>ENN(edited nearest neighborhood)</h3><ul>
<li>这种方法应用knn来编辑(edit)数据集，对于每一个要进行下采样的样本，那些绝大多数近邻样本不属于该类的样本会被移除，而绝大多数的近邻样本属于同一类的样本会被保留</li>
</ul>
<h1 id="综合采样-Oversampling-Undersampling"><a href="#综合采样-Oversampling-Undersampling" class="headerlink" title="综合采样(Oversampling + Undersampling)"></a>综合采样(Oversampling + Undersampling)</h1><h2 id="定义-4"><a href="#定义-4" class="headerlink" title="定义"></a>定义</h2><ul>
<li>先过采样，然后再进行数据的清洗</li>
</ul>
<h2 id="SMOTE-Tomek-Links"><a href="#SMOTE-Tomek-Links" class="headerlink" title="SMOTE+Tomek Links"></a>SMOTE+Tomek Links</h2><h3 id="算法流程-4"><a href="#算法流程-4" class="headerlink" title="算法流程"></a>算法流程</h3><ul>
<li>1.利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T</li>
<li>2.剔除T中的Tomek Links对</li>
</ul>
<h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul>
<li>普通的SMOTE方法生成的少数类样本是通过线性插值得到的，在平衡类别分布的同时也扩张了少数类的样本空间，产生的问题是可能原本属于多数类样本的空间被少数类“入侵”，容易造成模型的过拟合</li>
<li>Tomek Links对寻找的是那种噪声点或者边界点，可以很好地解决“入侵”问题，如图红色加号为SMOTE产生的少数类样本，可以看到红色样本“入侵”到原本属于多数类样本的空间，这种噪声数据问题可以通过Tomek Links很好地解决</li>
<li><img src="http://anki190912.xuexihaike.com/20201021171629.png?imageView2/2/h/150"></li>
</ul>
<h2 id="SMOTE-ENN"><a href="#SMOTE-ENN" class="headerlink" title="SMOTE+ENN"></a>SMOTE+ENN</h2><ul>
<li>1.利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T</li>
<li>2.对T中的每一个样本使用KNN(一般k取3)方法预测，若预测结果与实际类别标签不符，则剔除该样本</li>
</ul>
<h1 id="其它方法"><a href="#其它方法" class="headerlink" title="其它方法"></a>其它方法</h1><h2 id="基于异常检测的方法"><a href="#基于异常检测的方法" class="headerlink" title="基于异常检测的方法"></a>基于异常检测的方法</h2><ul>
<li>把小类的样本作为异常点(outliers)，因此该问题便转化为异常点检测(anomaly detection)与变化趋势检测问题(change detection)</li>
</ul>
<h2 id="分治ensemble"><a href="#分治ensemble" class="headerlink" title="分治ensemble"></a>分治ensemble</h2><ul>
<li>将大类中样本聚类到L个聚类中，然后训练L个分类器</li>
<li>每个分类器使用大类中的一个簇与所有的小类样本进行训练得到</li>
<li>最后对这L个分类器采取少数服从多数的方式对未知类别数据进行分类，如果是连续值，采用平均值</li>
</ul>
<h2 id="分层级ensemble"><a href="#分层级ensemble" class="headerlink" title="分层级ensemble"></a>分层级ensemble</h2><ul>
<li>使用原始数据集训练第一个学习器L1</li>
<li>将L1错分的数据集作为新的数据集训练L2</li>
<li>将L1和L2分类结果不一致的数据作为数据集训练L3</li>
<li>最后测试集上将三个分类器的结果汇总(结合这三个分类器，采用投票的方式来决定分类结果，因此只有当L2与L3都分类为false时，最终结果才为false，否则为true)</li>
</ul>
<h2 id="对小类错分进行加权惩罚"><a href="#对小类错分进行加权惩罚" class="headerlink" title="对小类错分进行加权惩罚"></a>对小类错分进行加权惩罚</h2><ul>
<li>对分类器的小类样本数据增加权值，降低大类样本的权重，从而使得分类器将重点集中在小类样本身上</li>
<li>一个具体做法是，在训练分类器时，若分类器将小类样本分错时，额外增加分类器一个小类样本分错代价，这个额外的代价可以使得分类器更加“关心”小类样本。如penalized-SVM和penalized-LDA算法</li>
<li>对小样本进行过采样(例如含L倍重复数据)，其实在计算小样本错分cost functions时会累加L倍的惩罚分数</li>
</ul>
<h2 id="尝试其它评价指标"><a href="#尝试其它评价指标" class="headerlink" title="尝试其它评价指标"></a>尝试其它评价指标</h2><ul>
<li>准确度这个评价指标在类别不均衡的分类任务中不够好，甚至会造成误导。可考虑更有说服力的评价指标。如混淆矩阵、精确度、召回率、F1得分，其中可关注Kappa和ROC曲线</li>
</ul>
<h2 id="尝试不同的分类算法"><a href="#尝试不同的分类算法" class="headerlink" title="尝试不同的分类算法"></a>尝试不同的分类算法</h2><ul>
<li>决策树在类别不均衡数据上表现不错。它使用基于类变量的划分规则去创建分类树，因此可以强制地将不同类别的样本分开</li>
<li>Lightgbm中有两个参数处理类别不平衡，分别是is_unbalance和scale_pos_weight</li>
<li>xgboost有一个参数类别不平衡，即scale_pos_weight</li>
</ul>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul>
<li><a href="https://www.jiqizhixin.com/articles/021704">机器学习中如何处理不平衡数据？</a></li>
<li><a href="https://blog.csdn.net/jiede1/article/details/70215477">SMOTE算法(人工合成数据)</a></li>
<li><a href="https://www.jianshu.com/p/13fc0f7f5565">SMOTE算法</a></li>
<li><a href="https://blog.csdn.net/anshuai_aw1/article/details/89177406">分类问题中类别不平衡问题的有效解决方法</a></li>
<li><a href="http://freewill.top/2017/04/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8817%EF%BC%89%EF%BC%9A%E9%9D%9E%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/">机器学习算法系列（17）：非平衡数据处理</a></li>
<li><a href="https://blog.csdn.net/weixin_44871660/article/details/90600522">样本不平衡处理</a></li>
<li><a href="https://imbalanced-learn.readthedocs.io/en/stable/index.html">imbalanced官网</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36093594">非平衡分类问题 | BalanceCascade方法及其Python实现</a></li>
<li><a href="https://blog.csdn.net/songhk0209/article/details/71484469">解决样本不平衡问题的奇技淫巧 汇总</a></li>
<li><a href="https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/">8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
        <tag>数据预处理</tag>
      </tags>
  </entry>
  <entry>
    <title>第一篇博客：为什么我要建博客和写博客</title>
    <url>/first-blog/</url>
    <content><![CDATA[<h1 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h1><p>很久之前就想搭建自己的博客了，只是嘛，一直都没有时间（划掉，就是懒）。这一次之所以突然下定决心要搞一波，完全是因为我的同桌真的是个超级大神，北大本科，杜克大学博士，也有一个自己的博客和介绍页，看着她的介绍页真的是超级牛，超级强，而且不仅是学习强，还在自己感兴趣的各个领域钻研的很深。让我不禁连连感叹，大牛的人生真的是超级强，真可谓最强鸡血，对比下来仿佛我白活了这么多年，没有留下任何值得留存的记录。</p>
<p>所以，受到这个启发（主要是刺激），我也要开始搭建自己的博客和个人主页，但同时我觉得我最好同时发布和维护3个平台，一个是CSDN，一个是公众号，一个是这个博客。毕竟文章写好之后，多发布一下几乎不费时间。</p>
<h1 id="为什么要选择github和hexo"><a href="#为什么要选择github和hexo" class="headerlink" title="为什么要选择github和hexo"></a>为什么要选择github和hexo</h1><p>选择github主要是因为能借助git的版本管理，顺便可以在github的热力图刷的好看一点，还有托管在github免费哇，不需要再另外维护云主机，不想当运维。省一笔主机钱，我只需要出钱买个域名即可，万网这个.tech域名买了10年也才199块，就是整个博客搭建中的唯一花销了</p>
<p>选择hexo是因为，能支持markdown的书写，和我现有的工具套件能配套上，可以无缝迁移过来，hexo的生态和主题都相对完善。</p>
<h1 id="过程和踩坑"><a href="#过程和踩坑" class="headerlink" title="过程和踩坑"></a>过程和踩坑</h1><h2 id="申请域名"><a href="#申请域名" class="headerlink" title="申请域名"></a>申请域名</h2><p>直接上<a href="https://wanwang.aliyun.com/">万网</a>购买自己的域名，做完实名认证之后即可先放着，详细步骤具体参考<a href="https://zhuanlan.zhihu.com/p/103860494">知乎</a>。</p>
<h2 id="安装node和hexo，并部署到github"><a href="#安装node和hexo，并部署到github" class="headerlink" title="安装node和hexo，并部署到github"></a>安装node和hexo，并部署到github</h2><p>具体参考<a href="https://zhuanlan.zhihu.com/p/105715224">知乎</a>，我是安装在macOS上，不需要搞这里面复杂的各种环境变量。</p>
<p>踩坑：我原本以为是建完git仓库后，把仓库pull下来，在里面初始化hexo，但后面看了一下，是要在空文件夹操作，并且后续发布到github的文件是hexo进行编译后的文件。</p>
<h2 id="域名解析"><a href="#域名解析" class="headerlink" title="域名解析"></a>域名解析</h2><p>能够部署之后保证通过github.io能访问即可做域名解析，具体参考<a href="https://zhuanlan.zhihu.com/p/103813944">知乎</a></p>
<h2 id="挑选主题"><a href="#挑选主题" class="headerlink" title="挑选主题"></a>挑选主题</h2><p>原本我想直接在<a href="https://hexo.io/themes/">官方的主题链接</a>里挑一个比较合适的，给自己定了几个标准：</p>
<ol>
<li>整体必须是简洁的，那种大量有图片装饰的，背景花哨的不考虑（原因是，挑图片暴露自己的垃圾审美，还要给每个博客挑配图太费心力了）</li>
<li>必须能支持公式、代码块高亮等的解析</li>
<li>偏好整体布局要简洁，偏好侧边栏在右边，并且偏好文章要有侧边栏</li>
<li>主题必须有开发者长期维护和更新</li>
<li>能有评论系统</li>
</ol>
<p>在上面看花了眼，都没有一个不合适的，看了大半天，猛然觉得自己挑选的思路不对，在最原始的未经过筛选的主题站里挑选，能不费劲吗？</p>
<p>转换思路，直接搜推荐的hexo主题，然后看到next主题是几乎完全符合我的要求的，然后发现next主题经历了好几个大版本的迭代，甚至github仓库都换了几次，直接上最新的8.0版本，拉下来</p>
<h2 id="next主题各种调整优化"><a href="#next主题各种调整优化" class="headerlink" title="next主题各种调整优化"></a>next主题各种调整优化</h2><p>next主题中可以进行自主化调整的地方还挺多的，而且8.0版本中，很多地方和以往版本中有不一样的调整方式，我尽量把我用到的写一下。所做的所有操作基本是改一下themes/next下的_config.yml，很少一部分是更改hexo下的_config.yml，偶尔会使用npm装个包</p>
<h3 id="设置首页信息"><a href="#设置首页信息" class="headerlink" title="设置首页信息"></a>设置首页信息</h3><figure class="highlight yaml"><figcaption><span>hexo/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">每天净瞎搞</span></span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">&#x27;关注：AI/CS/数学/自我提升等&#x27;</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&#x27;既然选择了远方，便只顾风雨兼程&#x27;</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">Shiqi</span> <span class="string">Lu</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">zh-CN</span></span><br><span class="line"><span class="attr">timezone:</span> <span class="string">&#x27;Asia/Shanghai&#x27;</span></span><br><span class="line"><span class="attr">url:</span> <span class="string">http://shiqi-lu.tech</span></span><br></pre></td></tr></table></figure>

<h3 id="风格选择"><a href="#风格选择" class="headerlink" title="风格选择"></a>风格选择</h3><p>我把四个风格都试了一遍，最后比较喜欢Gemini</p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Schemes</span></span><br><span class="line"><span class="comment"># scheme: Muse</span></span><br><span class="line"><span class="comment"># scheme: Mist</span></span><br><span class="line"><span class="comment"># scheme: Pisces</span></span><br><span class="line"><span class="attr">scheme:</span> <span class="string">Gemini</span></span><br></pre></td></tr></table></figure>

<h3 id="支持暗黑模式"><a href="#支持暗黑模式" class="headerlink" title="支持暗黑模式"></a>支持暗黑模式</h3><p>这可是个意外惊喜，还会根据系统的设置自动适配</p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Dark Mode</span></span><br><span class="line"><span class="attr">darkmode:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h3 id="设置建站时间"><a href="#设置建站时间" class="headerlink" title="设置建站时间"></a>设置建站时间</h3><figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">footer:</span></span><br><span class="line">  <span class="comment"># Specify the date when the site was setup. If not defined, current year will be used.</span></span><br><span class="line">  <span class="attr">since:</span> <span class="number">2020</span></span><br></pre></td></tr></table></figure>

<h3 id="设置网站脚注的信息（图标、备案等）"><a href="#设置网站脚注的信息（图标、备案等）" class="headerlink" title="设置网站脚注的信息（图标、备案等）"></a>设置网站脚注的信息（图标、备案等）</h3><figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">footer:</span></span><br><span class="line">  <span class="comment"># Icon between year and copyright info.</span></span><br><span class="line">  <span class="attr">icon:</span></span><br><span class="line">    <span class="comment"># Icon name in Font Awesome. See: https://fontawesome.com/icons</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">fa</span> <span class="string">fa-heart</span></span><br><span class="line">    <span class="comment"># If you want to animate the icon, set it to true.</span></span><br><span class="line">    <span class="attr">animated:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># Change the color of icon, using Hex Code.</span></span><br><span class="line">    <span class="attr">color:</span> <span class="string">&quot;#808080&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="网站图标"><a href="#网站图标" class="headerlink" title="网站图标"></a>网站图标</h3><p>先到网上找适合的图标，然后更新一下对应的文件，免费的图标素材网站：<a href="https://www.easyicon.net/1220579-maple_leaf_icon.html">Easyicon</a></p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">favicon:</span></span><br><span class="line">  <span class="attr">small:</span> <span class="string">/images/7-16.png</span></span><br><span class="line">  <span class="attr">medium:</span> <span class="string">/images/7-32.png</span></span><br><span class="line">  <span class="attr">apple_touch_icon:</span> <span class="string">/images/7-128.png</span></span><br><span class="line">  <span class="attr">safari_pinned_tab:</span> <span class="string">/images/7-128.png</span></span><br></pre></td></tr></table></figure>

<h3 id="标签页和分类页"><a href="#标签页和分类页" class="headerlink" title="标签页和分类页"></a>标签页和分类页</h3><p>参考<a href="https://theme-next.js.org/docs/theme-settings/custom-pages.html#Adding-%C2%ABTags%C2%BB-Page">next文档</a></p>
<h3 id="侧边栏"><a href="#侧边栏" class="headerlink" title="侧边栏"></a>侧边栏</h3><p>我喜欢放在右边，主要是因为视觉聚焦主要是在左边的</p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">sidebar:</span></span><br><span class="line">  <span class="comment"># Sidebar Position.</span></span><br><span class="line">  <span class="comment"># position: left</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">right</span></span><br></pre></td></tr></table></figure>

<h3 id="打开文章标题下方更新时间、阅读时长等信息"><a href="#打开文章标题下方更新时间、阅读时长等信息" class="headerlink" title="打开文章标题下方更新时间、阅读时长等信息"></a>打开文章标题下方更新时间、阅读时长等信息</h3><p>参考<a href="https://theme-next.js.org/docs/theme-settings/posts.html#Post-Wordcount">官方文档</a><br>先按照npm包：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ npm install hexo-word-counter</span><br><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><figcaption><span>hexo/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">symbols_count_time:</span></span><br><span class="line">  <span class="attr">symbols:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">total_symbols:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">total_time:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Post meta display settings</span></span><br><span class="line"><span class="attr">post_meta:</span></span><br><span class="line">  <span class="attr">item_text:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">created_at:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">updated_at:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">another_day:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">categories:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Post wordcount display settings</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/next-theme/hexo-word-counter</span></span><br><span class="line"><span class="attr">symbols_count_time:</span></span><br><span class="line">  <span class="attr">separated_meta:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">item_text_total:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h3 id="博客首页的摘要设置"><a href="#博客首页的摘要设置" class="headerlink" title="博客首页的摘要设置"></a>博客首页的摘要设置</h3><p>这个要配合文章中的description字段，或在文章中添加一行注释辅助，参考<a href="https://theme-next.js.org/docs/theme-settings/posts.html?highlight=more#Preamble-Text">官方文档</a></p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Automatically excerpt description in homepage as preamble text.</span></span><br><span class="line"><span class="attr">excerpt_description:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Read more button</span></span><br><span class="line"><span class="comment"># If true, the read more button will be displayed in excerpt section.</span></span><br><span class="line"><span class="attr">read_more_btn:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h3 id="置顶的百分比和顶部进度条"><a href="#置顶的百分比和顶部进度条" class="headerlink" title="置顶的百分比和顶部进度条"></a>置顶的百分比和顶部进度条</h3><p>默认给的颜色有点花哨，我改成了灰色</p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">back2top:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Back to top in sidebar.</span></span><br><span class="line">  <span class="attr">sidebar:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># Scroll percent label in b2t button.</span></span><br><span class="line">  <span class="attr">scrollpercent:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reading progress bar</span></span><br><span class="line"><span class="attr">reading_progress:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Available values: top | bottom</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">top</span></span><br><span class="line">  <span class="comment"># color: &quot;#37c6c0&quot;</span></span><br><span class="line">  <span class="attr">color:</span> <span class="string">&quot;#808080&quot;</span></span><br><span class="line">  <span class="attr">height:</span> <span class="string">3px</span></span><br></pre></td></tr></table></figure>

<h3 id="头像设置"><a href="#头像设置" class="headerlink" title="头像设置"></a>头像设置</h3><p>在url里放置本地图片或者图床链接</p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Sidebar Avatar</span></span><br><span class="line"><span class="attr">avatar:</span></span><br><span class="line">  <span class="comment"># Replace the default image and set the url here.</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">/images/7-128.png</span></span><br><span class="line">  <span class="comment"># If true, the avatar will be dispalyed in circle.</span></span><br><span class="line">  <span class="attr">rounded:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># If true, the avatar will be rotated with the cursor.</span></span><br><span class="line">  <span class="attr">rotated:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<h3 id="代码块高亮风格选择"><a href="#代码块高亮风格选择" class="headerlink" title="代码块高亮风格选择"></a>代码块高亮风格选择</h3><p>使用了hightlight.js的高亮样式</p>
<figure class="highlight yaml"><figcaption><span>hexo/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">highlight:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">line_number:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">auto_detect:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">tab_replace:</span> <span class="string">&#x27;    &#x27;</span></span><br><span class="line">  <span class="attr">wrap:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hljs:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">prismjs:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">preprocess:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">line_number:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">tab_replace:</span> <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">codeblock:</span></span><br><span class="line">  <span class="comment"># Code Highlight theme</span></span><br><span class="line">  <span class="comment"># All available themes: https://theme-next.js.org/highlight/</span></span><br><span class="line">  <span class="attr">theme:</span></span><br><span class="line">    <span class="attr">light:</span> <span class="string">default</span></span><br><span class="line">    <span class="attr">dark:</span> <span class="string">tomorrow-night-bright</span></span><br><span class="line">  <span class="attr">prism:</span></span><br><span class="line">    <span class="attr">light:</span> <span class="string">prism</span></span><br><span class="line">    <span class="attr">dark:</span> <span class="string">prism-dark</span></span><br><span class="line">  <span class="comment"># Add copy button on codeblock</span></span><br><span class="line">  <span class="attr">copy_button:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># Available values: default | flat | mac</span></span><br><span class="line">    <span class="attr">style:</span> <span class="string">flat</span></span><br></pre></td></tr></table></figure>

<h3 id="社交账号设置"><a href="#社交账号设置" class="headerlink" title="社交账号设置"></a>社交账号设置</h3><figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">social:</span></span><br><span class="line">  <span class="attr">GitHub:</span> <span class="string">https://github.com/shiqi-lu</span> <span class="string">||</span> <span class="string">fab</span> <span class="string">fa-github</span></span><br><span class="line">  <span class="attr">E-Mail:</span> <span class="string">mailto:traumlou@163.com</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-envelope</span></span><br><span class="line">  <span class="comment">#Weibo: https://weibo.com/yourname || fab fa-weibo</span></span><br><span class="line">  <span class="comment">#Google: https://plus.google.com/yourname || fab fa-google</span></span><br><span class="line">  <span class="comment">#Twitter: https://twitter.com/yourname || fab fa-twitter</span></span><br><span class="line">  <span class="comment">#FB Page: https://www.facebook.com/yourname || fab fa-facebook</span></span><br><span class="line">  </span><br><span class="line"><span class="attr">social_icons:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">icons_only:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">transition:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h3 id="支持本地搜索"><a href="#支持本地搜索" class="headerlink" title="支持本地搜索"></a>支持本地搜索</h3><p>参考<a href="https://theme-next.js.org/docs/third-party-services/search-services.html?highlight=search#Local-Search">官方文档</a><br>先装包：<code>$ npm install hexo-generator-searchdb </code></p>
<figure class="highlight yaml"><figcaption><span>hexo/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Local Search</span></span><br><span class="line"><span class="attr">search:</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">search.xml</span></span><br><span class="line">  <span class="attr">field:</span> <span class="string">post</span></span><br><span class="line">  <span class="attr">content:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">format:</span> <span class="string">html</span></span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Local search</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/next-theme/hexo-generator-searchdb</span></span><br><span class="line"><span class="attr">local_search:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># If auto, trigger search by changing input.</span></span><br><span class="line">  <span class="comment"># If manual, trigger search by pressing enter key or search button.</span></span><br><span class="line">  <span class="attr">trigger:</span> <span class="string">auto</span></span><br><span class="line">  <span class="comment"># Show top n results per article, show all results by setting to -1</span></span><br><span class="line">  <span class="attr">top_n_per_article:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment"># Unescape html strings to the readable one.</span></span><br><span class="line">  <span class="attr">unescape:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># Preload the search data when the page loads.</span></span><br><span class="line">  <span class="attr">preload:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<h3 id="评论系统"><a href="#评论系统" class="headerlink" title="评论系统"></a>评论系统</h3><p>评论系统选择了<a href="https://valine.js.org/">valine</a>，请参考<a href="https://theme-next.js.org/docs/third-party-services/comments.html?highlight=comme#Valine-China">next文档</a>，其中头像需要注册一下Gravatar，参考<a href="https://valine.js.org/avatar.html">头像配置</a>，这里的邮箱提醒好像有问题，官方说明的方法不能用了。这个以后再说吧，我也不想有个评论就给我发邮件，要真有比较紧急的事情，直接发我邮箱吧</p>
<h3 id="文章置顶"><a href="#文章置顶" class="headerlink" title="文章置顶"></a>文章置顶</h3><p>注意这里8.0更新之后，就不是通过安装插件改源码的方式实现，直接在文章的front-matter里面添加一个字段：sticky就可以实现了，值越高排的越前，默认为0是按照时间顺序，参考<a href="https://theme-next.js.org/docs/advanced-settings/front-matter.html?highlight=stick">官方文档</a></p>
<h3 id="文章赞赏"><a href="#文章赞赏" class="headerlink" title="文章赞赏"></a>文章赞赏</h3><p>要先准备好微信，支付宝等的二维码，然后放在images下或放在图床中</p>
<figure class="highlight yaml"><figcaption><span>themes/next/_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Donate (Sponsor) settings</span></span><br><span class="line"><span class="comment"># Front-matter variable (unsupport animation).</span></span><br><span class="line"><span class="attr">reward_settings:</span></span><br><span class="line">  <span class="comment"># If true, a donate button will be displayed in every article by default.</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">animation:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">comment:</span> <span class="string">觉得文章写得不错就请博主喝杯奶茶吧(*￣∇￣*)</span></span><br><span class="line"></span><br><span class="line"><span class="attr">reward:</span></span><br><span class="line">  <span class="attr">wechatpay:</span> <span class="string">/images/wechatpay.png</span></span><br><span class="line">  <span class="attr">alipay:</span> <span class="string">/images/alipay.png</span></span><br><span class="line">  <span class="comment">#paypal: /images/paypal.png</span></span><br><span class="line">  <span class="comment">#bitcoin: /images/bitcoin.png</span></span><br></pre></td></tr></table></figure>


<h1 id="简单的使用指南"><a href="#简单的使用指南" class="headerlink" title="简单的使用指南"></a>简单的使用指南</h1><h2 id="写新博文"><a href="#写新博文" class="headerlink" title="写新博文"></a>写新博文</h2><p>在blog目录下输入命令<code>hexo new post &lt;title&gt;</code>，会自动在<code>&lt;blog-dir&gt;/source/_posts</code>目录下生成对应的title文件，这时候用md编辑器打开写博客即可</p>
<h2 id="本地测试"><a href="#本地测试" class="headerlink" title="本地测试"></a>本地测试</h2><p>博文写完之后，因为各种不同的解析器和浏览器对md的支持会不一样，先本地看看效果，运行命令：<code>hexo clean &amp;&amp; hexo s</code>，然后根据提示在浏览器打开localhost:4000即可查看</p>
<h2 id="推送到网站上"><a href="#推送到网站上" class="headerlink" title="推送到网站上"></a>推送到网站上</h2><p>运行命令：<code>hexo clean &amp;&amp; hexo g -d</code>即可</p>
<h1 id="尚未完成部分"><a href="#尚未完成部分" class="headerlink" title="尚未完成部分"></a>尚未完成部分</h1><p>这部分以后看时间和心情做吧，每做一部分记录一部分吧</p>
<ul>
<li>SEO</li>
<li>个人简介</li>
<li>README</li>
<li>访问速度比较慢，考虑使用除github外的托管服务</li>
<li>考虑使用CI</li>
<li>考虑CDN加速</li>
<li>考虑把http转换成https</li>
<li>备案</li>
<li>图床替换成自己的域名</li>
<li>完善和链接一下领英</li>
<li>研究一下博客如何分享链接到微信</li>
</ul>
<h1 id="已知问题"><a href="#已知问题" class="headerlink" title="已知问题"></a>已知问题</h1><ul>
<li>在ipad上的safari显示的时候没有font awesome图标显示，文章内容侧边栏等显示不出来，但ipad的chrome没问题，iphone的safari也没问题，真是奇怪</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://kchen.cc/2016/11/12/hexo-instructions/">基于 Hexo 的全自动博客构建部署系统</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/102592286">从零开始搭建个人博客（超详细）</a></li>
<li><a href="https://hexo.io/zh-cn/docs/">Hexo官方文档</a></li>
<li><a href="https://github.com/next-theme/hexo-theme-next">Next8.0 Github</a></li>
<li><a href="https://theme-next.js.org/">Next8.0 文档</a></li>
</ul>
]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>感想</tag>
        <tag>自我提升</tag>
      </tags>
  </entry>
  <entry>
    <title>特征选择方法汇总</title>
    <url>/feature-selection/</url>
    <content><![CDATA[<h1 id="什么是特征选择"><a href="#什么是特征选择" class="headerlink" title="什么是特征选择"></a>什么是特征选择</h1><ul>
<li>对一个学习任务来说，给定属性集，有些属性很有用，另一些则可能没什么用。这里的属性即称为“特征”(feature)。对当前学习任务有用的属性称为“相关特征”(relevant feature)、没什么用的属性称为“无关特征”(irrelevant feature)。从给定的特征集合中选择出相关特征子集的过程，即“特征选择”(feature selection)</li>
</ul>
<h1 id="特征选择的目的"><a href="#特征选择的目的" class="headerlink" title="特征选择的目的"></a>特征选择的目的</h1><ul>
<li><p>1.简化模型，使模型更易于理解：去除不相关的特征会降低学习任务的难度。并且可解释性能对模型效果的稳定性有更多的把握</p>
</li>
<li><p>2.改善性能：节省存储和计算开销</p>
</li>
<li><p>3.改善通用性、降低过拟合风险：减轻维数灾难，特征的增多会大大增加模型的搜索空间，大多数模型所需要的训练样本随着特征数量的增加而显著增加。特征的增加虽然能更好地拟合训练数据，但也可能增加方差</p>
</li>
</ul>
<h1 id="使用特征选择的前提"><a href="#使用特征选择的前提" class="headerlink" title="使用特征选择的前提"></a>使用特征选择的前提</h1><ul>
<li><p>1.训练数据包含许多冗余或无用的特征，移除这些特征并不会导致丢失信息。其中冗余是指一个本身很有用的特征与另外一个有用的特征强相关，或它包含的信息能从其它特征推演出来</p>
</li>
<li><p>2.特征很多但样本相对较少</p>
</li>
</ul>
<h1 id="特征选择的4个步骤"><a href="#特征选择的4个步骤" class="headerlink" title="特征选择的4个步骤"></a>特征选择的4个步骤</h1><ul>
<li><p>1.产生过程：产生特征或特征子集候选集合</p>
</li>
<li><p>2.评价函数：衡量特征或特征子集的重要性或者好坏程度，即量化特征变量和目标变量之间的联系以及特征之间的相互联系。为了避免过拟合，可用交叉验证的方式来评估特征的好坏</p>
</li>
<li><p>3.停止准则：为了减少计算复杂度，需设定一个阈值，当评价函数值达到阈值后搜索停止</p>
</li>
<li><p>4.验证过程：在验证数据集上验证选出来的特征子集的有效性</p>
</li>
</ul>
<h1 id="特征选择的三个方法"><a href="#特征选择的三个方法" class="headerlink" title="特征选择的三个方法"></a>特征选择的三个方法</h1><ul>
<li><p>Filter(过滤法)</p>
</li>
<li><p>Wrapper(包装法)</p>
</li>
<li><p>Embedded(嵌入法)</p>
</li>
</ul>
<h1 id="Filter-过滤法"><a href="#Filter-过滤法" class="headerlink" title="Filter(过滤法)"></a>Filter(过滤法)</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><ul>
<li>按照发散性或相关性对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选，分为单变量过滤方法和多变量过滤方法</li>
</ul>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><ul>
<li><p>单变量过滤方法：不需要考虑特征之间的相互关系，按照特征变量和目标变量之间的相关性或互信息对特征进行排序，过滤掉最不相关的特征变量。优点是计算效率高、不易过拟合</p>
</li>
<li><p>多变量过滤方法：考虑特征之间的相互关系，常用方法有基于相关性和一致性的特征选择</p>
</li>
</ul>
<h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul>
<li>不依赖于任何机器学习方法，且不需要交叉验证，计算效率比较高</li>
</ul>
<h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ul>
<li>没有考虑机器学习算法的特点</li>
</ul>
<h2 id="图示"><a href="#图示" class="headerlink" title="图示"></a>图示</h2><ul>
<li><img src="http://anki190912.xuexihaike.com/20201014172426.png?imageView2/2/h/80"></li>
</ul>
<h2 id="常用的过滤方法"><a href="#常用的过滤方法" class="headerlink" title="常用的过滤方法"></a>常用的过滤方法</h2><h3 id="覆盖率"><a href="#覆盖率" class="headerlink" title="覆盖率"></a>覆盖率</h3><ul>
<li>即特征在训练集中出现的比例。若覆盖率很小，如有10000个样本，但某个特征只出现了5次，则次覆盖率对模型的预测作用不大，可删除</li>
</ul>
<h3 id="方差选择法"><a href="#方差选择法" class="headerlink" title="方差选择法"></a>方差选择法</h3><ul>
<li>先计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="comment"># 方差选择法，返回值为特征选择后的数据</span></span><br><span class="line"><span class="comment"># 参数threshold为方差的阈值</span></span><br><span class="line">VarianceThreshold(threshold=<span class="number">3</span>).fit_transform(iris.data)</span><br></pre></td></tr></table></figure>


<h3 id="Pearson-皮尔森-相关系数"><a href="#Pearson-皮尔森-相关系数" class="headerlink" title="Pearson(皮尔森)相关系数"></a>Pearson(皮尔森)相关系数</h3><ul>
<li>用于度量两个变量X和Y之间的线性相关性，结果的取值区间为[-1, 1]， -1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关性</li>
<li>计算方法为两个变量之间的协方差和标准差的商</li>
<li>$$\rho_{X, Y}=\frac{\operatorname{cov}(X, Y)}{\sigma_{X} \sigma_{Y}}=\frac{E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]}{\sigma_{X} \sigma_{Y}}$$</li>
<li>样本上的相关系数为</li>
<li>$$r=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}}$$</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="comment"># 选择K个最好的特征，返回选择特征后的数据</span></span><br><span class="line"><span class="comment"># 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，</span></span><br><span class="line"><span class="comment"># 输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。</span></span><br><span class="line"><span class="comment"># 在此为计算相关系数</span></span><br><span class="line"><span class="comment"># 其中参数k为选择的特征个数</span></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(map(<span class="keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, </span><br><span class="line">            k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h3 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h3><ul>
<li>检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量</li>
<li>$$\chi^{2}=\sum \frac{(A-E)^{2}}{E}$$</li>
<li>这个统计量的含义即自变量对因变量的相关性</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="comment">#选择K个最好的特征，返回选择特征后的数据</span></span><br><span class="line">SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h3 id="互信息法-KL散度、相对熵-和最大信息系数-Mutual-information-and-maximal-information-coefficient-MIC"><a href="#互信息法-KL散度、相对熵-和最大信息系数-Mutual-information-and-maximal-information-coefficient-MIC" class="headerlink" title="互信息法(KL散度、相对熵)和最大信息系数 Mutual information and maximal information coefficient (MIC)"></a>互信息法(KL散度、相对熵)和最大信息系数 Mutual information and maximal information coefficient (MIC)</h3><ul>
<li>评价定性自变量对定性因变量的相关性，评价类别型变量对类别型变量的相关性，互信息越大表明两个变量相关性越高，互信息为0时，两个变量相互独立。互信息的计算公式为</li>
<li>$$I(X ; Y)=\sum\limits_{x \in X} \sum\limits_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}=D_{K L}(p(x, y) | p(x) p(y))$$</li>
<li>其中，p(x)和p(y)为X和Y的边际概率分布函数，p(x,y)为X和Y的联合概率分布函数。直观上，互信息度量两个随机变量之间共享的信息，也可表示为由于X的引入而使Y的不确定性减少的量，这时互信息与信息增益相同</li>
<li>皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些</li>
<li>互信息不能直接用于特征选择的两个原因<ul>
<li>1.不属于度量方式，不能归一化，在不同数据上的结果不能做比较</li>
<li>2.对于连续变量的计算不是很方便(X和Y都是集合，$x_i,y$都是离散值)，通常变量需要先离散化，而互信息的结果对离散化的方式很敏感</li>
</ul>
</li>
<li>为了处理定量数据，提出了最大信息系数法，它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0, 1]</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> minepy <span class="keyword">import</span> MINE</span><br><span class="line"><span class="comment">#由于MINE的设计不是函数式的，定义mic方法将其为函数式的</span></span><br><span class="line"><span class="comment">#返回一个二元组，二元组的第2项设置成固定的P值0.5</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mic</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    m = MINE()</span><br><span class="line">    m.compute_score(x, y)</span><br><span class="line">    <span class="keyword">return</span> (m.mic(), <span class="number">0.5</span>)</span><br><span class="line"><span class="comment">#选择K个最好的特征，返回特征选择后的数据</span></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(map(<span class="keyword">lambda</span> x:mic(x, Y), X.T)).T, </span><br><span class="line">            k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h3 id="Fisher得分"><a href="#Fisher得分" class="headerlink" title="Fisher得分"></a>Fisher得分</h3><ul>
<li>对于分类问题，好的特征应该是在同一个类别中的取值比较相似，而在不同类别之间的取值差异比较大。因此特征i的重要性可用Fiser得分$S_i$来表示</li>
<li>$$S_{i}=\frac{\sum_{j=1}^{K} n_{j}\left(\mu_{i j}-\mu_{i}\right)^{2}}{\sum_{j=1}^{K} n_{j} \rho_{i j}^{2}}$$</li>
<li>其中，$u_{ij}$和$\rho_{ij}$分别是特征i在类别j中均值和方差，$\mu_i$为特征i的均值，$n_j$为类别j中的样本数。Fisher得分越高，特征在不同类别之间的差异性越大、在同一类别中的差异性越小，则特征越重要</li>
</ul>
<h3 id="相关特征选择-Correlation-Feature-Selection-CFS"><a href="#相关特征选择-Correlation-Feature-Selection-CFS" class="headerlink" title="相关特征选择(Correlation Feature Selection, CFS)"></a>相关特征选择(Correlation Feature Selection, CFS)</h3><ul>
<li>该方法基于的假设是，好的特征集合包含跟目标变量非常相关的特征，但这些特征之间彼此不相关。对于包含k个特征的集合，CFS准则定义为</li>
<li>$$\mathrm{CFS}=\max \limits_{S_{k}}\left[\frac{r_{c f_{1}}+r_{c f_{2}}+\cdots+r_{c f_{k}}}{\sqrt{k+2\left(r_{f_{1} f_{2}}+\cdots+r_{f_{i} f_{j}}+\cdots+r_{f_{k} f_{1}}\right)}}\right]$$</li>
<li>其中，$r_{cf_i}$和$r_{f_i f_j}$是特征变量和目标变量之间，以及特征变量和特征变量之间的相关性，这里的相关性不一定是皮尔森相关系数或斯皮尔曼相关系数</li>
</ul>
<h3 id="最小冗余最大相关性-Minimum-Redundancy-Maximum-Relevance-mRMR"><a href="#最小冗余最大相关性-Minimum-Redundancy-Maximum-Relevance-mRMR" class="headerlink" title="最小冗余最大相关性(Minimum Redundancy Maximum Relevance, mRMR)"></a>最小冗余最大相关性(Minimum Redundancy Maximum Relevance, mRMR)</h3><ul>
<li>由于单变量过滤法只考虑了单特征变量和目标变量之间的相关性，因此选择的特征子集可能过于冗余。mRMR在进行特征时考虑到了特征之间的冗余性，具体做法是对跟已选择特征相关性较高的冗余特征进行惩罚</li>
<li>mRMR可以使用多种相关性的度量指标，如互信息、相关系数以及其它距离或相似度分数</li>
<li>以互信息为例，特征集合S和目标变量c之间的相关性可定义为，特征集合中所有单个特征变量$f_i$和目标变量c的互信息值$I(f_i;c)$的平均值：</li>
<li>$$D(S, c)=\frac{1}{|S|} \sum\limits_{f_{i} \in S} I\left(f_{i} ; c\right)$$</li>
<li>S中所有特征的冗余性为所有特征变量之间的互信息$I(f_i;f_i)$的平均值</li>
<li>$$R(S)=\frac{1}{|S|^{2}} \sum\limits_{f_{i}, f_{j} \in S} I\left(f_{i} ; f_{j}\right)$$</li>
<li>则mRMR准则为</li>
<li>$$\operatorname{mRMR}=\max \limits_{S}[D(S, c)-R(S)]$$</li>
<li>通过求解上述优化问题即可得到特征子集</li>
<li>在一些特定的情况下，mRMR算法可能对特征的重要性估计不足，它没有考虑到特征之间的组合可能与目标变量比较相关。如果单个特征的分类能力都比较弱，但进行组合后分类能力很强，这时mRMR方法效果一般比较差(如目标变量由特征变量之间进行XOR运算得到)</li>
<li>mRMR是一种典型的进行特征选择的增量贪心策略：某个特征一旦被选择了，在后续的步骤不会删除</li>
<li>mRMR可改写为全局的二次规划的优化问题(即特征集合为特征全集的情况)：</li>
<li>$$\mathrm{QPFS}=\min\limits_{x}\left[\alpha \boldsymbol{x}^{\mathrm{T}} \boldsymbol{H} \boldsymbol{x}-\boldsymbol{x}^{\mathrm{T}} \boldsymbol{F}\right] \mathrm{s.t.} \sum\limits_{i=1}^{n} x_{i}=1, x_{i} \geqslant 0$$</li>
<li>其中$\boldsymbol{F}$为特征变量和目标变量相关性向量，$\boldsymbol{H}$为度量特征变量之间的冗余性的矩阵。QPFS可通过二次规划求解，QPFS偏向于选择熵比较小的特征，这是因为特征自身的冗余性$I(f_i;f_j)$</li>
<li>另一种全局的基于互信息的方法是基于条件相关性的</li>
<li>$$\mathrm{SPEC}_{\mathrm{CMI}}=\max\limits_{x}\left[\boldsymbol{x}^{\mathrm{T}} \boldsymbol{Q} \boldsymbol{x}\right] \text { s.t. }|x|=1, x_{i} \geqslant 0$$</li>
<li>其中，$Q_{i i}=I\left(f_{i} ; c\right), Q_{i j}=I\left(f_{i} ; c \mid f_{j}\right), i \neq j$。$\mathrm{SPEC}_{\mathrm{CMI}}$方法的优点是可以通过求解矩阵Q的主特征向量来求解，而且可以处理二阶的特征组合</li>
</ul>
<h1 id="Wrapper-包装法"><a href="#Wrapper-包装法" class="headerlink" title="Wrapper(包装法)"></a>Wrapper(包装法)</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><ul>
<li>使用机器学习算法评估特征子集的效果，可以检测两个或多个特征之间的交互关系，而且选择的特征子集让模型的效果达到最优。</li>
<li>这是特征子集搜索和评估指标相结合的方法。前者提供候选的新特征子集，后者基于新特征子集训练一个模型，并用验证集进行评估，为每一组特征子集进行打分</li>
<li>最简单的方法是在每一个特征子集上训练并评估模型，从而找出最优的特征子集</li>
</ul>
<h2 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h2><ul>
<li>1.需要对每一组特征子集训练一个模型，计算量很大</li>
<li>2.样本不够充分的情况下容易过拟合</li>
<li>3.特征变量较多时计算复杂度太高</li>
</ul>
<h2 id="图示-1"><a href="#图示-1" class="headerlink" title="图示"></a>图示</h2><p><img src="http://anki190912.xuexihaike.com/20201014173616.png?imageView2/2/h/120"></p>
<h2 id="3种常用的特征子集搜索方法"><a href="#3种常用的特征子集搜索方法" class="headerlink" title="3种常用的特征子集搜索方法"></a>3种常用的特征子集搜索方法</h2><h3 id="1-完全搜索"><a href="#1-完全搜索" class="headerlink" title="1.完全搜索"></a>1.完全搜索</h3><ul>
<li>即穷举法，遍历所有可能的组合达到全局最优，时间复杂度$2^n$</li>
</ul>
<h3 id="2-启发式搜索"><a href="#2-启发式搜索" class="headerlink" title="2.启发式搜索"></a>2.启发式搜索</h3><ul>
<li>序列向前选择：特征子集从空集开始，每次只加入一个特征，时间复杂度为$O(n+(n-1)+(n-2)+\ldots+1)=O\left(n^{2}\right)$</li>
<li>序列向后选择：特征子集从全集开始，每次删除一个特征，时间复杂度为$O(n^{2})$</li>
</ul>
<h3 id="3-随机搜索"><a href="#3-随机搜索" class="headerlink" title="3.随机搜索"></a>3.随机搜索</h3><ul>
<li>执行序列向前或向后选择时，随机选择特征子集</li>
</ul>
<h3 id="4-递归特征消除法"><a href="#4-递归特征消除法" class="headerlink" title="4.递归特征消除法"></a>4.递归特征消除法</h3><ul>
<li>使用一个基模型进行多轮训练，每轮训练后通过学习器返回的coef_或者feature_importances_消除若干权重较低的特征，再基于新的特征集进行下一轮训练</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment">#递归特征消除法，返回特征选择后的数据</span></span><br><span class="line"><span class="comment">#参数estimator为基模型</span></span><br><span class="line"><span class="comment">#参数n_features_to_select为选择的特征个数</span></span><br><span class="line">RFE(estimator=LogisticRegression(), </span><br><span class="line">    n_features_to_select=<span class="number">2</span>).fit_transform(iris.data, </span><br><span class="line">                                          iris.target)</span><br></pre></td></tr></table></figure>


<h1 id="Embedded-嵌入法"><a href="#Embedded-嵌入法" class="headerlink" title="Embedded(嵌入法)"></a>Embedded(嵌入法)</h1><h2 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h2><ul>
<li>将特征选择嵌入到模型的构建过程中，具有包装法与机器学习算法相结合的优点，也具有过滤法计算效率高的优点</li>
</ul>
<h2 id="图示-2"><a href="#图示-2" class="headerlink" title="图示"></a>图示</h2><ul>
<li><img src="http://anki190912.xuexihaike.com/20201014183741.png?imageView2/2/h/120"></li>
</ul>
<h2 id="LASSO方法"><a href="#LASSO方法" class="headerlink" title="LASSO方法"></a>LASSO方法</h2><ul>
<li>使用LASSO(Least Absolute Shrinkage and Selection Operator)方法</li>
<li>$$\min\limits_{\beta \in \mathbb{R}^{p}}\{\frac{1}{N}|y-X \boldsymbol{\beta}|_{2}^{2}+\lambda|\boldsymbol{\beta}| _{1}\}$$</li>
<li>通过对回归系数添加$L_1$惩罚项来防止过拟合，可以让特定的回归系数变为0，从而可以选择一个不包含那些系数的更简单的模型</li>
<li>实际应用中，$\lambda$越大，回归系数越稀疏，$\lambda$一般采用交叉验证的方式来确定</li>
<li>线性回归、逻辑回归、FM/FFM以及神经网络都可以添加$L_1$惩罚项</li>
<li>即使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维</li>
<li>实际上，L1惩罚项降维的原理是，在多个对目标值具有同等相关性的特征中，只保留一个，所以没保留的特征并不代表不重要</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment">#带L1惩罚项的逻辑回归作为基模型的特征选择</span></span><br><span class="line">SelectFromModel(LogisticRegression(</span><br><span class="line">          penalty=<span class="string">&quot;l1&quot;</span>, C=<span class="number">0.1</span>)).fit_transform(</span><br><span class="line">               iris.data,iris.target)</span><br></pre></td></tr></table></figure>


<h2 id="基于树模型的特征选择方法"><a href="#基于树模型的特征选择方法" class="headerlink" title="基于树模型的特征选择方法"></a>基于树模型的特征选择方法</h2><ul>
<li>在决策树中，深度较浅的节点一般对应的特征分类能力更强(可以将更多的样本区分开)</li>
<li>对于基于决策树的算法，如随机森林，重要的特征更有可能出现在深度较浅的节点，而且出现的次数可能越多</li>
<li>即可基于树模型中特征出现次数等指标对特征进行重要性排序<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line">  <span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">  <span class="comment">#GBDT作为基模型的特征选择</span></span><br><span class="line">SelectFromModel(</span><br><span class="line">      GradientBoostingClassifier()).fit_transform(</span><br><span class="line">        iris.data,iris.target)</span><br></pre></td></tr></table></figure>


</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><p>美团机器学习实践2.2节</p>
</li>
<li><p>精通特征工程2.6节</p>
</li>
<li><p>特征工程入门与实践第5章</p>
</li>
<li><p>机器学习-周志华第11章</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/74198735">【机器学习】特征选择(Feature Selection)方法汇总</a></p>
</li>
<li><p><a href="https://www.zhihu.com/question/28641663">机器学习中，有哪些特征选择的工程方法？</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s培训第4课 资源清单</title>
    <url>/k8s-4/</url>
    <content><![CDATA[<p>来源：<a href="https://www.bilibili.com/video/BV1w4411y7Go?p=1">b站</a></p>
<ul>
<li>Q:集群资源有哪些分类？<ul>
<li>名称空间级别<ul>
<li>工作负载型资源(workload): Pod、ReplicaSet、Deployment、StatefulSet、DaemonSet、Job、CronJob</li>
<li>服务发现及负载均衡型资源(ServiceDiscovery LoadBalance)：Service、Ingress</li>
<li>配置与存储型资源：Volume(存储卷)、CSI(容器存储接口，可扩展各种第三方存储卷)</li>
<li>特殊类型的存储卷：ConfigMap(当配置中心来使用的资源类型)、Secret(保存敏感数据)、DownwardAPI(把外部环境中的信息输出给容器)</li>
</ul>
</li>
<li>集群级别：Namespace、Node、Role、ClusterRole、RoleBinding、ClusterRoleBinding</li>
<li>元数据级别：HPA、PodTemplate、LimitRange</li>
</ul>
</li>
<li>Q:容器报错后如何处理？<ul>
<li><code>kubectl describe pod myapp-pod</code></li>
<li>若有多个容器需要用-c指定，查看日志</li>
<li><code>kubectl log myapp-pod -c test</code></li>
</ul>
</li>
<li>Q:initContainer模板是什么？<ul>
<li>initContainers里面会按照顺序依次往下执行</li>
</ul>
</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myapp-pod</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">myapp</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">myapp-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;echo The app is running! &amp;&amp; sleep 3600&#x27;</span>]</span><br><span class="line">  <span class="attr">initContainers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">init-myservice</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">  <span class="attr">command:</span> [<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;until nslookup myservice; do echo waiting for myservice; sleep 2; done;&#x27;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">init-mydb</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">  <span class="attr">command:</span> [<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;until nslookup mydb; do echo waiting for mydb; sleep 2; done;&#x27;</span>]</span><br></pre></td></tr></table></figure>

<ul>
<li>Q:检测探针-就绪检测模板？readinessProbe-httpget</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">readiness-httpget-pod</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">readiness-httpget-container</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">wangyanglinux/myapp:v1</span></span><br><span class="line">  <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">readinessProbe:</span></span><br><span class="line">    <span class="attr">httpGet:</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/index1.html</span></span><br><span class="line">    <span class="attr">initialDelaySeconds:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">periodSeconds:</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Q:检测探针-存活检测模板？livenessProbe-exec</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">liveness-exec-pod</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">liveness-exec-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">hub.atguigu.com/library/busybox</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>,<span class="string">&quot;-c&quot;</span>,<span class="string">&quot;touch /tmp/live ; sleep 60; rm -rf /tmp/live; sleep 3600&quot;</span>]</span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">exec:</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;test&quot;</span>,<span class="string">&quot;-e&quot;</span>,<span class="string">&quot;/tmp/live&quot;</span>]</span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Q:检测探针-存活检测模板？livenessProbe-httpget</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">liveness-httpget-pod</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">liveness-httpget-container</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">hub.atguigu.com/library/myapp:v1</span></span><br><span class="line">  <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">livenessProbe:</span></span><br><span class="line">    <span class="attr">httpGet:</span></span><br><span class="line">      <span class="attr">port:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">    <span class="attr">initialDelaySeconds:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">periodSeconds:</span> <span class="number">3</span></span><br><span class="line">    <span class="attr">timeoutSeconds:</span> <span class="number">10</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Q:检测探针-存活检测模板？livenessProbe-tcp</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">probe-tcp</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">hub.atguigu.com/library/myapp:v1</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">      <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">tcpSocket:</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Q:启动、退出动作pod模板</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">lifecycle-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">lifecycle-demo-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">lifecycle:</span></span><br><span class="line">      <span class="attr">postStart:</span></span><br><span class="line">        <span class="attr">exec:</span></span><br><span class="line">          <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;echo Hello from the postStart handler &gt; /usr/share/message&quot;</span>]</span><br><span class="line">      <span class="attr">preStop:</span></span><br><span class="line">        <span class="attr">exec:</span></span><br><span class="line">          <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;echo Hello from the poststop handler &gt; /usr/share/message&quot;</span>]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s培训第5课 资源管理器</title>
    <url>/k8s-5/</url>
    <content><![CDATA[<p>来源：<a href="https://www.bilibili.com/video/BV1w4411y7Go?p=1">b站</a></p>
<ul>
<li>Q:Pod有哪些分类？<ul>
<li>自主式Pod：Pod退出了，此类型的Pod不会被创建</li>
<li>控制器管理的Pod：在控制器的生命周期里，始终要维持Pod的副本数目</li>
</ul>
</li>
<li>Q:什么是控制器？<ul>
<li>k8s中内建了很多controller(控制器)，这些相当于一个状态机，用来控制Pod的具体状态和行为</li>
</ul>
</li>
<li>Q:控制器有哪些类型？<ul>
<li>ReplicationController和ReplicaSet</li>
<li>Deployment</li>
<li>DaemonSet</li>
<li>StateFulSet</li>
<li>Job/CronJob</li>
<li>Horizontal Pod Autoscaling</li>
</ul>
</li>
<li>Q:命令式编程和声明式编程的区别是什么？<ul>
<li>命令式编程：侧重于如何实现，需要把程序的实现过程按照逻辑结果一步步写下来，k8s使用create命令</li>
<li>声明式编程：侧重于定义想要什么，然后告诉计算机/引擎，让它帮忙实现，k8s使用apply命令</li>
</ul>
</li>
<li>Q:ReplicationController和ReplicaSet是什么？<ul>
<li>ReplicationController(RC)用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的Pod来替代，而如果异常多出来的容器也会自动回收</li>
<li>在新版本的k8s中建议使用ReplicaSet来取代ReplicationController。ReplicaSet跟ReplicationController没有本质的不同，只是名字不一样，并且ReplicaSet支持集合式的selector</li>
<li>创建命令<code>kubectl create -f rs.yaml</code><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ReplicaSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">selector:</span></span><br><span class="line">  <span class="attr">matchLabels:</span></span><br><span class="line">    <span class="attr">tier:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">template:</span></span><br><span class="line">  <span class="attr">metadata:</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">tier:</span> <span class="string">frontend</span></span><br><span class="line">  <span class="attr">spec:</span></span><br><span class="line">    <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">php-redis</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">gcr.io/google_samples/gb-frontend:v3</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">GET_HOSTS_FROM</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">dns</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>


</li>
</ul>
</li>
</ul>
<ul>
<li>Q:Deployment是什么？<ul>
<li>为Pod和ReplicaSet提供了一个声明式定义(declarative)方法，用来替代以前的ReplicationController来方便的管理应用</li>
<li>典型场景有：<ul>
<li>定义Deployment来创建Pod和ReplicaSet</li>
<li>滚动升级和回滚应用</li>
<li>扩容和缩容</li>
<li>暂停和继续Deployment</li>
</ul>
</li>
<li><img src="http://anki190912.xuexihaike.com/20200921170358.png"></li>
</ul>
</li>
<li><h2 id="Q-如何使用Deployment部署一个简单的Nginx应用"><a href="#Q-如何使用Deployment部署一个简单的Nginx应用" class="headerlink" title="Q:如何使用Deployment部署一个简单的Nginx应用"></a>Q:如何使用Deployment部署一个简单的Nginx应用</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>
<ul>
<li>创建命令：<code>kubectl create -f https://kubernetes.io/docs/user-guide/nginx-deployment.yaml --record</code></li>
<li>其中–record参数可以记录命令，可以很方便的查看每次revision的变化</li>
</ul>
</li>
<li>Q:如何查看deployment状态？<ul>
<li><code>kubectl get deployment</code></li>
</ul>
</li>
<li>Q:如何对Deployment进行扩容？<ul>
<li><code>kubectl scale deployment nginx-deployment --replicas 10</code></li>
</ul>
</li>
<li>Q:如何更新deployment镜像？<ul>
<li><code>kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1</code></li>
<li>=前是容器名，后是镜像</li>
</ul>
</li>
<li>Q:如何回滚deployment<ul>
<li><code>kubectl rollout undo deployment/nginx-deployment</code></li>
</ul>
</li>
<li>Q:如何查看可回滚的deployment历史版本？<ul>
<li><code>kubectl rollout history deployment/nginx-deployment</code></li>
</ul>
</li>
<li>Q:如何回滚deployment到指定的历史版本？<ul>
<li><code>kubectl rollout undo deployment/nginx-deployment --to-version=2</code></li>
</ul>
</li>
<li>Q:如何查看rollout状态？<ul>
<li><code>kubectl rollout status deployment/nginx-deployment</code></li>
</ul>
</li>
<li>Q:如何编辑Deployment？<ul>
<li><code>kubectl edit deployment/nginx-deployment</code></li>
</ul>
</li>
<li>Q:Deployment的清理策略是什么？<ul>
<li>可以通过设置<code>.spec.revisonHistoryLimit</code>项来指定deployment最多保留多少revision历史记录。默认的会保留所有的revision；如果将该项设置为0，Deployment就不允许回退了</li>
</ul>
</li>
<li>Q:Deployment的更新策略是什么？<ul>
<li>保证在升级时只有一定数量的Pod是down的。默认的，它会确保至少有比期望的Pod数量少一个是up状态（最多一个不可用）</li>
<li>Deployment 同时也可以确保只创建出超过期望数量的一定数量的Pod。默认的，它会确保最多比期望的Pod数 量多一个的Pod是up的（最多1个surge）</li>
<li>未来的 Kuberentes 版本中，将从1-1变成25%-25%</li>
</ul>
</li>
<li>Q:DaemonSet是什么？<ul>
<li>确保全部(或者一些)Node上运行一个Pod的副本。当有Node加入集群时，也会为它们新增一个Pod。当有Node从集群移除时，这些Pod也会被回收。删除DaemonSet将会删除它创建的所有Pod</li>
<li>使用DaemonSet的一些典型用法：<ul>
<li>运行集群存储daemon，例如在每个Node上运行glusterd、ceph</li>
<li>在每个Node上运行日志手机daemon，例如fluentd、logstash</li>
<li>在每个Node上运行监控daemon，例如Prometheus Node Exporter、collectd、Datadog代理、New Relic代理，或Ganglia gmond<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deamonset-example</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">daemonset</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="comment"># 注意这个name和上面的metadata中的name必须要一致</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">deamonset-example</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">deamonset-example</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">daemonset-example</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">wangyanglinux/myapp:v1</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li>Q:Job的作用是什么？<ul>
<li>负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个Pod成功结束</li>
<li>特殊说明<ul>
<li>spec.template格式同Pod</li>
<li>RestartPolicy仅支持Never或OnFailure</li>
<li>单个Pod时，默认Pod成功运行后Job即结束</li>
<li><code>.spec.completions</code>标志Job结束需要成功运行的Pod个数，默认为1</li>
<li><code>.spec.parallelism</code>标志并行运行的Pod的个数，默认为1</li>
<li><code>spec.activeDeadlineSeconds</code>标志失败Pod的重试最大时间，超过这个时间不会继续重试<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pi</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">pi</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pi</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">perl</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;perl&quot;</span>, <span class="string">&quot;-Mbignum=bpi&quot;</span>, <span class="string">&quot;-wle&quot;</span>, <span class="string">&quot;print bpi(2000)&quot;</span>] </span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>创建：<code>kubectl create -f job.yaml</code></li>
</ul>
</li>
<li>Q:CronJob的作用是什么？<ul>
<li>管理基于时间的Job，即：<ul>
<li>在给定时间点只运行一次</li>
<li>周期性地在给定时间点运行</li>
</ul>
</li>
<li>典型用法是：<ul>
<li>在给定的时间点调度Job运行</li>
<li>创建周期性运行的Job，如数据库备份、发送邮件</li>
</ul>
</li>
</ul>
</li>
<li>Q:如何查看cronjob状态？<ul>
<li><code>kubectl get cronjob</code></li>
</ul>
</li>
<li>Q:StatefulSet是什么？<ul>
<li>作为Controller为Pod提供唯一的标识。可以保证部署和scale的顺序</li>
<li>是为了解决有状态服务的问题(对应Delpoyments和ReplicaSets是为无状态服务而设计)，其应用场景包括：<ul>
<li>稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现</li>
<li>稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service(即没有Cluster IP的Service)来实现</li>
<li>有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次进行，基于init containers来实现</li>
<li>有序收缩，有序删除</li>
</ul>
</li>
</ul>
</li>
<li>Q:Horizontal Pod Autoscaling是什么？<ul>
<li>应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让service中的pod个数自动调整呢？就是Horizontal Pod Autoscaling，即使Pod水平自动缩放</li>
</ul>
</li>
<li>Q:如何查看命令详细信息如rs？<ul>
<li><code>kubectl explain rs</code></li>
</ul>
</li>
<li>Q:如何删除所有pod？<ul>
<li><code>kubectl delete pod --all</code></li>
</ul>
</li>
<li>Q:查看pod状态的时候如何查看标签？<ul>
<li><code>kubectl get pod --show-labels</code></li>
</ul>
</li>
<li>Q:如何给pod添加标签？<ul>
<li><code>kubectl label pod frontend-m8szd tier=frontend1</code></li>
<li>即给frontend-m8szd这个容器加了一个标签，若已存在，需要使用–overwrite=True来覆盖</li>
</ul>
</li>
<li>Q:如何删除所有rs？<ul>
<li><code>kubectl delete rs --all</code></li>
</ul>
</li>
<li>Q:如何查看pod的详细信息？<ul>
<li><code>kubectl get pod -o wide</code></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习项目清单</title>
    <url>/ml-checklist/</url>
    <content><![CDATA[<h1 id="1-框出问题并看整体"><a href="#1-框出问题并看整体" class="headerlink" title="1.框出问题并看整体"></a>1.框出问题并看整体</h1><ul>
<li>1.用业务术语定义目标。</li>
<li>2.你的解决方案将如何使用?</li>
<li>3.当前有什么解决方案/解决方法（如果有）?</li>
<li>4.你应该如何阐述这个问题（有监督/无监督，在线/离线等）？</li>
<li>5.应该如何衡量性能?</li>
<li>6.性能指标是否符合业务目标?</li>
<li>7.达到业务目标所需的最低性能是多少？</li>
<li>8.有没有一些相似的向题？你可以重用经验或工具吗？</li>
<li>9.有没有相关有经验的人？</li>
<li>10.你会如何手动解决问题？</li>
<li>11.列出你（或其他人）到目前为止所做的假设。</li>
<li>12.如果可能，请验证假设。</li>
</ul>
<h1 id="2-获取数据"><a href="#2-获取数据" class="headerlink" title="2.获取数据"></a>2.获取数据</h1><ul>
<li>注意：尽可能地自动化，以便你可以轻松地获取新数据</li>
<li>1.列出所需的数据以及你需要多少数据。</li>
<li>2.查找并记录可从何处获取该数据。</li>
<li>3.检查将占用多少空间。</li>
<li>4.检查法律义务，并在必要时获得授权。</li>
<li>5.获取访问授权。</li>
<li>6.创建一个工作空间（具有足够的存储空间）。</li>
<li>7.获取数据。</li>
<li>8.将数据转换为可以轻松操作的格式（无须更改数据本身）。</li>
<li>9.确保敏感信息被删除或受保护（例如匿名）。</li>
<li>10.检查数据的大小和类型（时间序列、样本、地理等）。</li>
<li>11.抽样一个测试集，将其放在一边，再也不要看它（无数据监听！）。</li>
</ul>
<h1 id="3-研究数据"><a href="#3-研究数据" class="headerlink" title="3.研究数据"></a>3.研究数据</h1><ul>
<li>注意：请尝试从现场专家那里获取有关这些步骤的见解。</li>
<li>1.创建数据副本来进行研究（必要时将其采样到可以管理的大小）。</li>
<li>2.创建Jupyter notebook以记录你的数据研究。</li>
<li>3.研究每个属性及其特征：<ul>
<li>名称</li>
<li>类型（分类、整数/浮点型、有界/无界、文本、结构化等）</li>
<li>缺失值的百分比</li>
<li>噪声和噪声类型（随机、异常值、舍入误差等）</li>
<li>任务的实用性</li>
<li>分布类型（高斯分布、均匀分布、对数分布等）</li>
</ul>
</li>
<li>4.对于有监督学习任务，请确定目标属性。</li>
<li>5.可视化数据。</li>
<li>6.研究属性之间的相关性。</li>
<li>7.研究如何手动解决问题。</li>
<li>8.确定你可能希望使用的转变。</li>
<li>9.确定有用的额外数据。</li>
<li>10.记录所学的知识。</li>
</ul>
<h1 id="4-准备数据"><a href="#4-准备数据" class="headerlink" title="4.准备数据"></a>4.准备数据</h1><ul>
<li>注意：<ul>
<li>在数据副本上工作（保持原始数据集完整）。</li>
<li>为你应用的所有数据转换编写函数，原因有5个：<ul>
<li>下次获取新的数据集时，你可以轻松准备数据。</li>
<li>可以在未来的项目中应用这些转换。</li>
<li>清理并准备测试集。</li>
<li>解决方案上线后清理并准备新的数据实例。</li>
<li>使你可以轻松地将准备选择视为超参数。</li>
</ul>
</li>
</ul>
</li>
<li>1.数据清理：<ul>
<li>修复或删除异常值（可选）。</li>
<li>填写缺失值（例如，零、均值、中位数）或删除其行（或列）。</li>
</ul>
</li>
<li>2.特征选择（可选）：<ul>
<li>删除没有为任务提供有用信息的属性。</li>
</ul>
</li>
<li>3.特征工程（如果适用）：<ul>
<li>离散化连续特征。</li>
<li>分解特征（例如分类、日期/时间等）。</li>
<li>添加有希望的特征转换(如 Iog(x)、sqrt(x）、$x^2$等）</li>
<li>将特征聚合为有希望的新特征。</li>
</ul>
</li>
<li>4.特征缩放：<ul>
<li>标准化或归一化特征。</li>
</ul>
</li>
</ul>
<h1 id="5-列出有前途的模型"><a href="#5-列出有前途的模型" class="headerlink" title="5.列出有前途的模型"></a>5.列出有前途的模型</h1><ul>
<li>注意：<ul>
<li>如果数据量巨大，则可能需要采样为较小的训练集，以便可以在合理的时间内训练许多不同的模型(请注意，这会对诸如大型神经网络或随机森林之类的复杂模型造成不利影响)</li>
<li>尽可能自动化地执行这些步骤</li>
</ul>
</li>
<li>1.使用标准参数训练来自不同类别（例如线性、朴素贝叶斯、SVM，随机森林、神经网络等）的许多快速和粗糙的模型。</li>
<li>2.衡量并比较其性能。</li>
<li>对于每个模型，使用N折交叉验证，在N折上计算性能度量的均值和标准差。</li>
<li>3.分析每种算法的最重要的变量。</li>
<li>4.分析模型所犯错误的类型。<ul>
<li>人类将使用什么数据来避免这些错误?</li>
</ul>
</li>
<li>5.快速进行特征选择和特征工程。</li>
<li>6.在前面5个步骤中执行一两个以上的快速迭代。</li>
<li>7.筛选出前三到五个最有希望的模型，优先选择会产生不同类型错误的模型。</li>
</ul>
<h1 id="6-微调系统"><a href="#6-微调系统" class="headerlink" title="6.微调系统"></a>6.微调系统</h1><ul>
<li>注意：<ul>
<li>你将需要在此步骤中使用尽可能多的数据，尤其是在微调结束时。</li>
<li>与往常一样，尽可能做到自动化。</li>
</ul>
</li>
<li>1.使用交叉验证微调超参数：<ul>
<li>将你的数据转换选择视为超参数，尤其是当你对它们不确定时（例如，如果不确定是否用零或中位数替换缺失值，或者只是删除行）。</li>
<li>除非要研究的超参数值很少，否则应优先选择随机搜索而不是网格搜索。如果训练时间很长，你可能更喜欢贝叶斯优化方法（如Jasper Snoek等人所述使用高斯过程先验）。</li>
</ul>
</li>
<li>2.尝试使用集成方法。组合最好的模型通常会比单独运行有更好的性能。</li>
<li>3.一旦对最终模型有信心，就可以在测试集中测量其性能，以估计泛化误差。</li>
<li>注意：在测量了泛化误差之后，请不要对模型进行调整：否则你会开始过拟合测试集。</li>
</ul>
<h1 id="7-演示你的解决方案"><a href="#7-演示你的解决方案" class="headerlink" title="7.演示你的解决方案"></a>7.演示你的解决方案</h1><ul>
<li>1.记录你所做的事情。</li>
<li>2.创建一个不错的演示文稿。<ul>
<li>确保先突出大的蓝图。</li>
</ul>
</li>
<li>3.说明你的解决方案为何可以实现业务目标。</li>
<li>4.别忘了介绍你一路上注意到的有趣观点。<ul>
<li>描述什么有效，什么无效。</li>
<li>列出你的假设和系统的局限性。</li>
</ul>
</li>
<li>5.确保通过精美的可视化效果或易于记忆的陈述来传达你的主要发现（例如，“中等收入是房价的第一大预测指标”）。</li>
</ul>
<h1 id="8-启动！"><a href="#8-启动！" class="headerlink" title="8.启动！"></a>8.启动！</h1><ul>
<li>1.使你的解决方案准备投入生产环境（插入生产数据输入、编写单元测试等）。</li>
<li>2.编写监控代码，以定期检查系统的实时性能，并在系统故障时触发警报。<ul>
<li>当心缓慢的退化：随着数据的发展，模型往往会“腐烂”。</li>
<li>评估性能可能需要人工流水线（例如通过众包服务）。</li>
<li>监视你的输入的质量（例如，传感器出现故障，发送了随机值，或者另一个团队的输出变得过时)。这对于在线学习系统尤其重要。</li>
</ul>
</li>
<li>3.定期根据新数据重新训练模型（尽可能自动进行）。</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch学习笔记</title>
    <url>/pytorch-learning/</url>
    <content><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>这个目录是从深度之眼的pytorch课程中学习并整理的学习笔记</p>
<ul>
<li><a href="https://ai.deepshare.net/detail/p_5df0ad9a09d37_qYqVmt85/6">课程页面入口</a></li>
<li><a href="https://github.com/JansonYuan/Pytorch-Camp">课程代码github</a></li>
<li><a href="https://github.com/greebear/pytorch-learning">作业讲解代码</a></li>
<li>课程所有代码汇总中配套数据百度网盘地址：<a href="https://pan.baidu.com/s/1mA8wSCLnKphByzvHBzc9Pw">https://pan.baidu.com/s/1mA8wSCLnKphByzvHBzc9Pw</a><br>提取码：g5ym</li>
<li>课程所有课件汇总百度网盘地址：<a href="https://pan.baidu.com/s/1svt3lbDgNGixk5lKM1zfig">https://pan.baidu.com/s/1svt3lbDgNGixk5lKM1zfig</a><br>提取码：9j2f</li>
</ul>
<h1 id="目录笔记"><a href="#目录笔记" class="headerlink" title="目录笔记"></a>目录笔记</h1><p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week1.ipynb">Week1 Pytorch基础概念</a></p>
<ul>
<li>Pytorch简介及环境配置</li>
<li>Pytorch基础数据结构——张量</li>
<li>张量操作与线性回归</li>
<li>计算图与动态图机制</li>
<li>autograd与逻辑回归</li>
</ul>
<p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week2.ipynb">Week2 PyTorch数据处理</a></p>
<ul>
<li>数据读取机制DataLoader与Dataset</li>
<li>数据预处理transforms模块机制</li>
<li>二十二种transforms数据预处理方法</li>
<li>学会自定义transforms方法</li>
</ul>
<p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week3.ipynb">Week3 PyTorch模型搭建</a></p>
<ul>
<li>nn.Module与网络模型构建步骤</li>
<li>模型容器与AlexNet构建</li>
<li>网络层中的卷积层</li>
<li>网络层中的池化层、全连接层和激活函数层</li>
</ul>
<p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week4.ipynb">Week4 PyTorch损失优化</a></p>
<ul>
<li>权值初始化</li>
<li>损失函数（一）</li>
<li>Pytorch的14种损失函数</li>
<li>优化器optimizer的概念</li>
<li>torch.optim.SGD</li>
</ul>
<p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week5.ipynb">Week5 PyTorch训练过程</a></p>
<ul>
<li>学习率调整</li>
<li>TensorBoard简介与安装</li>
<li>TensorBoard使用（一）</li>
<li>TensorBoard使用（二）</li>
<li>hook函数与CAM</li>
</ul>
<p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week6.ipynb">Week6 PyTorch的正则化</a></p>
<ul>
<li>weight_decay</li>
<li>dropout</li>
<li>Batch Normalization</li>
<li>Layer Normalization、Instance</li>
<li>Normalization和Group Normalization</li>
</ul>
<p><a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week7.ipynb">Week7 PyTorch训练技巧</a></p>
<ul>
<li>模型保存与加载</li>
<li>Finetune</li>
<li>GPU的使用</li>
<li>Pytorch中常见报错</li>
</ul>
<p>Week8、9 PyTorch深度体验</p>
<ul>
<li>图像分类一瞥</li>
<li>图像分割一瞥</li>
<li>目标检测一瞥（上）</li>
<li>目标检测一瞥（下）</li>
<li>对抗生成网络一瞥</li>
<li>循环神经网络一瞥</li>
</ul>
]]></content>
      <categories>
        <category>pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch第一周学习笔记</title>
    <url>/pytorch-week1/</url>
    <content><![CDATA[<p>最原始编辑版在<a href="https://nbviewer.jupyter.org/github/shiqi-lu/Learn-AI/blob/master/pytorch_deepshare/week1.ipynb">Github链接</a></p>
<h1 id="1-PyTorch简介与安装"><a href="#1-PyTorch简介与安装" class="headerlink" title="1.PyTorch简介与安装"></a>1.PyTorch简介与安装</h1><p>Q:如何安装Pytorch?</p>
<ul>
<li>安装anaconda：<code>conda install pytorch torchvision</code></li>
<li>测试是否安装成功：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cuda.is_available()</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.__version__</span><br><span class="line"><span class="string">&#x27;1.3.1&#x27;</span></span><br></pre></td></tr></table></figure>


<h1 id="2-张量简介与创建"><a href="#2-张量简介与创建" class="headerlink" title="2.张量简介与创建"></a>2.张量简介与创建</h1><p>Q:张量是什么？</p>
<ul>
<li>一个多维数组，它是标量、向量、矩阵的高维拓展</li>
<li><img src="http://anki190912.xuexihaike.com/20200918142143.png?imageView2/2/h/150"></li>
</ul>
<p>Q:Pytorch中的Variable是什么？与Tensor的关系是什么？</p>
<ul>
<li>Variable是torch.autograd中的数据类型主要用于封装Tensor，进行自动求导</li>
<li>data:被包装的Tensor</li>
<li>grad:data的梯度</li>
<li>grad_fn:创建Tensor的Function，是自动求导的关键</li>
<li>requires_grad:指示是否需要梯度</li>
<li>is_leaf:指示是否是叶子结点（张量）</li>
<li><img src="http://anki190912.xuexihaike.com/20200918143346.png?imageView2/2/w/200"></li>
</ul>
<p>Q:Pytorch中的Tensor是什么？</p>
<ul>
<li>PyTorch 0.4.0开始，Variable并入Tensor</li>
<li>dtype: 张量的数据类型，如torch.FloatTensor, torch.cuda.FloatTensor</li>
<li>shape: 张量的形状，如（64，3， 224， 224）</li>
<li>device: 张量所在设备，GPU/CPU，是加速的关键</li>
<li><img src="http://anki190912.xuexihaike.com/20200918143722.png?imageView2/2/h/100"></li>
</ul>
<p>Q:Tensor的函数原型是怎样？</p>
<ul>
<li><code>torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)</code></li>
<li>功能：从data创建tensor</li>
<li>data: 数据，可以是list，numpy</li>
<li>dtype: 数据类型，默认与data一致</li>
<li>device: 所在设备，cuda/cpu</li>
<li>requires_grad: 是否需要梯度</li>
<li>pin_memory:是否存于锁页内存</li>
</ul>
<p>Q:通过torch.tensor创建Tensor的代码是什么？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">arr = np.ones((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">print(arr)</span><br><span class="line">print(<span class="string">&#x27;ndarray的数据类型:&#x27;</span>, arr.dtype)</span><br><span class="line"></span><br><span class="line">t = torch.tensor(arr)</span><br><span class="line">print(t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 放到gpu上</span></span><br><span class="line">t = torch.tensor(arr, device=<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>

<pre><code>[[1. 1. 1.]
 [1. 1. 1.]
 [1. 1. 1.]]
ndarray的数据类型: float64
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], device=&#39;cuda:0&#39;, dtype=torch.float64)</code></pre>
<p>Q:如何通过torch.from_numpy创建张量？</p>
<ul>
<li>函数原型：<code>torch.from_numpy(ndarray)</code></li>
<li>功能：从numpy创建tensor</li>
<li>注意事项：从torch.from_numpy创建的tensor于原ndarray共享内存，当修改其中一个的数据，另外一个也将会被改动</li>
<li><img src="http://anki190912.xuexihaike.com/20200918151039.png?imageView2/2/h/150"></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arr = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">t = torch.from_numpy(arr)</span><br><span class="line">print(<span class="string">&quot;numpy array:&quot;</span>)</span><br><span class="line">print(arr)</span><br><span class="line">print(<span class="string">&quot;tensor:&quot;</span>)</span><br><span class="line">print(t)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;修改arr:&quot;</span>)</span><br><span class="line">arr[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">print(<span class="string">&quot;numpy array:&quot;</span>)</span><br><span class="line">print(arr)</span><br><span class="line">print(<span class="string">&quot;tensor:&quot;</span>)</span><br><span class="line">print(t)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;修改tensor:&quot;</span>)</span><br><span class="line">arr[<span class="number">1</span>, <span class="number">1</span>] = <span class="number">-10</span></span><br><span class="line">print(<span class="string">&quot;numpy array:&quot;</span>)</span><br><span class="line">print(arr)</span><br><span class="line">print(<span class="string">&quot;tensor:&quot;</span>)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>

<pre><code>numpy array:
[[1 2 3]
 [4 5 6]]
tensor:
tensor([[1, 2, 3],
        [4, 5, 6]])
修改arr:
numpy array:
[[0 2 3]
 [4 5 6]]
tensor:
tensor([[0, 2, 3],
        [4, 5, 6]])
修改tensor:
numpy array:
[[  0   2   3]
 [  4 -10   6]]
tensor:
tensor([[  0,   2,   3],
        [  4, -10,   6]])</code></pre>
<p>Q:如何通过torch.zeros或torch.ones创建张量？</p>
<ul>
<li>函数原型：<code>torch.zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li>函数原型：<code>torch.ones(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li>功能：依size创建全0张量和全1</li>
<li>size:张量的形状</li>
<li>out:输出的张量，貌似其原始类型必须为tensor，通过out得到的和返回值得到的是完全一样的，相当于赋值</li>
<li>layout:内存中布局形式，有strided,sparse_coo等</li>
<li>device:所在设备,gpu/cpu</li>
<li>requires_grad: 是否需要梯度</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out_t = torch.tensor([<span class="number">1</span>])</span><br><span class="line">t = torch.zeros((<span class="number">3</span>,<span class="number">3</span>), out=out_t)</span><br><span class="line">print(t)</span><br><span class="line">print(out_t)</span><br><span class="line">print(id(t), id(out_t), id(t) == id(out_t))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
140556294938560 140556294938560 True</code></pre>
<p>Q:如何通过torch.zeros_like或torch.ones_like创建张量？</p>
<ul>
<li>函数原型：<code>torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False)</code></li>
<li>函数原型：<code>torch.ones_like(input, dtype=None, layout=None, device=None, requires_grad=False)</code></li>
<li>功能：依input形状创建全0张量或全1，input是一个tensor类型</li>
<li>input:创建与input同形状的全0张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.empty(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">print(torch.zeros_like(t))</span><br><span class="line">print(torch.ones_like(t))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])</code></pre>
<p>Q:如何通过torch.full创建张量？</p>
<ul>
<li><code>torch.full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li><code>torch.full_like(input, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format)</code></li>
<li>功能：创建全等张量</li>
<li>size: 张量的形状，如（3,3）</li>
<li>fill_value: 张量的值</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.full((<span class="number">3</span>,<span class="number">3</span>), <span class="number">8</span>)</span><br></pre></td></tr></table></figure>


<pre><code>tensor([[8., 8., 8.],
        [8., 8., 8.],
        [8., 8., 8.]])</code></pre>
<p>Q:如何通过torch.arange创建等差数列的1维张量？</p>
<ul>
<li>函数原型：<code>torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li>功能：创建等差为1的张量</li>
<li>注意事项：数值区间为[start, end)</li>
<li>start: 数列起始值</li>
<li>end: 数列“结束值”</li>
<li>step: 数列公差，默认为1</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.arange(<span class="number">2</span>,<span class="number">10</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([2, 4, 6, 8])</code></pre>
<p>Q:如何通过torch.linspace创建均分数列张量</p>
<ul>
<li>函数原型：<code>torch.linspace(start=0, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li>功能：创建均分的1维张量</li>
<li>注意事项：数值区间为[start, end]</li>
<li>start: 数列起始值</li>
<li>end: 数列结束值</li>
<li>steps: 数列长度</li>
<li>步长为：(end-start)/(steps-1)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.linspace(<span class="number">2</span>, <span class="number">10</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 2.0000,  3.3333,  4.6667,  6.0000,  7.3333,  8.6667, 10.0000])</code></pre>
<p>Q:如何通过torch.logspace创建对数均分的1维张量？</p>
<ul>
<li>函数原型：<code>torch.logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li>功能：创建对数均分的1维张量</li>
<li>注意事项：长度为steps，底为base</li>
<li>base: 对数函数的低，默认为10</li>
</ul>
<p>Q:如何通过torch.eye创建单位对角矩阵？</p>
<ul>
<li>函数原型：<code>torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
<li>功能：创建单位对角矩阵（2维张量）</li>
<li>注意事项：默认为方阵</li>
<li>n: 矩阵行数</li>
<li>m: 矩阵列数</li>
</ul>
<p>Q:如何通过torch.normal生成正态分布的张量？</p>
<ul>
<li>函数原型：<code>torch.normal(mean, std, *, generator=None, out=None)</code></li>
<li>功能：生成正态分布（高斯分布）</li>
<li>mean: 均值</li>
<li>std: 标准差</li>
<li>因mean和std可以分别为标量和张量，有4种不同的组合</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mean：张量 std: 张量</span></span><br><span class="line"><span class="comment"># 其中t[i]是从mean[i],std[i]的标准正态分布中采样得来</span></span><br><span class="line">mean = torch.arange(<span class="number">1</span>, <span class="number">5</span>, dtype=torch.float)</span><br><span class="line">std = torch.arange(<span class="number">1</span>, <span class="number">5</span>, dtype=torch.float)</span><br><span class="line">t = torch.normal(mean, std)</span><br><span class="line">print(<span class="string">&quot;mean:&#123;&#125;\nstd:&#123;&#125;&quot;</span>.format(mean, std))</span><br><span class="line">print(t)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean：标量 std: 标量，此时要指定size大小</span></span><br><span class="line">t_normal = torch.normal(<span class="number">0.</span>, <span class="number">1.</span>, size=(<span class="number">4</span>,))</span><br><span class="line">print(t_normal)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean：张量 std: 标量</span></span><br><span class="line">mean = torch.arange(<span class="number">1</span>, <span class="number">5</span>, dtype=torch.float)</span><br><span class="line">std = <span class="number">1</span></span><br><span class="line">t_normal = torch.normal(mean, std)</span><br><span class="line">print(<span class="string">&quot;mean:&#123;&#125;\nstd:&#123;&#125;&quot;</span>.format(mean, std))</span><br><span class="line">print(t_normal)</span><br></pre></td></tr></table></figure>

<pre><code>mean:tensor([1., 2., 3., 4.])
std:tensor([1., 2., 3., 4.])
tensor([ 0.6063,  2.9914,  4.0138, -0.5877])

tensor([ 1.1977, -0.1746,  1.5572, -1.1905])

mean:tensor([1., 2., 3., 4.])
std:1
tensor([0.7165, 1.5649, 3.2308, 3.2504])</code></pre>
<p>Q:如何创建标准正态分布的张量？</p>
<ul>
<li><code>torch.randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor</code></li>
<li><code>torch.randn_like(input, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor</code></li>
<li>size:张量的形状</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(torch.randn(<span class="number">4</span>))</span><br><span class="line">print(torch.randn(<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([-0.2387, -0.3638,  0.3597,  0.1225])
tensor([[ 0.4709,  0.8593, -0.5970],
        [-0.1133,  0.3273,  0.0106]])</code></pre>
<p>Q:如何生成均匀分布和整数均匀分布的张量？</p>
<ul>
<li>在[0,1)区间上，生成均匀分布</li>
<li><code>torch.rand(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor</code></li>
<li><code>torch.rand_like(input, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor</code></li>
<li>在[low, high)区间生成整数均匀分布</li>
<li><code>torch.randint(low=0, high, size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor</code></li>
<li><code>torch.randint_like(input, low=0, high, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor</code></li>
<li>其中size是张量形状</li>
</ul>
<p>Q:如何生成从0到n-1的随机排列？</p>
<ul>
<li><code>torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False) → LongTensor</code></li>
<li>n是张量的长度</li>
<li>经常用于生成乱序索引</li>
</ul>
<p>Q:如何生成一个伯努利分布的张量？</p>
<ul>
<li><code>torch.bernoulli(input, *, generator=None, out=None) → Tensor</code></li>
<li>以input为概率，生成伯努利分布（0-1分布，两点分布）</li>
</ul>
<h1 id="3-张量操作与线性回归"><a href="#3-张量操作与线性回归" class="headerlink" title="3.张量操作与线性回归"></a>3.张量操作与线性回归</h1><h2 id="张量的操作：拼接、切分、索引和变换"><a href="#张量的操作：拼接、切分、索引和变换" class="headerlink" title="张量的操作：拼接、切分、索引和变换"></a>张量的操作：拼接、切分、索引和变换</h2><p>Q:如何用torch.cat对张量进行拼接？</p>
<ul>
<li><code>torch.cat(tensors, dim=0, out=None) → Tensor</code></li>
<li>功能：将张量按维度dim进行拼接</li>
<li>tensors: 张量序列</li>
<li>dim：要拼接的维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">print(t)</span><br><span class="line">t1 = torch.cat([t,t], dim=<span class="number">0</span>)</span><br><span class="line">print(t1)</span><br><span class="line">print(<span class="string">&quot;shape:&quot;</span>, t1.shape)</span><br><span class="line">t2 = torch.cat([t,t], dim=<span class="number">1</span>)</span><br><span class="line">print(t2)</span><br><span class="line">print(<span class="string">&quot;shape:&quot;</span>, t2.shape)</span><br><span class="line"><span class="comment"># dim是指在哪个方向上进行叠加</span></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.6851,  0.0099, -1.4586],
        [ 0.2965,  0.4991, -0.4938]])
tensor([[-0.6851,  0.0099, -1.4586],
        [ 0.2965,  0.4991, -0.4938],
        [-0.6851,  0.0099, -1.4586],
        [ 0.2965,  0.4991, -0.4938]])
shape: torch.Size([4, 3])
tensor([[-0.6851,  0.0099, -1.4586, -0.6851,  0.0099, -1.4586],
        [ 0.2965,  0.4991, -0.4938,  0.2965,  0.4991, -0.4938]])
shape: torch.Size([2, 6])</code></pre>
<p>Q:如何用torch.stack对张量进行拼接？</p>
<ul>
<li><code>torch.stack(tensors, dim=0, out=None) → Tensor</code></li>
<li>功能：在新创建的维度dim上进行拼接</li>
<li>tensors:张量序列</li>
<li>dim：要拼接的维度</li>
<li>注意：cat不会扩张张量的维度，stack会扩张，相当于insert</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">print(t)</span><br><span class="line">t1 = torch.stack([t,t], dim=<span class="number">0</span>)</span><br><span class="line">print(t1)</span><br><span class="line">print(<span class="string">&quot;shape:&quot;</span>, t1.shape)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.6266,  0.8918,  0.6165],
        [ 1.1646, -1.8152, -0.7309]])
tensor([[[ 0.6266,  0.8918,  0.6165],
         [ 1.1646, -1.8152, -0.7309]],

        [[ 0.6266,  0.8918,  0.6165],
         [ 1.1646, -1.8152, -0.7309]]])
shape: torch.Size([2, 2, 3])</code></pre>
<p>Q:如何用torch.chunk对切分张量？</p>
<ul>
<li><code>torch.chunk(input, chunks, dim=0) → List of Tensors</code></li>
<li>功能：将张量按维度dim进行平均切分</li>
<li>返回值：张量列表</li>
<li>注意事项：若不能整除，最后一份张量小于其它张量</li>
<li>input：要切分的张量</li>
<li>chunks：要切分的份数</li>
<li>dim：要切分的维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones((<span class="number">2</span>,<span class="number">7</span>))</span><br><span class="line">print(a)</span><br><span class="line">list_of_tensors = torch.chunk(a, dim=<span class="number">1</span>, chunks=<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(list_of_tensors):</span><br><span class="line">    print(<span class="string">f&quot;第<span class="subst">&#123;i&#125;</span>个张量：&quot;</span>)</span><br><span class="line">    print(t)</span><br><span class="line"><span class="comment"># 切分后的长度的计算方式为：7/3向上取整为3</span></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.]])
第0个张量：
tensor([[1., 1., 1.],
        [1., 1., 1.]])
第1个张量：
tensor([[1., 1., 1.],
        [1., 1., 1.]])
第2个张量：
tensor([[1.],
        [1.]])</code></pre>
<p>Q:如何用torch.split对张量进行切分？</p>
<ul>
<li><code>torch.split(tensor, split_size_or_sections, dim=0)</code></li>
<li>功能：将张量按维度dim进行平均切分</li>
<li>返回值：张量列表</li>
<li>tensor：要切分的张量</li>
<li>split_size_or_sections：为int时，表示每一份的长度；为list时，按list元素切分，list元素和必须为该维度的长度</li>
<li>dim：要切分的维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones((<span class="number">2</span>,<span class="number">7</span>))</span><br><span class="line">print(a)</span><br><span class="line">list_of_tensors = torch.split(a, [<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(list_of_tensors):</span><br><span class="line">    print(<span class="string">f&quot;第<span class="subst">&#123;i&#125;</span>个张量：&quot;</span>)</span><br><span class="line">    print(t)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1.]])
第0个张量：
tensor([[1., 1.],
        [1., 1.]])
第1个张量：
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.]])
第2个张量：
tensor([[1.],
        [1.]])</code></pre>
<p>Q:如何在dim维度上，按index索引数据？</p>
<ul>
<li><code>torch.index_select(input, dim, index, out=None) → Tensor</code></li>
<li>返回值：按index索引数据拼接的张量</li>
<li>input：要索引的张量</li>
<li>dim：要索引的维度</li>
<li>index：要索引数据的序号</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.randint(<span class="number">0</span>,<span class="number">9</span>, size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">idx = torch.tensor([<span class="number">0</span>,<span class="number">2</span>], dtype=torch.long)</span><br><span class="line">t_select = torch.index_select(t, dim=<span class="number">0</span>, index=idx)</span><br><span class="line">print(t)</span><br><span class="line">print(idx)</span><br><span class="line">print(t_select)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1, 2, 8],
        [2, 3, 0],
        [0, 2, 0]])
tensor([0, 2])
tensor([[1, 2, 8],
        [0, 2, 0]])</code></pre>
<p>Q:如何对张量按mask中的True进行索引？</p>
<ul>
<li><code>torch.masked_select(input, mask, out=None) → Tensor</code></li>
<li>返回值：一维张量</li>
<li>input：要索引的张量</li>
<li>mask：与input同形状的布尔类型张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.randint(<span class="number">0</span>, <span class="number">9</span>, size=(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">print(t)</span><br><span class="line">mask = t.le(<span class="number">5</span>) <span class="comment"># le是小于等于，还有lt,gt,ge</span></span><br><span class="line">print(mask)</span><br><span class="line">t_select = torch.masked_select(t, mask)</span><br><span class="line">print(t_select)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[2, 5, 0],
        [5, 8, 5],
        [2, 8, 5]])
tensor([[ True,  True,  True],
        [ True, False,  True],
        [ True, False,  True]])
tensor([2, 5, 0, 5, 5, 2, 5])</code></pre>
<p>Q:如何改变张量的形状？</p>
<ul>
<li><code>torch.reshape(input, shape) → Tensor</code></li>
<li>注意事项：当张量在内存中是连续时，新张量与input共享数据内存</li>
<li>input：要变换的张量</li>
<li>shape：新张量的形状，允许某个维度为-1，意味着这个维度根据其它的算出来的</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.randperm(<span class="number">8</span>)</span><br><span class="line">t_reshape = torch.reshape(t, (<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">print(t)</span><br><span class="line">print(t_reshape)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([2, 0, 7, 5, 6, 4, 3, 1])
tensor([[2, 0, 7, 5],
        [6, 4, 3, 1]])</code></pre>
<p>Q:如何交换张量的两个维度？</p>
<ul>
<li><code>torch.transpose(input, dim0, dim1) → Tensor</code></li>
<li>input：要交换的张量</li>
<li>dim0，dim1：要交换的维度</li>
<li>若为2维张量转置，即矩阵转置，可使用<code>torch.t(input) → Tensor</code>，等价于<code>torch.transpose(input, 0, 1)</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.rand((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">t_transpose = torch.transpose(t, dim0=<span class="number">1</span>,dim1=<span class="number">2</span>)</span><br><span class="line">print(t)</span><br><span class="line">print(t_transpose)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[0.8657, 0.5869, 0.1105, 0.4381],
         [0.7276, 0.6606, 0.3778, 0.3643],
         [0.6180, 0.6693, 0.9983, 0.4252]],

        [[0.3526, 0.6365, 0.6643, 0.5310],
         [0.4653, 0.5056, 0.1065, 0.7873],
         [0.6175, 0.6650, 0.1325, 0.5837]]])
tensor([[[0.8657, 0.7276, 0.6180],
         [0.5869, 0.6606, 0.6693],
         [0.1105, 0.3778, 0.9983],
         [0.4381, 0.3643, 0.4252]],

        [[0.3526, 0.4653, 0.6175],
         [0.6365, 0.5056, 0.6650],
         [0.6643, 0.1065, 0.1325],
         [0.5310, 0.7873, 0.5837]]])</code></pre>
<p>Q:如何压缩长度为1的维度（轴）？</p>
<ul>
<li><code>torch.squeeze(input, dim=None, out=None) → Tensor</code></li>
<li>dim: 若为None，移除所有长度为1的轴；若指定维度，当且仅当该轴长度为1时，可以被移除</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.rand((<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">t_sq = torch.squeeze(t)</span><br><span class="line">t_0 = torch.squeeze(t, dim=<span class="number">0</span>)</span><br><span class="line">t_1 = torch.squeeze(t, dim=<span class="number">1</span>)</span><br><span class="line">print(t.shape)</span><br><span class="line">print(t_sq.shape)</span><br><span class="line">print(t_0.shape)</span><br><span class="line">print(t_1.shape)</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([1, 2, 3, 1])
torch.Size([2, 3])
torch.Size([2, 3, 1])
torch.Size([1, 2, 3, 1])</code></pre>
<p>Q:如何根据dim扩展维度？</p>
<ul>
<li><code>torch.unsqueeze(input, dim) → Tensor</code></li>
<li>dim:扩展的维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">print(t)</span><br><span class="line">t1 = torch.unsqueeze(t, <span class="number">0</span>)</span><br><span class="line">print(t1)</span><br><span class="line">t2 = torch.unsqueeze(t, <span class="number">1</span>)</span><br><span class="line">print(t2)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([1, 2, 3, 4])
tensor([[1, 2, 3, 4]])
tensor([[1],
        [2],
        [3],
        [4]])</code></pre>
<h2 id="张量的数学运算"><a href="#张量的数学运算" class="headerlink" title="张量的数学运算"></a>张量的数学运算</h2><p>Q:有哪些常见的数学运算？</p>
<ul>
<li>一、加减乘除<ul>
<li>torch.add()</li>
<li>torch.addcdiv()</li>
<li>torch.addcmul()</li>
<li>torch.sub() </li>
<li>torch.div()</li>
<li>torch.mul()</li>
</ul>
</li>
<li>二、对数，指数，幂函数<ul>
<li>torch.log(input, out=None)</li>
<li>torch.log10(input, out=None)</li>
<li>torch.log2(input, out=None)</li>
<li>torch.exp(input, out=None)</li>
<li>torch.pow()</li>
</ul>
</li>
<li>三、三角函数<ul>
<li>torch.abs(input, out=None)</li>
<li>torch.acos(input, out=None)</li>
<li>torch.cosh(input, out=None)</li>
<li>torch.cos(input, out=None)</li>
<li>torch.asin(input, out=None)</li>
<li>torch.atan(input, out=None)</li>
<li>torch.atan2(input, other, out=None)</li>
</ul>
</li>
</ul>
<p>Q:如何逐元素计算input + alpha x other?</p>
<ul>
<li><code>torch.add(input, other, *, alpha=1, out=None)</code></li>
<li>input：第一个张量</li>
<li>alpha：乘项因子</li>
<li>other：第二个张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t_0 = torch.randn((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">t_1 = torch.ones_like(t_0)</span><br><span class="line">t_add = torch.add(t_0, t_1, alpha=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;t_0:\n&#123;&#125;\nt_1:\n&#123;&#125;\nt_add_10:\n&#123;&#125;&quot;</span>.format(t_0, t_1, t_add))</span><br></pre></td></tr></table></figure>

<pre><code>t_0:
tensor([[ 0.5570, -0.4743,  1.0113],
        [-1.2665,  0.1997, -0.6957],
        [-0.0714, -0.7002, -1.4687]])
t_1:
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
t_add_10:
tensor([[10.5570,  9.5257, 11.0113],
        [ 8.7335, 10.1997,  9.3043],
        [ 9.9286,  9.2998,  8.5313]])</code></pre>
<p>Q:如何计算$\text { out }<em>{i}=\text { input }</em>{i}+\text { value } \times \text { tensor } 1_{i} \times \text { tensor } 2_{i}$</p>
<ul>
<li><code>torch.addcmul(input, tensor1, tensor2, *, value=1, out=None) → Tensor</code></li>
</ul>
<p>Q:如何计算$\text { out }<em>{i}=\text { input }</em>{i}+\text { value } \times \frac{\text { tensor } 1}{\text { tensor } 2_{i}}$</p>
<ul>
<li><code>torch.addcdiv(input, tensor1, tensor2, *, value=1, out=None) → Tensor</code></li>
</ul>
<h2 id="线性回归的Pytorch实现"><a href="#线性回归的Pytorch实现" class="headerlink" title="线性回归的Pytorch实现"></a>线性回归的Pytorch实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.05</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练数据</span></span><br><span class="line">x = torch.rand(<span class="number">20</span>, <span class="number">1</span>) * <span class="number">10</span>  <span class="comment"># x data (tensor), shape=(20, 1)</span></span><br><span class="line">y = <span class="number">2</span>*x + (<span class="number">5</span> + torch.randn(<span class="number">20</span>, <span class="number">1</span>))  <span class="comment"># y data (tensor), shape=(20, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建线性回归参数的初始值</span></span><br><span class="line">w = torch.randn((<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros((<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播，计算y_pred=w * x+b</span></span><br><span class="line">    wx = torch.mul(w, x)</span><br><span class="line">    y_pred = torch.add(wx, b)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 MSE loss</span></span><br><span class="line">    loss = (<span class="number">0.5</span> * (y - y_pred) ** <span class="number">2</span>).mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    b.data.sub_(lr * b.grad)</span><br><span class="line">    w.data.sub_(lr * w.grad)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 清零张量的梯度</span></span><br><span class="line">    w.grad.zero_()</span><br><span class="line">    b.grad.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图</span></span><br><span class="line">    <span class="keyword">if</span> iteration % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        plt.plot(x.data.numpy(), y_pred.data.numpy(), <span class="string">&#x27;r-&#x27;</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">2</span>, <span class="number">20</span>, <span class="string">&#x27;Loss=%.4f&#x27;</span> % loss.data.numpy(), fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>:  <span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.xlim(<span class="number">1.5</span>, <span class="number">10</span>)</span><br><span class="line">        plt.ylim(<span class="number">8</span>, <span class="number">28</span>)</span><br><span class="line">        plt.title(<span class="string">f&quot;Iteration: <span class="subst">&#123;iteration&#125;</span>\nw: <span class="subst">&#123;w.data.numpy()&#125;</span> b: <span class="subst">&#123;b.data.numpy()&#125;</span>&quot;</span>)</span><br><span class="line">        plt.pause(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> loss.data.numpy() &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="http://anki190912.xuexihaike.com/20200920095346.png"></p>
<h1 id="4-计算图与动态图机制"><a href="#4-计算图与动态图机制" class="headerlink" title="4.计算图与动态图机制"></a>4.计算图与动态图机制</h1><p>Q:计算图是什么？</p>
<ul>
<li>用来描述运算的有向无环图</li>
<li>有两个主要元素：结点（Node）和边（Edge）</li>
<li>结点表示数据，如向量、矩阵、张量，边表示运算，如加减乘除卷积等</li>
</ul>
<p>Q:如何用计算图表示$y = (x+w)*(w+1)$?</p>
<ul>
<li>$a = x + w, b = w + 1, y = a * b$</li>
<li><img src="http://anki190912.xuexihaike.com/20200919144309.png?imageView2/2/h/200"></li>
</ul>
<p>Q:如何用计算图进行梯度求导，如$y = (x+w)*(w+1)$</p>
<ul>
<li>$a = x + w, b = w + 1, y = a * b$</li>
<li>$$\begin{aligned}<br>\frac{\partial \mathrm{y}}{\partial w} &amp;=\frac{\partial \mathrm{y}}{\partial a} \frac{\partial a}{\partial w}+\frac{\partial \mathrm{y}}{\partial b} \frac{\partial b}{\partial w} \\<br>&amp;=b * 1+\mathrm{a} * 1 \\<br>&amp;=\mathrm{b}+\mathrm{a} \\<br>&amp;=(\mathrm{w}+1)+(\mathrm{x}+\mathrm{w}) \\<br>&amp;=2 * \mathrm{w}+\mathrm{x}+1 \\<br>&amp;=2 * 1+2+1=5\end{aligned}$$</li>
<li><img src="http://anki190912.xuexihaike.com/20200919144541.png?imageView2/2/h/200"></li>
<li>y对w求导在计算图中其实就是找到y到w的所有路径上的导数，进行求和</li>
</ul>
<p>Q:叶子结点是什么？</p>
<ul>
<li><img src="http://anki190912.xuexihaike.com/20200919144541.png?imageView2/2/h/200"></li>
<li>用户创建的结点称为叶子结点，如X和W</li>
<li>torch.Tensor中有is_leaf指示张量是否为叶子结点</li>
<li>设置叶子结点主要是为了节省内存，因为非叶子结点的梯度在反向传播后会被释放掉</li>
<li>若需要保留非叶子结点的梯度，可使用retain_grad()方法</li>
</ul>
<p>Q:torch.Tensor中的grad_fn作用是什么？</p>
<ul>
<li>记录创建该张量时所用的方法（函数）</li>
<li><img src="http://anki190912.xuexihaike.com/20200919144541.png?imageView2/2/h/200"></li>
<li>y.grad_fn = &lt;MulBackward0&gt;</li>
<li>a.grad_fn = &lt;AddBackward0&gt;</li>
</ul>
<p>Q:$y = (x+w)*(w+1)$计算图的代码示例，求解y对w的梯度？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">w = torch.tensor([<span class="number">1.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">x = torch.tensor([<span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">a = torch.add(w, x)</span><br><span class="line"><span class="comment"># 若需要保留非叶子结点a的梯度，否则调用a.grad时为None</span></span><br><span class="line"><span class="comment"># a.retain_grad()</span></span><br><span class="line">b = torch.add(w, <span class="number">1</span>)</span><br><span class="line">y = torch.mul(a, b)</span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line">print(w.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看叶子结点</span></span><br><span class="line">print(<span class="string">&quot;\nis_leaf:\n&quot;</span>, w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看梯度</span></span><br><span class="line">print(<span class="string">&quot;\ngradient:\n&quot;</span>, w.grad, x.grad, a.grad, b.grad, y.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 grad_fn</span></span><br><span class="line">print(<span class="string">&quot;\ngrad_fn:\n&quot;</span>, w.grad_fn, x.grad_fn, a.grad_fn, b.grad_fn, y.grad_fn)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([5.])

is_leaf:
 True True False False False

gradient:
 tensor([5.]) tensor([2.]) None None None

grad_fn:
 None None &lt;AddBackward0 object at 0x7fd54938bb00&gt; &lt;AddBackward0 object at 0x7fd5285f4c50&gt; &lt;MulBackward0 object at 0x7fd5285f4be0&gt;</code></pre>
<h1 id="5-autograd与逻辑回归"><a href="#5-autograd与逻辑回归" class="headerlink" title="5.autograd与逻辑回归"></a>5.autograd与逻辑回归</h1><p>Q:torch.autograd.backward是什么？</p>
<ul>
<li>torch.autograd.backward(tensors: Union[torch.Tensor, Sequence[torch.Tensor]], grad_tensors: Union[torch.Tensor, Sequence[torch.Tensor], None] = None, retain_graph: Optional[bool] = None, create_graph: bool = False, grad_variables: Union[torch.Tensor, Sequence[torch.Tensor], None] = None) → None</li>
<li>功能：自动求取梯度</li>
<li>tensors：用于求导的张量，如loss</li>
<li>retain_graph：保存计算图，若不保存，则紧接着再调用一次backward()会报错</li>
<li>create_graph：创建导数计算图，用于高阶求导</li>
<li>grad_tensors：多梯度权重</li>
</ul>
<p>Q:torch.autograd.backward中的retain_graph的代码示例？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = torch.tensor([<span class="number">1.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">x = torch.tensor([<span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">a = torch.add(w, x)</span><br><span class="line">b = torch.add(w, <span class="number">1</span>)</span><br><span class="line">y = torch.mul(a, b)</span><br><span class="line"></span><br><span class="line">y.backward(retain_graph=<span class="literal">True</span></span><br><span class="line">          )</span><br><span class="line">print(w.grad)</span><br><span class="line">y.backward()</span><br><span class="line">print(w.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([5.])
tensor([10.])</code></pre>
<p>Q:torch.autograd.backward中的grad_tensors的代码示例？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = torch.tensor([<span class="number">1.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">x = torch.tensor([<span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">a = torch.add(w, x)     <span class="comment"># retain_grad()</span></span><br><span class="line">b = torch.add(w, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y0 = torch.mul(a, b)    <span class="comment"># y0 = (x+w) * (w+1)</span></span><br><span class="line">y1 = torch.add(a, b)    <span class="comment"># y1 = (x+w) + (w+1)    dy1/dw = 2</span></span><br><span class="line"></span><br><span class="line">loss = torch.cat([y0, y1], dim=<span class="number">0</span>)       <span class="comment"># [y0, y1]</span></span><br><span class="line">grad_tensors = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># gradient 传入 torch.autograd.backward()中的grad_tensors</span></span><br><span class="line">loss.backward(gradient=grad_tensors)</span><br><span class="line"><span class="comment"># 实际上相当于1*y0导数+2*y1导数</span></span><br><span class="line"></span><br><span class="line">print(w.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([9.])</code></pre>
<p>Q:torch.autograd.grad是什么？</p>
<ul>
<li>torch.autograd.grad(outputs: Union[torch.Tensor, Sequence[torch.Tensor]], inputs: Union[torch.Tensor, Sequence[torch.Tensor]], grad_outputs: Union[torch.Tensor, Sequence[torch.Tensor], None] = None, retain_graph: Optional[bool] = None, create_graph: bool = False, only_inputs: bool = True, allow_unused: bool = False) → Tuple[torch.Tensor, …]</li>
<li>功能：求取梯度</li>
<li>outputs：用于求导的张量，如loss</li>
<li>inputs：需要梯度的张量</li>
<li>create_graph：创建导数计算图，用于高阶求导</li>
<li>retain_graph：保存计算图</li>
<li>grad_outputs：多梯度权重</li>
</ul>
<p>Q:如何使用torch.autograd.grad对$y=x^2$进行一阶和二阶求导？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.pow(x, <span class="number">2</span>)  <span class="comment"># y = x**2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># grad_1 = dy/dx = 2x = 2 * 3 = 6</span></span><br><span class="line">grad_1 = torch.autograd.grad(y, x, create_graph=<span class="literal">True</span>)</span><br><span class="line">print(grad_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># grad_2 = d(dy/dx)/dx = d(2x)/dx = 2</span></span><br><span class="line"><span class="comment"># grad_1的返回值是元组，所以要取出第一个</span></span><br><span class="line">grad_2 = torch.autograd.grad(grad_1[<span class="number">0</span>], x)</span><br><span class="line">print(grad_2)</span><br></pre></td></tr></table></figure>

<pre><code>(tensor([6.], grad_fn=&lt;MulBackward0&gt;),)
(tensor([2.]),)</code></pre>
<p>Q:autograd的3点使用小贴士是什么？</p>
<ul>
<li>1.梯度不自动清零，每次传播时会一直叠加上去，所以使用梯度之后要手动进行清零，即w.grad.zero_()，其中下划线表示inplace（原地）操作</li>
<li>2.依赖于叶子结点的节点，requires_grad默认为True</li>
<li>3.叶子结点不可执行in-place</li>
</ul>
<p>Q:逻辑回归的pytorch代码实现是什么？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">torch.manual_seed(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">sample_nums = <span class="number">100</span></span><br><span class="line">mean_value = <span class="number">1.7</span></span><br><span class="line">bias = <span class="number">1</span></span><br><span class="line">n_data = torch.ones(sample_nums, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 类别0 数据 shape=(100, 2)</span></span><br><span class="line">x0 = torch.normal(mean_value * n_data, <span class="number">1</span>) + bias</span><br><span class="line"><span class="comment"># 类别0 标签 shape=(100)</span></span><br><span class="line">y0 = torch.zeros(sample_nums)</span><br><span class="line"><span class="comment"># 类别1 数据 shape=(100, 2)</span></span><br><span class="line">x1 = torch.normal(-mean_value * n_data, <span class="number">1</span>) + bias</span><br><span class="line"><span class="comment"># 类别1 标签 shape=(100)</span></span><br><span class="line">y1 = torch.ones(sample_nums)</span><br><span class="line">train_x = torch.cat((x0, x1), <span class="number">0</span>)</span><br><span class="line">train_y = torch.cat((y0, y1), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LR</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(LR, self).__init__()</span><br><span class="line">        self.features = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = self.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化逻辑回归模型</span></span><br><span class="line">lr_net = LR()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择损失函数，交叉熵损失</span></span><br><span class="line">loss_fn = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择优化器</span></span><br><span class="line">lr = <span class="number">0.01</span> <span class="comment"># 学习率</span></span><br><span class="line">optimizer = torch.optim.SGD(lr_net.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    y_pred = lr_net(train_x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算loss</span></span><br><span class="line">    loss = loss_fn(y_pred.squeeze(), train_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绘图</span></span><br><span class="line">    <span class="keyword">if</span> iteration % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 以0.5为阈值进行分类</span></span><br><span class="line">        mask = y_pred.ge(<span class="number">0.5</span>).float().squeeze()</span><br><span class="line">        <span class="comment"># 计算正确预测的样本个数</span></span><br><span class="line">        correct = (mask == train_y).sum()</span><br><span class="line">        <span class="comment"># 计算分类准确率</span></span><br><span class="line">        acc = correct.item() / train_y.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        plt.scatter(x0.data.numpy()[:, <span class="number">0</span>], x0.data.numpy()[:, <span class="number">1</span>], c=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;class 0&#x27;</span>)</span><br><span class="line">        plt.scatter(x1.data.numpy()[:, <span class="number">0</span>], x1.data.numpy()[:, <span class="number">1</span>], c=<span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;class 1&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        w0, w1 = lr_net.features.weight[<span class="number">0</span>]</span><br><span class="line">        w0, w1 = float(w0.item()), float(w1.item())</span><br><span class="line">        plot_b = float(lr_net.features.bias[<span class="number">0</span>].item())</span><br><span class="line">        plot_x = np.arange(<span class="number">-6</span>, <span class="number">6</span>, <span class="number">0.1</span>)</span><br><span class="line">        plot_y = (-w0 * plot_x - plot_b) / w1</span><br><span class="line">        </span><br><span class="line">        plt.xlim(<span class="number">-5</span>, <span class="number">7</span>)</span><br><span class="line">        plt.ylim(<span class="number">-7</span>, <span class="number">7</span>)</span><br><span class="line">        plt.plot(plot_x, plot_y)</span><br><span class="line">        </span><br><span class="line">        plt.text(<span class="number">-5</span>, <span class="number">5</span>, <span class="string">&#x27;Loss=%.4f&#x27;</span> % loss.data.numpy(), fontdict=&#123;<span class="string">&#x27;size&#x27;</span>:<span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>:<span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.title(<span class="string">&quot;Iteration: &#123;&#125;\nw0:&#123;:.2f&#125; w1:&#123;:.2f&#125; b: &#123;:.2f&#125; accuracy:&#123;:.2%&#125;&quot;</span>.format(iteration, w0, w1, plot_b, acc))</span><br><span class="line">        plt.legend()</span><br><span class="line"></span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> acc &gt; <span class="number">0.99</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p><img src="http://anki190912.xuexihaike.com/20200920100028.png"></p>
]]></content>
      <categories>
        <category>pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>恶魔奶爸语法1-3课</title>
    <url>/gramma1-3/</url>
    <content><![CDATA[<h1 id="1-英语简单句的造句规则和基本语法概念"><a href="#1-英语简单句的造句规则和基本语法概念" class="headerlink" title="1.英语简单句的造句规则和基本语法概念"></a>1.英语简单句的造句规则和基本语法概念</h1><h2 id="一些基本的语法概念"><a href="#一些基本的语法概念" class="headerlink" title="一些基本的语法概念"></a>一些基本的语法概念</h2><ul>
<li>英语单词分为虚词和实词<ul>
<li>虚词：介词，连词</li>
<li>实词：名词，动词，形容词，副词</li>
</ul>
</li>
<li>句子成分：主谓宾</li>
</ul>
<h2 id="英语句子的本质和灵魂：五大动词——五大句型"><a href="#英语句子的本质和灵魂：五大动词——五大句型" class="headerlink" title="英语句子的本质和灵魂：五大动词——五大句型"></a>英语句子的本质和灵魂：五大动词——五大句型</h2><h3 id="系动词-gt-主语-系动词-表语（主系表句型）"><a href="#系动词-gt-主语-系动词-表语（主系表句型）" class="headerlink" title="系动词-&gt;主语+系动词+表语（主系表句型）"></a>系动词-&gt;主语+系动词+表语（主系表句型）</h3><ul>
<li>什么是系动词<ul>
<li>所谓的系动词，也属于动词的一种在句子里做谓语，没有实际意思比如汉语里的”是”、”为”英语里的am、is、 are就是联系，在句子中起一个联系作用，叫做系动词。</li>
</ul>
</li>
<li>什么是表语<ul>
<li>而在联系动词之后的补充说明主语性质的部分，就叫做表语</li>
</ul>
</li>
<li>如何判断主系表结构<ul>
<li>用中文判断这个句子里的谓语，可否用“是”或者“为”来翻译</li>
<li>I am a good person</li>
<li>I become a good person</li>
</ul>
</li>
<li>4大类系动词<ul>
<li>1.be动词：am is are和它们对应的过去将来时态，在be动词之后，有3大类表语：名词、形容词、地点副词<ul>
<li>名词作表语：这种句子和汉语可以完全对应，be动词翻译成汉语“是”<ul>
<li>Tim is an engineer.蒂姆是个工程师</li>
<li>The price is £2,000! 价格是2,000英镑!</li>
</ul>
</li>
<li>形容词作表语：和汉语不一样，be动词不会被翻译，直接被省略，介词短语也相当于形容词<ul>
<li>The play was very interesting. 戏很有意思</li>
<li>The milk is in the refrigerator. 牛奶在冰箱里</li>
</ul>
</li>
<li>地点副词作表语：只能是地点副词，别的副词不行<ul>
<li>Your sister is here these days. 你姐姐这几天在这</li>
<li>My bedroom is downstairs. 我的卧室在楼下</li>
</ul>
</li>
</ul>
</li>
<li>2.状态保持动词（keep, remain, stay）后只能加形容词作表语，可以和be无缝切换<ul>
<li>You should keep quiet! 你应该保持安静!</li>
<li>No one can remain youthful forever. 没有人能永保青春。</li>
<li>The weather stayed fine for a week. 这个星期天气一直很好。</li>
</ul>
</li>
<li>3.状态转变类动词(become, get, go, come, grow, turn)<ul>
<li>become:万能词，表示“变成”的时候后面只能接名词，表示“变得”后面一般接形容词<ul>
<li>He became a teacher.他成为了一名教师</li>
<li>He became very nervous. 他变得很紧张</li>
</ul>
</li>
<li>get:表示“变得怎样”，后面只能加形容词<ul>
<li>He got very angry. 他非常生气</li>
</ul>
</li>
<li>come:本意是来，通常是好的东西会来，所以come含有“变好”的意思<ul>
<li>Thing will come right.事情会变好</li>
</ul>
</li>
<li>go:意思是走，去，通常是坏的东西离你而去，所以可表达“变坏”<ul>
<li>The meat always goes bad in summer. 肉在夏天经常会坏掉</li>
</ul>
</li>
<li>grow:本意“生长”，可表达“慢慢变”<ul>
<li>The weather grew cold in the night.晚上天气慢慢变冷</li>
</ul>
</li>
<li>turn:本意转身，可表达“快速变”<ul>
<li>His face turned pale.他脸色变得苍白</li>
</ul>
</li>
</ul>
</li>
<li>4.感官动词(look, sound, smell, taset, feel)：一律翻译为“。。。起来”，即看起来，听起来。。后面只能接形容词（或相当于形容词的分词）作表语<ul>
<li>I felt very nervous when I went into his office.我走进他的办公室，感到非常紧张。</li>
<li>He looked very angry.他看上去非常气愤。</li>
<li>They were all hungry and the food smelled good.他们全都饿了，饭菜散发出阵阵香味。</li>
<li>感官动词后，决不能直接加名词作表语，如要加名词，必须用：感官动词+介词like+名词，此时翻译为”像…..”<ul>
<li>The sun looks like an orange globe.太阳看上去像只橙色的球体。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>造句练习<ul>
<li>每件事都很有序。( in good order )<ul>
<li>Everything is in good order.</li>
</ul>
</li>
<li>我的房子就在这里。<ul>
<li>My house is here.</li>
</ul>
</li>
<li>这辆轿车看起来很棒。<ul>
<li>This car looks very good.</li>
</ul>
</li>
<li>鳄鱼肉尝起来就像鸡肉。<ul>
<li>Alligator meat tastes like chicken.</li>
</ul>
</li>
<li>地震期间你应该保持冷静。<ul>
<li>You should keep calm during an earthquake.</li>
</ul>
</li>
<li>迈克去年成为一名职业篮球运动员。<ul>
<li>Mike became a professional basketball player last year.</li>
</ul>
</li>
<li>天气变得寒冷而多风( cold and windy )<ul>
<li>The weather has turned cold and windy.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="不及物动词-gt-主谓句型"><a href="#不及物动词-gt-主谓句型" class="headerlink" title="不及物动词-&gt;主谓句型"></a>不及物动词-&gt;主谓句型</h3><ul>
<li>不及物动词是什么<ul>
<li>intransitive verb，简称vi. 就是本身意就很完全，没有作用对象，不需要加宾语就能构成完整的句子，例如：游泳，出生，笑，做梦.</li>
<li>判断这类动词，有个方法很简单，把这个动词前面加上”被”字，看正常不正常。如果正常，就是及物动词，不正常，就是不及物动词。例如：被打，被处罚，被喜欢.</li>
<li>但如果说：被跳舞，被做梦，被游泳…..，肯定自己会觉得不通顺</li>
</ul>
</li>
<li>注意事项<ul>
<li>1.这个句型，一般来说都带有状语，来进一步说明这个动作发生的时间，地点，目的….<ul>
<li>Detectives(主语) were waiting(谓语) at the airport(地点状语) all morning(时间状语</li>
<li>They(主语) were talking(谓语) loudly(方式状语)</li>
</ul>
</li>
<li>2.很多动词，本身既可作不及物动词，也可作及物动词，得在具体语境中判断</li>
</ul>
</li>
<li>造句练习<ul>
<li>1.鸟儿快乐地唱着歌。<ul>
<li><ol>
<li>The birds sing happily.</li>
</ol>
</li>
</ul>
</li>
<li>2.这场雨下午会停。<ul>
<li><ol start="2">
<li>The rain will stop in the afternoon.</li>
</ol>
</li>
</ul>
</li>
<li>3.孩子们正在公园里玩耍。<ul>
<li><ol start="3">
<li>The children are playing in the park.</li>
</ol>
</li>
</ul>
</li>
<li>4.我的老师昨天在医院去世了。(pass away)<ul>
<li><ol start="4">
<li>My teacher passed away in the hospital yesterday.</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="及物动词-gt-主谓宾"><a href="#及物动词-gt-主谓宾" class="headerlink" title="及物动词-&gt;主谓宾"></a>及物动词-&gt;主谓宾</h3><ul>
<li>举例<ul>
<li>Yesterday(时间状语), a pigeon(主语) carried(谓语) the first(定语) message(宾语) from Pinhurst to Silbury(地点状语). 昨天，一只鸽子把第一封信从平赫特带到锡尔伯里。</li>
<li>The bird(主语) covered(谓语) the distance(宾语) in three minutes.(时间状语) 这只鸟只用了3分钟就飞完了全程。</li>
<li>The bride and the groom cut the wedding cake together.新郎和新娘一起切下结婚蛋糕。</li>
<li>I had an amusing experience last year.去年我有过一次有趣的经历。</li>
<li>This wonderful plane can carry seven passengers.这架奇妙的飞机可以载7名乘客。</li>
</ul>
</li>
<li>及物动词是什么？<ul>
<li>transitive verb，简称vt. 就是加了宾语以后意思很完全的动词，有主动和被动两种语态</li>
<li>有些短语相当于及物动词，称为及物动词短语</li>
</ul>
</li>
<li>造句练习<ul>
<li>1.老师在教室的墙上贴了一些照片。<ul>
<li><ol>
<li>The teacher put up some pictures on the wall in the classroom.</li>
</ol>
</li>
</ul>
</li>
<li>2.在公共场合你应该尊敬老人。<ul>
<li><ol start="2">
<li>You should respect the old in public places.</li>
</ol>
</li>
</ul>
</li>
<li>3.他一周前开始节食。<ul>
<li><ol start="3">
<li>He began his diet a week ago.</li>
</ol>
</li>
</ul>
</li>
<li>4.他于1935年9月创造了一项新的世界纪录。<ul>
<li><ol start="4">
<li>He set up a new world record in September 1935.</li>
</ol>
</li>
</ul>
</li>
<li>5.只有极少数人能实现他们的梦想。<ul>
<li><ol start="5">
<li>Only very few people can realize their dreams.</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="不完全及物动词-gt-主语-谓语-宾语-宾语补语"><a href="#不完全及物动词-gt-主语-谓语-宾语-宾语补语" class="headerlink" title="不完全及物动词-&gt;主语+谓语+宾语+宾语补语"></a>不完全及物动词-&gt;主语+谓语+宾语+宾语补语</h3><ul>
<li>概述<ul>
<li>她使我爱上生活。</li>
<li>这个”使”就是不完全及物动词，如果只说“她使我”意思当然不完整, 得加上补语”爱上生活”，才是完整的句子</li>
<li>他的表演让我失望。</li>
<li>这个“让”如果说”他的表演让我”，当然也不完全得加上补语”失望”</li>
</ul>
</li>
<li>不完全及物动词是什么？<ul>
<li>incomplete transitive verb, 简称i.vt 这种动词加了宾语以后，意思仍</li>
<li>然不完全, 需要加上补语( complement )才能使句子完整</li>
<li>补语是补充说明宾语的特征，或者宾语的动作。</li>
</ul>
</li>
<li>如何区分完全及物动词和不完全及物动词<ul>
<li>最常见的不完全及物动词：使役动词：使…..做…. (make, have, let, get)</li>
<li>make / have /let +宾语+动词原形(补语)</li>
<li>get +宾语+动词不定式(补语)<ul>
<li>I made him wash the car. 我叫他洗车。</li>
<li>I got him to wash the car.我叫他洗车。</li>
</ul>
</li>
</ul>
</li>
<li>综上所述，只要是宾语发出的动作，或宾语的状态，均可构成此类句型。知道了这一道理，遇到类似句子，都能做出准确判断。</li>
<li>造句练习<ul>
<li>炎热的天气使我感到昏昏欲睡(feel lethargic)。<ul>
<li>The hot weather made me feel lethargic.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="授予动词-gt-主语-谓语-间接宾语-直接宾语"><a href="#授予动词-gt-主语-谓语-间接宾语-直接宾语" class="headerlink" title="授予动词-&gt;主语+谓语+间接宾语+直接宾语"></a>授予动词-&gt;主语+谓语+间接宾语+直接宾语</h3><ul>
<li>概述<ul>
<li>我给了他一本书”</li>
<li>动作“给”需要两个步骤,先拿起书,再给他</li>
<li>有两个宾语“他”和“书”，因为先拿书,再给他，所以“书”是直接宾语，“他”是间接宾语</li>
</ul>
</li>
<li>授予动词是什么？<ul>
<li>及物动词中的一种，但需要接两个宾语，第一个是间接宾语(indirect object， i.o.)，表示授予的对象，第二个宾语是直接宾语(direct object, d.o.)，表示授予的东西</li>
<li>要注意的是，直接宾语和间接宾语，都是谓语动作的作用对象，这是与第四大句型的区别。在第四大句型中，补语是说明宾语的性质，或者是宾语发出的动作</li>
</ul>
</li>
<li>最常用的授予动词：give, send, tell, teach, pay, show, offer<ul>
<li>Richard Mattes gave the testers six different kinds of things. Richard Mattes给了这些测试者6种不同类型的东西。</li>
<li>I send him a book in reward for his help.我送给他一本书来答谢他的帮助。</li>
<li>The scientist told us many stories about birds.博物学家给我们讲述了许多有关鸟儿的故事。</li>
<li>A friendly waiter taught me a few words of Italian. Then he lent me a book.一位好客的服务员教了我几句意大利语，之后还借给我一本书。</li>
<li>Yesterday I paid him a visit.昨天我去看望 了他。</li>
<li>Then he showed me the contents of the parcel.接着他给我看了包里的东西。</li>
<li>He offered me a lot of money.他给了我很多钱。</li>
</ul>
</li>
<li>第4大句型和第5大句型的区分<ul>
<li>只要是谓语的动作，作用于两个不同的名词，也就是两个宾语,就是第5大句型。而在第4大类句型中，补语是宾语的动作或状态。</li>
<li>举例:</li>
<li>1.他让我学习。”学习”是“我”发出的动作</li>
<li>2.他给了我一本书。 “书”和“我”都是谓语“给”的作用对象</li>
<li>所以，第1句是第4大句型，第2句是第5大句型。</li>
</ul>
</li>
<li>造句练习<ul>
<li>1.情人节他送给女友一束花。<ul>
<li><ol>
<li>He gave a bunch of flowers to his girlfriend on Valentine’s Day.</li>
</ol>
</li>
</ul>
</li>
<li>2.请寄给我一张收到此款的收据。<ul>
<li><ol start="2">
<li>Please send me a receipt for the money.</li>
</ol>
</li>
</ul>
</li>
<li>3.他告诉我几个关于英语老师的神奇故事。<ul>
<li><ol start="3">
<li>He told me some magical stories about our English teacher.</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="首相的英语学习方法"><a href="#首相的英语学习方法" class="headerlink" title="首相的英语学习方法"></a>首相的英语学习方法</h2><ul>
<li>1.了解句子成分的具体含义识别出句子的不同成分</li>
<li>2.学习时态、语态等基础知识</li>
<li>3.学习从句的构句方法把一个长句子变成一个句子成分把不同句子成分组合</li>
<li>4.用彩色笔标注不同句子成分，从句子相关成分中，积累相关用法</li>
<li>5.把复杂句拆成若干个简单句子，设定好主语宾语，采用学习的规则，拼成长句子</li>
<li>或，读完句子英汉互译，先翻译成中文再翻译成英文再对照原句</li>
</ul>
<h2 id="7大英语句子成分概述"><a href="#7大英语句子成分概述" class="headerlink" title="7大英语句子成分概述"></a>7大英语句子成分概述</h2><ul>
<li>主语：句子的主人</li>
<li>谓语：表达主语动作状态</li>
<li>宾语：客体、受体</li>
<li>表语：表达主语具体情况</li>
<li>补语：补充说明</li>
<li>定语：一个句子里用来界定、限定名词的部分<ul>
<li>能作定语的语法成分：形容词以及相当于形容词性质的语法成分</li>
<li>英语95%的定语遵循“前小后大”法则<ul>
<li>1个单词组成的定语(限定词，形容词，名词及名词所有格)， 放在所修饰名词的前面。</li>
<li>2个以上单词组成的定语( of属格，形容词短语，介词短语，分词短语，不定式短语等)， 放在所修饰词后面。 <ul>
<li><ol>
<li>They were expecting a(限定词) valuable(形容词) parcel of diamonds(of属格) from South Africa.(介词短语)他们正期待从南非来的一个装着钻石的贵重包裹。(上面四个定语,均修饰parcel)</li>
</ol>
</li>
<li><ol start="2">
<li>Mrs. Rumbold was a(限定词) large(形容词), unsmiling(形容词) lady in a tight black dress.(介词短语) 兰伯尔德夫人是一位身材高大、表情严肃的女人，穿一件紧身的黑衣服。</li>
</ol>
</li>
<li><ol start="3">
<li>First of all, he wrote out a(限定词) long(形容词) list of all the foods.(of属格)首先，他开列了一张长长列了所有食物的目录。</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li>造句练习<ul>
<li>1.我喜欢课桌上那本英语书。<ul>
<li>I like the English book on the table.</li>
</ul>
</li>
<li>2.他们正在研究一个关于贸易标准(trading standard)的复杂问题。<ul>
<li>They are studying a complicated problem about trading Standard.</li>
</ul>
</li>
<li>3.我将告诉你们昨天老师给我讲的那个非常有趣的关于月亮的中国古代故事<ul>
<li>I will tell you a very interesting old Chinese story about the moon that my teacher told me yesterday</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>状语：用来描述一个动作的细节特征的成分，除了修饰动词外，状语也可以修饰除了名词之外的任何词(如形容词，介词，连词，还可以修饰副词本身!)<ul>
<li>能作状语的语法成分：副词以及相当于副词的语法成分</li>
<li>小状语(单独的副词)，放在其所修饰的词语之前：修饰动词时，放在动词之前。修饰其它成分(形容词、介词、连词、副词)时，放在其它成分之前。注：如有需要，小状语可以放在句中任何位置!<ul>
<li>We are now living in a beautiful new house in the country.我们现在住在乡间的一栋漂亮的新住宅里。</li>
<li>Letters will cost a little more, but they will certainly travel faster.这样会稍微多花点钱，但肯定是快得多了。</li>
</ul>
</li>
<li>大状语(2个及以上单词构成的状语，如介词短语、不定式短语、状语从句)，放在整句的两头。放在开头时，一般要加上逗号。<ul>
<li>On Wednesday evening, we went to the Town Hall. 星期三的晚上，我们去了市政厅。</li>
<li>I was having dinner at a restaurant when Tony Steele came in.我正在一家饭馆吃饭，托尼斯蒂尔走进来。</li>
</ul>
</li>
<li>1.句子同时出现几个时间或者地点状语时，从小到大<ul>
<li>We landed in America at 8 o’ clock on June 15th，2012.我们2012年6月15日上午8点在美国着陆。</li>
<li>We live at number 35, South Renmin Road, Chengdu.我们住在成都市人民南路35号。</li>
</ul>
</li>
<li>2.句子后面有多种状语时，顺序是方式-地点-时间<ul>
<li>He put his milk bottles carefully on the doorstep every morning. 他每天早上小心地把牛奶瓶放在门口台阶上。</li>
</ul>
</li>
</ul>
</li>
<li>同位语：句子中指代同一事物的两个词、短语或从句，称为同位关系<ul>
<li>英语里边的同位语， 不属于单独的7大句子成分，而是和英语里边的名词成分(主语，宾语，表语)是并列关系，相当于对该名词的进一步解释说明。名词或任何相当于名词的成分，均可作同位语。</li>
<li>造句练习<ul>
<li>1.我的英语老师Brent Peter先生是加拿大人。<ul>
<li>My English teacher, Mr. Brent Peter, is a Canadian.</li>
</ul>
</li>
<li>2.昨天我遇到了我弟弟的朋友汤姆。<ul>
<li>Yesterday I met Tom, a friend of my brother’ s.</li>
</ul>
</li>
<li>3.我们中国人民是勤劳勇敢的。<ul>
<li>We Chinese people are brave and hardworking.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="2-如何避免犯时态错误"><a href="#2-如何避免犯时态错误" class="headerlink" title="2.如何避免犯时态错误"></a>2.如何避免犯时态错误</h1><h2 id="时态的本质"><a href="#时态的本质" class="headerlink" title="时态的本质"></a>时态的本质</h2><ul>
<li>时态=”时间”和”状态”=tense and aspect</li>
<li><img src="http://anki190912.xuexihaike.com/20201104130711.png?imageView2/2/h/200"></li>
</ul>
<h2 id="谓语动词的四种状态"><a href="#谓语动词的四种状态" class="headerlink" title="谓语动词的四种状态"></a>谓语动词的四种状态</h2><ul>
<li>1.一般（simple）:强调动作发生的时间，而非状态<ul>
<li>I play basketball. 我打篮球</li>
</ul>
</li>
<li>2.进行（continuous）：强调动作的过程（描绘，生动性）<ul>
<li>I am playing basketball. 我正在打篮球</li>
</ul>
</li>
<li>3.完成（perfect）:强调动作的结果（逻辑推导性）<ul>
<li>I have played basketball. 我打完篮球了</li>
</ul>
</li>
<li>4.完成进行（perfect continuous）：强调动作的结果和过程<ul>
<li>I have been playing basket ball. 我一直都有打篮球</li>
</ul>
</li>
</ul>
<h2 id="动词在不同时态下词形的变化"><a href="#动词在不同时态下词形的变化" class="headerlink" title="动词在不同时态下词形的变化"></a>动词在不同时态下词形的变化</h2><ul>
<li>进行：be+现在分词(be按照3时及主语人称变化)</li>
<li>完成：have+过去分词(have按照3时及主语人称变化)</li>
<li>完成进行：have been+现在分词(have按照3时及主语人称变化)</li>
<li>以动词do为例：<ul>
<li>现在 进行: am/is/are doing</li>
<li>过去 进行: was/were doing</li>
<li>将来 进行: will be doing</li>
<li>现在 完成: have/has done</li>
<li>过去 完成: had done</li>
<li>将来 完成: will have done</li>
<li>现在 完成进行: have/has been doing</li>
<li>过去 完成进行: had been doing</li>
<li>将来 完成进行: will have been doing</li>
</ul>
</li>
</ul>
<h2 id="一般态：强调动作本身"><a href="#一般态：强调动作本身" class="headerlink" title="一般态：强调动作本身"></a>一般态：强调动作本身</h2><ul>
<li>定义<ul>
<li>只强调动作发生的时间，不强调动作的状态，就是说它只关心这个动作是发生在过去，现在，还是将来，不关心这件事情有没有做好</li>
</ul>
</li>
<li>一般态和进行态的主要区别<ul>
<li>I walk to school.我走路去学校， 这个句子只强调一般情况，不强调动作的状态是怎么样。</li>
<li>I’m walking to school.这个强调的就是动作本身，我正在路上</li>
</ul>
</li>
<li>一般现在时<ul>
<li>1.表示事物现在的情况或状态（主系表句型）<ul>
<li>I <code>am</code> a doctor. 我是一个医生。</li>
<li>You <code>are</code> an unrepeatable miracle. 你是一个无法重复的奇迹。</li>
</ul>
</li>
<li>2.表示经常性、习惯性动作（常有时间副词修饰）<ul>
<li>I <strong>never</strong> <code>get up</code> early on Sundays. I sometimes <strong>stay</strong> in bed until lunch time. 星期天我是从来不早起的，有时我要直躺到吃 午饭的时候。</li>
<li>Winners <strong>often</strong> <code>seek</code> opportunity when losers <strong>want</strong> security. 失败者寻求安全的时候，成功者寻求机遇。</li>
<li>Do you <strong>always</strong> <code>get up</code> so late? It’s one o’clock! 你总是起得这么晚吗?现在已经1点钟了!</li>
<li>注：一般现在时常使用表示频率的副词，如never, often, always</li>
</ul>
</li>
<li>3.表示客观真理、格言<ul>
<li>Time <code>flies</code>. 时光飞逝。</li>
<li>The early bird <code>catches</code> the worm. 早起的鸟儿有虫吃。</li>
<li>Failure <code>is</code> the mother of success. 失败是成功之母。</li>
</ul>
</li>
</ul>
</li>
<li>一般过去式：表示过去的动作、习惯、事实<ul>
<li>Last Sunday I <code>got up</code> very late. I <code>looked</code> out of the window. It <code>was</code> dark outside. 在上个星期天，我起得很晚。我望望窗外，外面一片昏暗。</li>
<li>Last summer, I <code>went</code> to Italy. 去年夏天，我去了意大利。</li>
<li>Colubus discovered America in 1742. 哥伦布于1492年发现了美洲。</li>
<li>注：一般过去时常使用过去具体时间的副词，如yesterday, last week, two years ago, in 1998……</li>
</ul>
</li>
<li>一般将来时<ul>
<li>1.表示将来发生的动作或状态，或倾向(will译为“将要”)<ul>
<li>He <code>will</code> soon <code>visit</code> Darwin. From there, he <code>will fly</code> to Perth. 他不久还将到达达尔文去，从那里，他再飞往珀斯。</li>
<li>People <code>will</code> run into problems in their lives. 人们在生活中总会遇到问题。</li>
<li>A small leak <code>will</code> sink a great ship. 小裂缝可以沉大船。(千里之堤，溃于蚁穴)</li>
</ul>
</li>
<li>be going to+动词原形：表示将来时，但多表示“计划”，主语多为“人”。will表示“意愿”时，主语是“人”。但也可以表示“预测”，主语是“物”<ul>
<li>Debbie Hart is going to swim across the English Channel tomorrow. 黛比.哈特准备明天横渡英吉利海峡。</li>
<li>She is going to set out from the French coast at five o’clock in the morning. 她打算早上5点钟从法国海岸出发。</li>
<li>Mr. Thompson is going to sell it because it is haunted. 汤普森先生之所以想卖它，是因为那里常闹鬼。</li>
</ul>
</li>
<li>be about to + 动词原形：即将。。<ul>
<li>He is about to leave for Shenyang. 他将要离开去沈阳。</li>
</ul>
</li>
</ul>
</li>
<li>造句练习<ul>
<li>1.这家便利店全天24小时营业。<ul>
<li><ol>
<li>This convenience store is open 24 hours a day.</li>
</ol>
</li>
</ul>
</li>
<li>2.我爷爷每天早上都在公园里散步。<ul>
<li><ol start="2">
<li>My grandfather takes a walk in the park every morning.</li>
</ol>
</li>
</ul>
</li>
<li>3.昨天我很无聊，便跟着几个朋友看电影去了。<ul>
<li><ol start="3">
<li>I felt bored yesterday, So I went to the movies with several friends.</li>
</ol>
</li>
</ul>
</li>
<li>4.他明天将要去纽约。<ul>
<li><ol start="4">
<li>He will go to New York tomorrow.</li>
</ol>
</li>
</ul>
</li>
<li>5.看！那艘船快要沉没了。(be about to)<ul>
<li><ol start="5">
<li>Look! The boat is about to sink.</li>
</ol>
</li>
</ul>
</li>
<li>6.我们马上就要吃午餐了。<ul>
<li><ol start="6">
<li>We are going to have lunch soon.</li>
</ol>
</li>
</ul>
</li>
<li>7.太阳东升西落。<ul>
<li>7.The sun rises in the east and sets in the west.</li>
</ul>
</li>
<li>8.我每周去两次健身房。<ul>
<li><ol start="8">
<li>I go to the gym twice a week.</li>
</ol>
</li>
</ul>
</li>
<li>9.我昨天早上在图书馆看到他。<ul>
<li><ol start="9">
<li>I saw him in the library yesterday morning.</li>
</ol>
</li>
</ul>
</li>
<li>10.我准备学习计算机科学。<ul>
<li><ol start="10">
<li>I am going to learn the computer science.</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="进行态：强调动作的过程"><a href="#进行态：强调动作的过程" class="headerlink" title="进行态：强调动作的过程"></a>进行态：强调动作的过程</h2><ul>
<li>延续性动词和瞬间动词<ul>
<li>在英语中，按照动作发生的时间长短，动词分成了延续性动词和瞬间动词</li>
<li>如结婚是一个瞬间动词，交换仪式之后，就是结婚了。所以不能说They were marrying last week. 只能说They married last week. 若要表达正在举行婚礼，可以说They are having a wedding ceremony</li>
</ul>
</li>
<li>现在进行时<ul>
<li>1.表示现在正在做的动作，此时be动词译为“正在”<ul>
<li>He <code>is playing</code> basketball. 他正在打篮球。</li>
<li><code>It&#39;s raining</code> heavily now. 现在正在下大雨。</li>
<li>注意：主系表句型在口语中通常用一般现在时，但如果要强调此时的状态，可以用现在进行时</li>
<li>You are very rude! ——&gt; You are <code>being</code> very rude! 你太粗鲁了！（你现在的行为粗鲁，而不是你这个人粗鲁）</li>
</ul>
</li>
<li>2.表示即将发生的动作（通常是表示“位移”短暂动词come, go, arrive, leave, start, begin, return, die, take），此时be动词译为“即将”<ul>
<li>“A new play <code>is coming</code> to’The Globe’ soon,” I said.“Will you be seeing it?” “一出新剧要来’环球剧场’上演了，”我说，“您去看吗?”</li>
<li>‘We <code>are going back</code> now,’ said the conductor. “我们现在要返回去，”售票员说。</li>
</ul>
</li>
</ul>
</li>
<li>过去进行时<ul>
<li>表示过去某时正在做的事情<ul>
<li>A man <code>was lying</code> in the box during the flight. 那个航班上，有一个人正躺在箱子里。</li>
<li>I <code>was having</code> dinner at a restaurant when Tony Steele came in. 我正在一家饭馆吃饭，托尼.斯蒂尔走了进来。</li>
</ul>
</li>
</ul>
</li>
<li>将来进行时<ul>
<li>表示将来某时将进行的动作<ul>
<li>They <code>will be arriving</code> here tomorrow. 他们明天就要到达此地。</li>
<li>Tomorrow evening they <code>will be singing</code> at the Workers’ Club. 明晚他们将在工人俱乐部演出。</li>
<li>The Greenwood Boys <code>will be staying</code> for five days. “绿林少年”准备在此逗留5天。</li>
<li>They <code>will be trying</code> to keep order. 他们将设法维持秩序。</li>
<li>The shuttle Endeavour <code>will be taking</code> the astronauts to the Hubble. “奋进”号航天飞机将把宇航员送上哈勃。</li>
</ul>
</li>
</ul>
</li>
<li>注意事项<ul>
<li>进行态，其实就是一般态的生动模式。任何一个进行态的句子，都可以改成一般态。一般态却不一定能改成进行态，比如动词是延续动词才能改。<ul>
<li>I <code>looked</code> out of the window. 可以改成 I <code>was looking</code> out of the window. (look是延续的)</li>
<li>I never get up early on Sundays. 不能改成 I was never getting up early on Sundays. 因为get up是短暂的。</li>
</ul>
</li>
</ul>
</li>
<li>造句练习<ul>
<li>1.你在桌子下面做什么? <ul>
<li><ol>
<li>What are you doing under the table?</li>
</ol>
</li>
</ul>
</li>
<li>2.我正梦见你时，电话铃响了。(dream about)<ul>
<li><ol start="2">
<li>I was dreaming about you when the telephone rang.</li>
</ol>
</li>
</ul>
</li>
<li>3.明天早上这个时候，我爸爸将正在修剪草坪。(mow the lawn)<ul>
<li><ol start="3">
<li>My dad will be mowing the lawn at this time tomorrow morning.</li>
</ol>
</li>
</ul>
</li>
<li>4.我女儿正在学习，所以你最好别去烦她。<ul>
<li><ol start="4">
<li>My daughter is studying, so you’d better not bother her.</li>
</ol>
</li>
</ul>
</li>
<li>5.明年这个时候我将正在美国念书。<ul>
<li><ol start="5">
<li>l’ll be studying in the United States at this time next year.</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="完成态：强调动作的结果"><a href="#完成态：强调动作的结果" class="headerlink" title="完成态：强调动作的结果"></a>完成态：强调动作的结果</h2><ul>
<li>现在完成时<ul>
<li>1.表示到现在为止已经完成的动作（发生时间不明）<ul>
<li>I <code>have</code> just <code>received</code> a letter from my brother, Tim. 我刚刚收到弟弟蒂姆的来信。</li>
<li>He <code>has</code> just <code>bought</code> an Australian car and <code>has gone</code> to Alice Springs, a small town in the centre of Australia. 他刚买了一辆澳大利亚小汽车，现在去了澳大利亚中部的小镇艾利斯斯普林斯。</li>
<li>Since then, he <code>has developed</code> another bad habit. 从那以后，它养成了另外一种坏习惯。</li>
<li>He <code>has gone</code> to Shanghai. 他去了上海(言外之意:他不在说话现场)</li>
</ul>
</li>
<li>2.如果是延续性动词，表示持续到现在的动作（或状态）。（注意：be动词是可延续的）<ul>
<li>She <code>has lived</code> here for 10 years. 她住在这里10年了。</li>
<li>He <code>has been</code> there for six months. 他在那儿已经住了6个月了。</li>
<li>I <code>have been</code> to the Great Wall. 我去过长城。</li>
</ul>
</li>
</ul>
</li>
<li>现在完成时常用时间副词<ul>
<li>1.自从：since+时间点<ul>
<li>Since then, Captain Fawcett <code>has flown</code> passengers to many unusual places. 从那时开始，弗西特机长已经载送乘客到过许多不寻常的地方。</li>
</ul>
</li>
<li>2.有若干时间之久：for+时间段<ul>
<li>Mr. Hart <code>has trained</code> his daughter for years. 哈特先生训练她的女儿已经多年了。</li>
</ul>
</li>
<li>3.到目前为止：so far/up to now<ul>
<li>But so far, the public <code>has expressed</code> its gratitude to the students in letters to the Press. 但到目前为止，公众已经向新闻界写信表达他们对学生们的感激之情了。</li>
<li>Up to now, Mr. Scott <code>has sent</code> a great many requests for spare parts and other urgent messages from one garage to the other. 到目前为止，斯科特先生从一个汽车修理部向另一个发送了大量索取备件的信件和其他紧急函件。</li>
</ul>
</li>
<li>4.最近：recently/lately<ul>
<li><code>Have</code> you <code>talked</code> to Jane lately? 你最近有没有和Jane说过话?</li>
</ul>
</li>
<li>5.一次/两次/几次/多次：once/twice/a few times/many times<ul>
<li>Jack <code>has read</code> the novel three times. (这里的read是过去分词) 这本小说杰克已经看过三遍了。</li>
</ul>
</li>
<li>6.过去若干年/月/日以来：over/during/for + the last/past+数字+years/months/days<ul>
<li>Over the last three months, oil prices <code>have reached</code> a record high. 过去三个月以来，油价创了历史新高。</li>
</ul>
</li>
</ul>
</li>
<li>中国人最容易用错的3个短暂动词<ul>
<li>请翻译一下:<ul>
<li>1:他去北京3天了。</li>
<li>2:他结婚已经3年了。</li>
<li>3:他已经死了3年了</li>
</ul>
</li>
<li>答案：<ul>
<li>常见错误翻译</li>
<li>1: He has go to Beijing for 3 days. (X)</li>
<li>2: He has married for 3 years. (X)</li>
<li>3: He has died for 3 years. (X)</li>
<li>go, marry, die在英语中均为短暂动词，都是一瞬间完成的，后面不可以接时间副词，这里可以说:</li>
<li>1: He has gone to Beijing. </li>
<li>2: He has married. </li>
<li>3: He has died. </li>
<li>他不可能不停地做“去”这个动作3天，也不可能不停“结婚”这个动作3年，更不可能“死”3年才死透。要正确表达，必须使用对应的主系表句型，因为系动词是可以延续的，用主系表句型表示状态，才能加表示一段时间的状语。所以正确的说法是:</li>
<li>1: He has <code>been to Beijing</code> for 3 days.</li>
<li>2: He has <code>been married</code> for 3 years.</li>
<li>3: He has <code>been dead</code> for 3 years.</li>
<li>此时，句子的动词是be动词，后面的to Beijing (介词短语)，married, dead (形容词)为表语。</li>
</ul>
</li>
</ul>
</li>
<li>过去完成时<ul>
<li>表示截止过去某时为止所完成的动作或经验。（过去完成时不能单独存在，要与另一使用一般过去时的句子或者表示过去的副词短语连用）。(had译为“已经”或“曾经”)<ul>
<li>My old friend, Harrison, <code>had lived</code> in the Mediterranean for many years before he returned to England. 我的老朋友哈里森在回到英国以前曾多年居住在地中海地区。</li>
<li>A short time before, great trees <code>had covered</code> the countryside for miles around. 就在不久之前，参天大树还覆盖着方圆数英里的土地。</li>
<li>By then, however, in many places the grass <code>had</code> already <code>taken root</code>. 然而到那时，很多地方的草已经生了根。</li>
</ul>
</li>
</ul>
</li>
<li>将来完成时<ul>
<li>表示到将来某时为止所完成或仍然继续的动作或经验等。（常与介词by构成的时间状语连用，表示“到…的时候”）<ul>
<li>Workers <code>will have completed</code> the new roads by the end of this year. 工人们将在今年年底前把新路铺好。</li>
<li>By the end of next year, they <code>will have finished</code> work on the new stadium. 到明年年底，他们将把新体育场建成。</li>
</ul>
</li>
</ul>
</li>
<li>造句练习<ul>
<li>1.彼得去过香港很多次。<ul>
<li><ol>
<li>Peter has been to Hong Kong many times.</li>
</ol>
</li>
</ul>
</li>
<li>2.进入初中以来，我对英语很狂热。(be crazy about)<ul>
<li><ol start="2">
<li>I have been crazy about English since I entered junior high school.</li>
</ol>
</li>
</ul>
</li>
<li>3.到目前为止，我已经完成这项计划的三分之二。<ul>
<li><ol start="3">
<li>So far, I have finished two third of the project.</li>
</ol>
</li>
</ul>
</li>
<li>4.我最近很忙，恐怕要到下周一我才有空。<ul>
<li><ol start="4">
<li>I have been very busy recently. l’m afraid that I won’t be free until next Monday.</li>
</ol>
</li>
</ul>
</li>
<li>5.过去5年来，这个好孩子都尽力照顾他生病的母亲。<ul>
<li><ol start="5">
<li>Over the past 5 years, the good boy has tried his best to take care of his ill mother.</li>
</ol>
</li>
</ul>
</li>
<li>6.史密斯先生搬来这里之前已经在加拿大住了20年。<ul>
<li><ol start="6">
<li>Mr. Smith had lived in Canada for 20 years before he moved here.</li>
</ol>
</li>
</ul>
</li>
<li>7.等我到达车站时，火车已经开走了。<ul>
<li><ol start="7">
<li>By the time I got to the station, the train had left.</li>
</ol>
</li>
</ul>
</li>
<li>8.玛丽昨天告诉我她很久以来一直想出国旅游。<ul>
<li><ol start="8">
<li>Mary told me yesterday that she had long wanted to travel abroad.</li>
</ol>
</li>
</ul>
</li>
<li>9.我很生气，因为我女朋友又对我爽约了。(stand sb. up)<ul>
<li><ol start="9">
<li>I was very angry because my girlfriend had stood me up again.</li>
</ol>
</li>
</ul>
</li>
<li>10.明天这个时候，约翰将已经达到芝加哥了。<ul>
<li><ol start="10">
<li>John will have arrived in Chicago by this time tomorrow.</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="完成进行态：同时强调动作的结果和过程（延续性动词）"><a href="#完成进行态：同时强调动作的结果和过程（延续性动词）" class="headerlink" title="完成进行态：同时强调动作的结果和过程（延续性动词）"></a>完成进行态：同时强调动作的结果和过程（延续性动词）</h2><ul>
<li>延续性动词<ul>
<li>“我收到一封信”<ul>
<li>你只能说I have received a letter.</li>
<li>但是绝对不能说“I have been receiving a letter.”</li>
<li>因为receive收到这个动词， 是一瞬间完成的，不能延续，你不可能一直不停收同一 封信。</li>
</ul>
</li>
<li>“我住在广州3年了“<ul>
<li>你可以说I have lived in Guangzhou for three years.</li>
<li>也可以说I have been living in Guangzhou for three years.两句话意思完全相同。但是第二句更加生动形象。</li>
<li>瞬间动词是不能放在完成进行形态的。</li>
</ul>
</li>
</ul>
</li>
<li>现在完成进行时<ul>
<li>表示一直继续到现在，且可能继续下去的动作。（通常和表示时间段的副词连用，如for, since, all morning…）<ul>
<li>We have just moved into a new house and I <code>have been working</code> hard all morning. 我们刚刚搬进一所新房子，我辛辛苦苦地干了整整一个上午。</li>
<li>If you haven’t discovered your dream, probably <code>you&#39;ve been missing</code> too much. 如果你还没有发现梦想，或许一直以来你失去的太多了。</li>
</ul>
</li>
</ul>
</li>
<li>过去完成进行时<ul>
<li>表示一直继续到过去某时，而当时仍然在继续的动作。（过去完成进行时的句中必须有表示过去的时间状语）<ul>
<li>Firemen had been fighting the forest fire for nearly three weeks before they could get it under control. 消防队员们同那场森林大火搏斗了将近3个星期才最后把火势控制住。</li>
<li>The planes had been planting seed for nearly a month when it began to rain. 飞机撒播近一一个月后，开始下起雨来。</li>
<li>Bleriot had been making planes since 1905 and this was his latest model. 布莱里奥从1905年起便开始研制飞机，这架飞机是他制作的最新型号。</li>
</ul>
</li>
</ul>
</li>
<li>将来完成进行时<ul>
<li>一直继续到将来某时，且可能继续下去的动作<ul>
<li>The day before his retirement, Mr. Page will have been teaching for a total of forty years. 佩奇先生退休的前一天正好是他执教满40年的日子。</li>
<li>By the time you come back tonight, I will have been sleeping for five hours. 等你今晚回来时，我已经持续睡了5个小时了。</li>
</ul>
</li>
</ul>
</li>
<li>总结<ul>
<li>任何完成进行态，都能改成完成态。但是完成态不一定能改成完成进行态，必须是延续动词，才能改</li>
</ul>
</li>
<li>造句练习<ul>
<li>1.约翰自去年起就一直学日语。他希望去日本留学。<ul>
<li><ol>
<li>John has been learning Japanese since last year. He expects to study in Japan in the future.</li>
</ol>
</li>
</ul>
</li>
<li>2.Lulu的车子抛锚时，她已经持续开了8个小时了。<ul>
<li><ol start="2">
<li>By the time her car broke down, Lulu had been driving for 8 hours.</li>
</ol>
</li>
</ul>
</li>
<li>3.到今年年底，王老师教英语将有10年了。<ul>
<li><ol start="3">
<li>Mr. Wang will have been teaching English for 10 years by the end of this year.</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="过去将来时"><a href="#过去将来时" class="headerlink" title="过去将来时"></a>过去将来时</h2><ul>
<li>通常多用于叙述性故事中，或间接引语中<ul>
<li>A few hours earlier, someone had told the police that thieves <code>would try</code> to steal the diamonds. 数小时之前，有人向警方报告，说有人企图偷走这些钻石。(一般态)</li>
<li>Then he smiled and told me I <code>would receive</code> an extra thousand pounds a year! 然后他微笑了一下告诉我说，我每年将得到1000英镑的额外收入。(一般态)</li>
<li>He said that it <code>would be</code> possible to build a platform in the centre of the Channel. 他说，可以在隧道中央建造一座平台。(一般态)</li>
<li>She said she <code>would be setting off</code> on the 10 o’clock train. 她说她将乘10点钟的火车走。(进行态)</li>
<li>I guessed that Helen <code>would have told</code> her something. 我猜海伦会告诉她一些情况的。(完成态)</li>
<li>He told me that by the end of the year he <code>would have been living</code> there for thirty years. 他告诉我到今年末他已经住在那儿30年了。(完成进行态)</li>
</ul>
</li>
<li>造句练习<ul>
<li>1.在信中她说她明年将到英国来。<ul>
<li><ol>
<li>In her letter, she said that she would come to England next year.</li>
</ol>
</li>
</ul>
</li>
<li>2.他问我明天上午10点我将干什么。<ul>
<li><ol start="2">
<li>He asked me what I should be doing at 10 a.m. the next day</li>
</ol>
</li>
</ul>
</li>
<li>3.他告诉我们他会在8点以前干完工作。<ul>
<li><ol start="3">
<li>He told us he would have finished the work by 8 o’clock.</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="3-英文造句的被动语态"><a href="#3-英文造句的被动语态" class="headerlink" title="3.英文造句的被动语态"></a>3.英文造句的被动语态</h1><h2 id="及物动词和不及物动词"><a href="#及物动词和不及物动词" class="headerlink" title="及物动词和不及物动词"></a>及物动词和不及物动词</h2><ul>
<li>只有及物动词才有被动语态，不及物动词没有被动语态</li>
<li>及物动词后面可以加宾语<ul>
<li>主动语态: I eat meat.我吃肉</li>
<li>被动语态: the meat is eaten by me.</li>
</ul>
</li>
<li>不及物动词后面不可以加宾语，没有被动语态<ul>
<li>主动语态: I appear.我出现了</li>
</ul>
</li>
</ul>
<h2 id="主动语态变成被动语态的方式"><a href="#主动语态变成被动语态的方式" class="headerlink" title="主动语态变成被动语态的方式"></a>主动语态变成被动语态的方式</h2><ul>
<li>1.be动词（根据16中时态变化，与原句时态一致）+过去分词，原句的宾语作主句，而原句的主语，在被动语态中省略，或者前面加上介词by作状语</li>
<li>2.所谓被动语态，其实是一种特殊的主系表句型：过去分词作表语</li>
<li>举例<ul>
<li>I was beaten, 我被打了。(主语+be动词+动词的过去分词)</li>
<li>主动语态: Somebody beat me. 主谓宾结构</li>
<li>被动语态: I was beaten by somebody. 这里的by somebody可以省略。</li>
</ul>
</li>
</ul>
<h2 id="被动语态的时态变化"><a href="#被动语态的时态变化" class="headerlink" title="被动语态的时态变化"></a>被动语态的时态变化</h2><ul>
<li>一般态<ul>
<li>一般现在时<ul>
<li>Our clavichord is kept in the living-room.我们的这架古钢琴存放在起居室里。(这句话里边，可以认为clavichord是主语，is是系动词，kept是表语)</li>
</ul>
</li>
<li>一般过去时<ul>
<li>The instrument was bought by my grandfather many years ago.这件乐器是我祖父在很多年以前买的。(这句话里边，可以认为instrument是主语，was是系动词，bought是表语)</li>
</ul>
</li>
<li>一般将来时<ul>
<li>The Olympic Games will be held in our country in four years’ time. 4年以后，奥林匹克运动会将在我们国家举行。(这句话里边，可以认为Games是主语)</li>
</ul>
</li>
<li>被动语态句中出现情态动词时，用法同will<ul>
<li>The work must be finished in one way or another. 这件事必须设法做好。in one way or another是无论如何的意思</li>
<li>This passage may be given several interpretations.这段文字可以有不同的解释。</li>
</ul>
</li>
</ul>
</li>
<li>进行态: be动词+being+过去分词，be动词时态和原句保持一致<ul>
<li>现在进行时<ul>
<li>It is being repaired by a friend of my father’s.父亲的一个朋友正在修理它。(这句话里边，可以认为It是主语，is being是系动词，repaired是表语)</li>
<li>主动语态: A friend of my father’s is repairing it.</li>
<li>I am a doctor (常规状态).</li>
<li>I am being a doctor. (过去和未来不确定，仅表示现在)</li>
</ul>
</li>
<li>过去进行时<ul>
<li>I was being tested for a driving licence for the third time.(过去进行时)我第3次接受驾驶执照考试。(这句话里边，可以认为I是主语，was being是系动词，tested是表语)</li>
<li>主动语态: Somebody was testing me for a driving licence for the third time.</li>
</ul>
</li>
<li>将来进行时<ul>
<li>He will be being examined when we get there.当我们到那儿时他将正被检查。(这句话里边，可以认为He是主语，will be being是系动词，examined是表语)</li>
<li>主动语态: Somebody will be examming him when we get there</li>
</ul>
</li>
<li>完成态: have/has/had been +动词过去分词<ul>
<li>现在完成时<ul>
<li>The fantastic modern buildings have been designed by Kurt Gunter.这些巨大的现代化建筑是由库尔特.冈特设计的。(这句话里边，可以认为buildings是主语，have been是系动词，designed是表语)</li>
<li>主动语态: Kurt Gunter has designed the fantastic modern buildings</li>
</ul>
</li>
<li>过去完成时<ul>
<li>I had been asked to drive in heavy traffic and had done so successfully.按照要求在车辆拥挤的路上驾驶，我圆满地完成了。(这句话里边，可以认为I是主语，had been是系动词，asked是 表语)</li>
<li>主动语态: Somebody had asked me to drive in heavy traffic</li>
<li>Bluebird, the car he was driving, had been specially built for him.他驾驶的“蓝鸟”牌汽车是专门为他制造的。</li>
<li>主动语态: Somebody had specially built Bluebird, the car he was driving for him</li>
</ul>
</li>
<li>将来完成时<ul>
<li>Your character will have been completed by the time your life comes to an end.当生命走到尽头的时候，你的人格才变得完全。(这句话里边，可以认为character是主语， will have been是系动词，completed是表语)</li>
</ul>
</li>
</ul>
</li>
<li>完成进行态很少用于被动语句</li>
<li>造句练习<ul>
<li>1.欧元在大部分欧洲国家都被使用。<ul>
<li><ol>
<li>The euro is used in most European countries.</li>
</ol>
</li>
</ul>
</li>
<li>2.这些电脑是在台湾制造的。<ul>
<li><ol start="2">
<li>These computers were manufactured in Taiwan.</li>
</ol>
</li>
</ul>
</li>
<li>3.2012年的奥运会将在伦敦举行。<ul>
<li><ol start="3">
<li>The 2012 Olympic Games will be held in London.</li>
</ol>
</li>
</ul>
</li>
<li>4.一名应聘者正被我们的人事经理面试着<ul>
<li><ol start="4">
<li>An applicant is being interviewed by our personnel manager.</li>
</ol>
</li>
</ul>
</li>
<li>5.那栋旧大楼已被拆除。(tear down)<ul>
<li><ol start="5">
<li>The old building has been torn down.</li>
</ol>
</li>
</ul>
</li>
<li>6.我们办公室的房间都是每天打扫的。<ul>
<li><ol start="6">
<li>Our office rooms are cleaned up every day.</li>
</ol>
</li>
</ul>
</li>
<li>7.汤姆昨天被一只狗咬到，所幸无大碍。<ul>
<li><ol start="7">
<li>Tom was bitten by a dog yesterday. Fortunately, it was nothing serious.</li>
</ol>
</li>
</ul>
</li>
<li>8.因为经济不景气(the economic recession),大约5000名员工将被裁员(lay off)。<ul>
<li><ol start="8">
<li>Because of the economic recession, about 5,000 employees will be laid off.</li>
</ol>
</li>
</ul>
</li>
<li>9.六个人被困在矿井里已有17个小时了。<ul>
<li><ol start="9">
<li>Six men have been trapped in a mine for seventeen hours.</li>
</ol>
</li>
</ul>
</li>
<li>10.这场地震结束的时候有多少建筑被毁坏了?<ul>
<li><ol start="10">
<li>How many buildings had been destroyed when the earthquake ended?</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="使用被动语态的情况"><a href="#使用被动语态的情况" class="headerlink" title="使用被动语态的情况"></a>使用被动语态的情况</h2><ul>
<li>1.为了突出受动者（主动语态中的宾语）<ul>
<li>A hero is distinguished in difficult circumstances.困境之中显英雄。</li>
<li>A liar is not believed when he tells the truth.撒谎的人讲真理也没人相信。</li>
</ul>
</li>
<li>2.施动者（主动语态的主语）不明确或不必指明时<ul>
<li>Then in 1989, twenty-six years after the crash, the plane was accidentally rediscovered in an aerial survey of the island. (没有提到到底是谁发现的) 于是，到了1989年，飞机失事26年后， 在对小岛的一次航空勘查中那架飞机被意外地发现了。</li>
<li>Once a year, a race is held for old cars.旧式汽车的比赛每年举行一次。(没有提到举办者是谁)</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>英语</category>
      </categories>
      <tags>
        <tag>英语</tag>
        <tag>语法</tag>
      </tags>
  </entry>
</search>
